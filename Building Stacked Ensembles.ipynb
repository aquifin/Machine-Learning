{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kTbeeaDmdnwI"
   },
   "source": [
    "# Building Stacked Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GrRUDJHXdnwL"
   },
   "source": [
    "## Import Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sDKqryFidnwM"
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Image\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from random import randint\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from scipy.spatial import distance\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import neighbors\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import KFold\n",
    "import itertools\n",
    "%matplotlib inline\n",
    "#%qtconsole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NkY4HbpldnwP"
   },
   "source": [
    "## Define StackedEnsembleClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWwJZLridnwQ"
   },
   "source": [
    "Utility function to create classifer objects based on a name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hs5o47NAdnwR"
   },
   "outputs": [],
   "source": [
    "def create_classifier(classifier_type, tree_min_samples_split = 20):\n",
    "\n",
    "    if classifier_type == \"svm\":\n",
    "        c = svm.SVC(probability=True,gamma='auto')\n",
    "\n",
    "    elif classifier_type == \"logreg\":\n",
    "        c = linear_model.LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=1000)\n",
    "\n",
    "    elif classifier_type == \"knn\":\n",
    "        c = neighbors.KNeighborsClassifier()\n",
    "\n",
    "    elif classifier_type == \"tree\":\n",
    "        c = tree.DecisionTreeClassifier(min_samples_split = tree_min_samples_split)\n",
    "\n",
    "    elif classifier_type == \"randomforest\":\n",
    "        c = ensemble.RandomForestClassifier()\n",
    "        \n",
    "    else:\n",
    "        c = linear_model.LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=1000)\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gM1VqM0xdnwU"
   },
   "source": [
    "StackedEnsembleClassifier class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "07ZqXEB1dnwV"
   },
   "outputs": [],
   "source": [
    "# Create a new classifier which is based on the sckit-learn BaseEstimator and ClassifierMixin classes\n",
    "class StackedEnsembleClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    \"\"\"An ensemble classifier that uses heterogeneous models at the base layer and a aggregatnio model at the aggregation layer. A k-fold cross validation is used to gnerate training data for the stack layer model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimators: list \n",
    "        A list of the classifiers in the base layer of the ensemble. Supported types are\n",
    "        - \"svm\" Support Vector Machine implemented by sklearn.svm.SVC\n",
    "        - \"logreg\" Logistic Regression implemented by sklearn.linear_models.LogisticRegression\n",
    "        - \"knn\" k Nearest Neighbour implemented by sklearn.neighbors.KNeighborsClassifier\n",
    "        - \"tree\" Decision Tree implemented by sklearn.tree.DecisionTreeClassifier\n",
    "        - \"randomforest\" RandomForest implemented by sklearn.tree.RandomForestClassifier    \n",
    "    classifier_duplicates: int, optional (default = 1)\n",
    "        How many instances of each classifier type listed in base_estimators is included in the ensemble\n",
    "    stack_layer_classifier: string, optional (default = \"logreg')\n",
    "        The classifier type used at the stack layer. The same classifier types as are supported at the base layer are supported        \n",
    "    training_folds: int, optional (default = 4)\n",
    "        How many folds will be used to generate the training set for the stacked layer\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : array of shape = [n_classes] \n",
    "        The classes labels (single output problem).\n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The default values for most base learners are used.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    \n",
    "    ----------\n",
    "    .. [1]  van der Laan, M., Polley, E. & Hubbard, A. (2007). \n",
    "            Super Learner. Statistical Applications in Genetics \n",
    "            and Molecular Biology, 6(1) \n",
    "            doi:10.2202/1544-6115.1309\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_iris\n",
    "    >>> from sklearn.model_selection import cross_val_score\n",
    "    >>> clf = StackedEnsembleClassifier()\n",
    "    >>> iris = load_iris()\n",
    "    >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
    "\n",
    "    \"\"\"\n",
    "    # Constructor for the classifier object\n",
    "    def __init__(self, base_estimator_types = [\"svm\", \"logreg\", \"tree\"], base_estimator_duplicates = 8, stack_layer_classifier_type = \"logreg\"):\n",
    "        \"\"\"Setup a SuperLearner classifier .\n",
    "        Parameters\n",
    "        ----------\n",
    "        base_estimator_types: The types of classifiers to include at the base layer\n",
    "        base_estimator_duplicates: The number of duplicates of each type of classiifer to include\n",
    "        stack_layer_classifier_type: The type of classifier to include at the stack layer \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Nothing\n",
    "        \"\"\"     \n",
    "\n",
    "        # Initialise class variabels\n",
    "        self.base_estimator_types = base_estimator_types\n",
    "        self.base_estimator_type_list = list()\n",
    "        self.base_estimator_duplicates = base_estimator_duplicates\n",
    "        self.stack_layer_classifier_type = stack_layer_classifier_type\n",
    "        \n",
    "\n",
    "    # The fit function to train a classifier\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build a SuperLearner classifier from the training set (X, y).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            The training input samples. \n",
    "        y : array-like, shape = [n_samples] \n",
    "            The target values (class labels) as integers or strings.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"    \n",
    "        \n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "        \n",
    "        ########################\n",
    "        # LEVEL 0\n",
    "        ########################\n",
    "        \n",
    "        # Set up the base classifeirs in the ensemble\n",
    "        self.classifiers_ = list()\n",
    "        \n",
    "        for i in range(0, self.base_estimator_duplicates):\n",
    "            for t in self.base_estimator_types:\n",
    "\n",
    "                self.base_estimator_type_list.append(t)      \n",
    "                c = create_classifier(t, tree_min_samples_split=math.ceil(len(X)*0.05))\n",
    "                self.classifiers_.append(c)\n",
    "        \n",
    "        # Store the number of classifers in the ensemble\n",
    "        self.n_estimators_ = len(self.classifiers_)\n",
    "\n",
    "        # Use all training data to train base classifiers\n",
    "        X_train = X\n",
    "        y_train = y\n",
    "        \n",
    "        # Set up empty arrays to hold stack layer training data\n",
    "        self.X_stack_train = None #(dtype = float)\n",
    "        self.y_stack_train = y_train\n",
    "          \n",
    "        # Train each base calssifier and generate the stack layer training dataset\n",
    "        for classifier in self.classifiers_:\n",
    "\n",
    "            # Extract a bootstrap sample\n",
    "            X_train_samp, y_train_samp = resample(X_train, y_train, replace=True)    \n",
    "            \n",
    "            # Train a base classifier\n",
    "            classifier.fit(X_train_samp, y_train_samp)\n",
    "            \n",
    "            # Make predictions for all instances in the training set\n",
    "            y_pred = classifier.predict_proba(X_train)\n",
    "\n",
    "            # Append the predictions ot the stack layer traing set (a bit of hacking here!)\n",
    "            try:\n",
    "                self.X_stack_train = np.c_[self.X_stack_train, y_pred]\n",
    "            except ValueError:\n",
    "                self.X_stack_train = y_pred\n",
    "      \n",
    "        ########################\n",
    "        # LEVEL 1\n",
    "        ########################\n",
    "        \n",
    "        # Create the stack layer classifier\n",
    "        self.stack_layer_classifier_ = create_classifier(self.stack_layer_classifier_type, tree_min_samples_split=math.ceil(len(X)*0.05))\n",
    "\n",
    "        # Train the stack layer using the newly created dataset\n",
    "        self.stack_layer_classifier_.fit(self.X_stack_train, self.y_stack_train)\n",
    "            \n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels of the input samples X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like matrix of shape = [n_samples, n_features]\n",
    "            The input samples. \n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, ].\n",
    "            The predicted class labels of the input samples. \n",
    "        \"\"\"\n",
    "        \n",
    "        # Check is fit had been called by confirming that the teamplates_ dictiponary has been set up\n",
    "        check_is_fitted(self, ['stack_layer_classifier_'])\n",
    "\n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        X = check_array(X)\n",
    "   \n",
    "        X_stack_queries = None\n",
    "              \n",
    "        # Make a prediction with each base classifier and assemble the stack layer query\n",
    "        for classifier in self.classifiers_:\n",
    "            \n",
    "            y_pred = classifier.predict_proba(X)\n",
    "            \n",
    "            try:\n",
    "                X_stack_queries = np.c_[X_stack_queries, y_pred]\n",
    "            except ValueError:\n",
    "                X_stack_queries = y_pred\n",
    "        \n",
    "        # Return the prediction made by the stack layer classifier\n",
    "        return self.stack_layer_classifier_.predict(X_stack_queries)\n",
    "    \n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities of the input samples X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like matrix of shape = [n_samples, n_features]\n",
    "            The input samples. \n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, n_labels].\n",
    "            The predicted class label probabilities of the input samples. \n",
    "        \"\"\"\n",
    "        # Check is fit had been called by confirming that the teamplates_ dictiponary has been set up\n",
    "        check_is_fitted(self, ['stack_layer_classifier_'])\n",
    "\n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        X = check_array(X)\n",
    "        \n",
    "        X_stack_queries = None\n",
    "        \n",
    "        # Make a prediction with each base classifier\n",
    "        for classifier in self.classifiers_:\n",
    "            \n",
    "            y_pred = classifier.predict_proba(X)\n",
    "                \n",
    "            try:\n",
    "                X_stack_queries = np.c_[X_stack_queries, y_pred]\n",
    "            except ValueError:\n",
    "                X_stack_queries = y_pred\n",
    "\n",
    "        # Return the prediction made by the stack layer classifier        \n",
    "        return self.stack_layer_classifier_.predict_proba(X_stack_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OwlcILeXdnwY"
   },
   "source": [
    "## Test the StackedEnsembleClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JkdbSCf1dnwZ"
   },
   "source": [
    "Perform a simple test using the StackedEnsembleClassifier on the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "colab_type": "code",
    "id": "zHei2tb8dnwa",
    "outputId": "c46c5ada-c462-469f-c5a3-7cc0a01add7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       1.00      0.96      0.98        50\n",
      "           2       0.96      1.00      0.98        50\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       150\n",
      "   macro avg       0.99      0.99      0.99       150\n",
      "weighted avg       0.99      0.99      0.99       150\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>50</td>\n",
       "      <td>48</td>\n",
       "      <td>52</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1   2  All\n",
       "True                      \n",
       "0          50   0   0   50\n",
       "1           0  48   2   50\n",
       "2           0   0  50   50\n",
       "All        50  48  52  150"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "clf = StackedEnsembleClassifier()\n",
    "clf.fit(iris.data, iris.target)\n",
    "y_pred = clf.predict(iris.data)\n",
    "print(metrics.classification_report(iris.target, y_pred))\n",
    "print(\"Confusion Matrix\")\n",
    "display(pd.crosstab(np.array(iris.target), y_pred, rownames=['True'], colnames=['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1rudtRRIdnwg"
   },
   "source": [
    "Perform a cross validation experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "U437iLF3dnwh",
    "outputId": "ee77146e-271f-46df-d048-5b699c42593a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.93333333 1.         0.93333333 0.93333333 0.93333333\n",
      " 0.93333333 1.         1.         1.        ]\n",
      "0.9666666666666666  +/-  0.033333333333333326\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, iris.data, iris.target, cv=10)\n",
    "print(scores)\n",
    "print(np.mean(scores), \" +/- \", np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1XDQdVvpdnwl"
   },
   "source": [
    "## Task 1: Design the StackedEnsembleHoldOut Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n6IFJ2zvdnwm"
   },
   "outputs": [],
   "source": [
    "class StackedEnsembleHoldOut(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    \"\"\"An ensemble classifier that uses heterogeneous models at the base layer and a aggregatnio model at the aggregation layer. A k-fold cross validation is used to gnerate training data for the stack layer model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimators: list \n",
    "        A list of the classifiers in the base layer of the ensemble. Supported types are\n",
    "        - \"svm\" Support Vector Machine implemented by sklearn.svm.SVC\n",
    "        - \"logreg\" Logistic Regression implemented by sklearn.linear_models.LogisticRegression\n",
    "        - \"knn\" k Nearest Neighbour implemented by sklearn.neighbors.KNeighborsClassifier\n",
    "        - \"tree\" Decision Tree implemented by sklearn.tree.DecisionTreeClassifier\n",
    "        - \"randomforest\" RandomForest implemented by sklearn.tree.RandomForestClassifier    \n",
    "    classifier_duplicates: int, optional (default = 1)\n",
    "        How many instances of each classifier type listed in base_estimators is included in the ensemble\n",
    "    stack_layer_classifier: string, optional (default = \"logreg')\n",
    "        The classifier type used at the stack layer. The same classifier types as are supported at the base layer are supported        \n",
    "    training_folds: int, optional (default = 4)\n",
    "        How many folds will be used to generate the training set for the stacked layer\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : array of shape = [n_classes] \n",
    "        The classes labels (single output problem).\n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The default values for most base learners are used.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    \n",
    "    ----------\n",
    "    .. [1]  van der Laan, M., Polley, E. & Hubbard, A. (2007). \n",
    "            Super Learner. Statistical Applications in Genetics \n",
    "            and Molecular Biology, 6(1) \n",
    "            doi:10.2202/1544-6115.1309\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_iris\n",
    "    >>> from sklearn.model_selection import cross_val_score\n",
    "    >>> clf = StackedEnsembleClassifier()\n",
    "    >>> iris = load_iris()\n",
    "    >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
    "\n",
    "    \"\"\"\n",
    "    # Constructor for the classifier object\n",
    "    def __init__(self, base_estimator_types = [\"svm\", \"logreg\", \"tree\"], base_estimator_duplicates = 8, stack_layer_classifier_type = \"logreg\",hold_out_testset=0.3):\n",
    "        \"\"\"Setup a SuperLearner classifier .\n",
    "        Parameters\n",
    "        ----------\n",
    "        base_estimator_types: The types of classifiers to include at the base layer\n",
    "        base_estimator_duplicates: The number of duplicates of each type of classiifer to include\n",
    "        stack_layer_classifier_type: The type of classifier to include at the stack layer \n",
    "        hold_out_testset: Percentage of the data to be hold out to test and generate stack layed training data\n",
    "        Returns\n",
    "        -------\n",
    "        Nothing\n",
    "        \"\"\"     \n",
    "\n",
    "        # Initialise class variabels\n",
    "        self.base_estimator_types = base_estimator_types\n",
    "        self.base_estimator_type_list = list()\n",
    "        self.base_estimator_duplicates = base_estimator_duplicates\n",
    "        self.stack_layer_classifier_type = stack_layer_classifier_type\n",
    "        self.hold_out_testset = hold_out_testset\n",
    "\n",
    "    # The fit function to train a classifier\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build a SuperLearner classifier from the training set (X, y).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            The training input samples. \n",
    "        y : array-like, shape = [n_samples] \n",
    "            The target values (class labels) as integers or strings.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"    \n",
    "        \n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "        \n",
    "        ########################\n",
    "        # LEVEL 0\n",
    "        ########################\n",
    "        \n",
    "        # Set up the base classifeirs in the ensemble\n",
    "        self.classifiers_ = list()\n",
    "        \n",
    "        for i in range(0, self.base_estimator_duplicates):\n",
    "            for t in self.base_estimator_types:\n",
    "\n",
    "                self.base_estimator_type_list.append(t)      \n",
    "                c = create_classifier(t, tree_min_samples_split=math.ceil(len(X)*0.05))\n",
    "                self.classifiers_.append(c)\n",
    "        \n",
    "        # Store the number of classifers in the ensemble\n",
    "        self.n_estimators_ = len(self.classifiers_)\n",
    "\n",
    "        # Split data into training and holdout set       \n",
    "        \n",
    "        X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=self.hold_out_testset)\n",
    "        \n",
    "        # Set up empty arrays to hold stack layer training data\n",
    "        self.X_stack_train = None #(dtype = float)\n",
    "        self.y_stack_train = y_holdout\n",
    "          \n",
    "        # Train each base calssifier and generate the stack layer training dataset\n",
    "        for classifier in self.classifiers_:\n",
    "\n",
    "            # Train a base classifier \n",
    "            classifier.fit(X_train, y_train)\n",
    "            \n",
    "            # Make predictions for all instances in the holdout set\n",
    "            y_pred = classifier.predict_proba(X_holdout)\n",
    "\n",
    "            # Append the predictions to the stack layer traing set (a bit of hacking here!)\n",
    "            try:\n",
    "                self.X_stack_train = np.c_[self.X_stack_train, y_pred]\n",
    "            except ValueError:\n",
    "                self.X_stack_train = y_pred\n",
    "      \n",
    "        ########################\n",
    "        # LEVEL 1\n",
    "        ########################\n",
    "                \n",
    "        \n",
    "        # Create the stack layer classifier\n",
    "        self.stack_layer_classifier_ = create_classifier(self.stack_layer_classifier_type, tree_min_samples_split=math.ceil(len(X)*0.05))\n",
    "\n",
    "        # Train the stack layer using the newly created dataset\n",
    "        self.stack_layer_classifier_.fit(self.X_stack_train, self.y_stack_train)\n",
    "        \n",
    "        # Train the base classifier on all the data \n",
    "        \n",
    "        for classifier in self.classifiers_:            \n",
    "            # Train a base classifier\n",
    "            classifier.fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels of the input samples X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like matrix of shape = [n_samples, n_features]\n",
    "            The input samples. \n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, ].\n",
    "            The predicted class labels of the input samples. \n",
    "        \"\"\"\n",
    "        \n",
    "        # Check is fit had been called by confirming that the teamplates_ dictiponary has been set up\n",
    "        check_is_fitted(self, ['stack_layer_classifier_'])\n",
    "\n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        X = check_array(X)\n",
    "   \n",
    "        X_stack_queries = None\n",
    "              \n",
    "        # Make a prediction with each base classifier and assemble the stack layer query\n",
    "        for classifier in self.classifiers_:\n",
    "            \n",
    "            y_pred = classifier.predict_proba(X)\n",
    "            \n",
    "            try:\n",
    "                X_stack_queries = np.c_[X_stack_queries, y_pred]\n",
    "            except ValueError:\n",
    "                X_stack_queries = y_pred\n",
    "        \n",
    "        # Return the prediction made by the stack layer classifier\n",
    "        return self.stack_layer_classifier_.predict(X_stack_queries)\n",
    "    \n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities of the input samples X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like matrix of shape = [n_samples, n_features]\n",
    "            The input samples. \n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, n_labels].\n",
    "            The predicted class label probabilities of the input samples. \n",
    "        \"\"\"\n",
    "        # Check is fit had been called by confirming that the teamplates_ dictiponary has been set up\n",
    "        check_is_fitted(self, ['stack_layer_classifier_'])\n",
    "\n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        X = check_array(X)\n",
    "        \n",
    "        X_stack_queries = None\n",
    "        \n",
    "        # Make a prediction with each base classifier\n",
    "        for classifier in self.classifiers_:\n",
    "            \n",
    "            y_pred = classifier.predict_proba(X)\n",
    "                \n",
    "            try:\n",
    "                X_stack_queries = np.c_[X_stack_queries, y_pred]\n",
    "            except ValueError:\n",
    "                X_stack_queries = y_pred\n",
    "\n",
    "        # Return the prediction made by the stack layer classifier        \n",
    "        return self.stack_layer_classifier_.predict_proba(X_stack_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSfGps-Pdnwq"
   },
   "source": [
    "## Test the StackedEnsembleHoldout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kdMgGezsdnwr"
   },
   "source": [
    "Perform a simple test using the StackedEnsembleHoldout on the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "colab_type": "code",
    "id": "1eZXWNg4dnws",
    "outputId": "1546f82b-75cd-46af-9576-f18525b73464"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       1.00      0.94      0.97        50\n",
      "           2       0.94      1.00      0.97        50\n",
      "\n",
      "   micro avg       0.98      0.98      0.98       150\n",
      "   macro avg       0.98      0.98      0.98       150\n",
      "weighted avg       0.98      0.98      0.98       150\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>50</td>\n",
       "      <td>47</td>\n",
       "      <td>53</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1   2  All\n",
       "True                      \n",
       "0          50   0   0   50\n",
       "1           0  47   3   50\n",
       "2           0   0  50   50\n",
       "All        50  47  53  150"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "clf = StackedEnsembleHoldOut()\n",
    "clf.fit(iris.data, iris.target)\n",
    "y_pred = clf.predict(iris.data)\n",
    "print(metrics.classification_report(iris.target, y_pred))\n",
    "print(\"Confusion Matrix\")\n",
    "display(pd.crosstab(np.array(iris.target), y_pred, rownames=['True'], colnames=['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GkFaVAP6dnww"
   },
   "source": [
    "Perform a cross validation experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "n2hDSoVfdnww",
    "outputId": "6f1a4296-55ed-44e9-a831-cb34cf028b90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.93333333 1.         0.93333333 0.93333333 0.93333333\n",
      " 0.93333333 1.         1.         1.        ]\n",
      "0.9666666666666666  +/-  0.033333333333333326\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, iris.data, iris.target, cv=10)\n",
    "print(scores)\n",
    "print(np.mean(scores), \" +/- \", np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kp7FxnROdnw0"
   },
   "source": [
    "## Task 2: Design the StackedEnsembleKFold Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RFY2-tr_dnw0"
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# Create a new classifier which is based on the sckit-learn BaseEstimator and ClassifierMixin classes\n",
    "class StackedEnsembleKFold(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    \"\"\"An ensemble classifier that uses heterogeneous models at the base layer and a aggregatnio model at the aggregation layer. A k-fold cross validation is used to gnerate training data for the stack layer model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimators: list \n",
    "        A list of the classifiers in the base layer of the ensemble. Supported types are\n",
    "        - \"svm\" Support Vector Machine implemented by sklearn.svm.SVC\n",
    "        - \"logreg\" Logistic Regression implemented by sklearn.linear_models.LogisticRegression\n",
    "        - \"knn\" k Nearest Neighbour implemented by sklearn.neighbors.KNeighborsClassifier\n",
    "        - \"tree\" Decision Tree implemented by sklearn.tree.DecisionTreeClassifier\n",
    "        - \"randomforest\" RandomForest implemented by sklearn.tree.RandomForestClassifier    \n",
    "    classifier_duplicates: int, optional (default = 1)\n",
    "        How many instances of each classifier type listed in base_estimators is included in the ensemble\n",
    "    stack_layer_classifier: string, optional (default = \"logreg')\n",
    "        The classifier type used at the stack layer. The same classifier types as are supported at the base layer are supported        \n",
    "    training_folds: int, optional (default = 4)\n",
    "        How many folds will be used to generate the training set for the stacked layer\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : array of shape = [n_classes] \n",
    "        The classes labels (single output problem).\n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The default values for most base learners are used.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    \n",
    "    ----------\n",
    "    .. [1]  van der Laan, M., Polley, E. & Hubbard, A. (2007). \n",
    "            Super Learner. Statistical Applications in Genetics \n",
    "            and Molecular Biology, 6(1) \n",
    "            doi:10.2202/1544-6115.1309\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_iris\n",
    "    >>> from sklearn.model_selection import cross_val_score\n",
    "    >>> clf = StackedEnsembleClassifier()\n",
    "    >>> iris = load_iris()\n",
    "    >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
    "\n",
    "    \"\"\"\n",
    "    # Constructor for the classifier object\n",
    "    def __init__(self, base_estimator_types = [\"svm\", \"logreg\", \"tree\"], base_estimator_duplicates = 8, stack_layer_classifier_type = \"logreg\",no_of_folds=3):\n",
    "        \"\"\"Setup a SuperLearner classifier .\n",
    "        Parameters\n",
    "        ----------\n",
    "        base_estimator_types: The types of classifiers to include at the base layer\n",
    "        base_estimator_duplicates: The number of duplicates of each type of classiifer to include\n",
    "        stack_layer_classifier_type: The type of classifier to include at the stack layer \n",
    "        no_of_folds: Number of folds to generate stack layer training data\n",
    "        Returns\n",
    "        -------\n",
    "        Nothing\n",
    "        \"\"\"     \n",
    "\n",
    "        # Initialise class variabels\n",
    "        self.base_estimator_types = base_estimator_types\n",
    "        self.base_estimator_type_list = list()\n",
    "        self.base_estimator_duplicates = base_estimator_duplicates\n",
    "        self.stack_layer_classifier_type = stack_layer_classifier_type\n",
    "        self.no_of_folds = no_of_folds\n",
    "\n",
    "    # The fit function to train a classifier\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build a SuperLearner classifier from the training set (X, y).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            The training input samples. \n",
    "        y : array-like, shape = [n_samples] \n",
    "            The target values (class labels) as integers or strings.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"    \n",
    "        \n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "        \n",
    "        ########################\n",
    "        # LEVEL 0\n",
    "        ########################\n",
    "        \n",
    "        # Set up the base classifeirs in the ensemble\n",
    "        self.classifiers_ = list()\n",
    "        \n",
    "        for i in range(0, self.base_estimator_duplicates):\n",
    "            for t in self.base_estimator_types:\n",
    "                self.base_estimator_type_list.append(t)      \n",
    "                c = create_classifier(t, tree_min_samples_split=math.ceil(len(X)*0.05))\n",
    "                self.classifiers_.append(c)\n",
    "        \n",
    "        # Store the number of classifers in the ensemble\n",
    "        self.n_estimators_ = len(self.classifiers_)\n",
    "\n",
    "        \n",
    "        \n",
    "        # Set up empty arrays to hold stack layer training data\n",
    "        self.X_stack_train = None #(dtype = float)\n",
    "        self.y_stack_train = None\n",
    "        \n",
    "        #creating the folds\n",
    "        kf = KFold(n_splits=self.no_of_folds,random_state=None,shuffle=True)\n",
    "        \n",
    "        #for each fold each classifier is train on train data in that fold and predict on test data of that fold\n",
    "        \n",
    "        for train_index, test_index in kf.split(X):\n",
    "            X_train, X_test = X[train_index], X[test_index]        \n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            \n",
    "            X_stack_train_fold=None\n",
    "            y_stack_train_fold=None\n",
    "            \n",
    "                        \n",
    "            for classifier in self.classifiers_:\n",
    "                \n",
    "                #train on folds train data\n",
    "                classifier.fit(X_train, y_train)   \n",
    "                \n",
    "                # Make predictions on test data of the fold                               \n",
    "                y_pred = classifier.predict_proba(X_test)  \n",
    "                                                         \n",
    "                try:\n",
    "                    X_stack_train_fold = np.c_[X_stack_train_fold, y_pred]\n",
    "                except ValueError:\n",
    "                    X_stack_train_fold = y_pred\n",
    "                \n",
    "            # Append the predictions to the stack layer traing set (a bit of hacking here!)    \n",
    "            try:\n",
    "                self.X_stack_train = np.concatenate((self.X_stack_train, X_stack_train_fold), axis=0) \n",
    "            except ValueError:\n",
    "                self.X_stack_train = X_stack_train_fold\n",
    "            try:\n",
    "                self.y_stack_train = np.concatenate((self.y_stack_train, y_test), axis=0) \n",
    "            except ValueError:\n",
    "                self.y_stack_train = y_test\n",
    "  \n",
    "            \n",
    "     \n",
    "        ########################\n",
    "        # LEVEL 1\n",
    "        ########################\n",
    "        \n",
    "        # Create the stack layer classifier\n",
    "        self.stack_layer_classifier_ = create_classifier(self.stack_layer_classifier_type, tree_min_samples_split=math.ceil(len(X)*0.05))\n",
    "\n",
    "        # Train the stack layer using the newly created dataset\n",
    "        self.stack_layer_classifier_.fit(self.X_stack_train, self.y_stack_train)\n",
    "\n",
    "        # Train each base calssifier on entire training dataset\n",
    "        for classifier in self.classifiers_:        \n",
    "            # Train a base classifier\n",
    "            classifier.fit(X, y)                   \n",
    "        \n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels of the input samples X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like matrix of shape = [n_samples, n_features]\n",
    "            The input samples. \n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, ].\n",
    "            The predicted class labels of the input samples. \n",
    "        \"\"\"\n",
    "        \n",
    "        # Check is fit had been called by confirming that the teamplates_ dictiponary has been set up\n",
    "        check_is_fitted(self, ['stack_layer_classifier_'])\n",
    "        \n",
    "\n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        X = check_array(X)\n",
    "        \n",
    "   \n",
    "        X_stack_queries = None\n",
    "              \n",
    "        # Make a prediction with each base classifier and assemble the stack layer query\n",
    "        for classifier in self.classifiers_:\n",
    "            \n",
    "            y_pred = classifier.predict_proba(X)\n",
    "            \n",
    "            try:\n",
    "                X_stack_queries = np.c_[X_stack_queries, y_pred]\n",
    "            except ValueError:\n",
    "                X_stack_queries = y_pred\n",
    "        \n",
    "        # Return the prediction made by the stack layer classifier\n",
    "        return self.stack_layer_classifier_.predict(X_stack_queries)\n",
    "    \n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities of the input samples X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like matrix of shape = [n_samples, n_features]\n",
    "            The input samples. \n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, n_labels].\n",
    "            The predicted class label probabilities of the input samples. \n",
    "        \"\"\"\n",
    "        # Check is fit had been called by confirming that the teamplates_ dictiponary has been set up\n",
    "        check_is_fitted(self, ['stack_layer_classifier_'])\n",
    "\n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        X = check_array(X)\n",
    "        \n",
    "        X_stack_queries = None\n",
    "        \n",
    "        # Make a prediction with each base classifier\n",
    "        for classifier in self.classifiers_:\n",
    "            \n",
    "            y_pred = classifier.predict_proba(X)\n",
    "                \n",
    "            try:\n",
    "                X_stack_queries = np.c_[X_stack_queries, y_pred]\n",
    "            except ValueError:\n",
    "                X_stack_queries = y_pred\n",
    "\n",
    "        # Return the prediction made by the stack layer classifier        \n",
    "        return self.stack_layer_classifier_.predict_proba(X_stack_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-7e4u6RPdnw3"
   },
   "source": [
    "\n",
    "## Test the StackedEnsembleKfold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A8mOafpjdnw4"
   },
   "source": [
    "Perform a simple test using the StackedEnsembleKfold on the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "colab_type": "code",
    "id": "RBxXFeo4dnw4",
    "outputId": "a47d43ab-45a1-4261-bc42-ac6f95d0089d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       1.00      0.96      0.98        50\n",
      "           2       0.96      1.00      0.98        50\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       150\n",
      "   macro avg       0.99      0.99      0.99       150\n",
      "weighted avg       0.99      0.99      0.99       150\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>50</td>\n",
       "      <td>48</td>\n",
       "      <td>52</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1   2  All\n",
       "True                      \n",
       "0          50   0   0   50\n",
       "1           0  48   2   50\n",
       "2           0   0  50   50\n",
       "All        50  48  52  150"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "clf = StackedEnsembleKFold()\n",
    "clf.fit(iris.data, iris.target)\n",
    "y_pred = clf.predict(iris.data)\n",
    "print(metrics.classification_report(iris.target, y_pred))\n",
    "print(\"Confusion Matrix\")\n",
    "display(pd.crosstab(np.array(iris.target), y_pred, rownames=['True'], colnames=['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NgN4ggM1dnw9"
   },
   "source": [
    "Perform a cross validation experiment on iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "Bzb3ikrcdnw9",
    "outputId": "e42fa9ff-6c66-427d-a080-5673a695d268"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         1.         1.         1.         0.86666667 0.93333333\n",
      " 0.93333333 1.         1.         1.        ]\n",
      "0.9733333333333334  +/-  0.04422166387140532\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, iris.data, iris.target, cv=10)\n",
    "print(scores)\n",
    "print(np.mean(scores), \" +/- \", np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AXnccrcEdnxB"
   },
   "source": [
    "## Task 3: Compare the Performance of Different Stack Layer Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34nu13RxdnxC"
   },
   "source": [
    "Perform the StackedEnsembleClassifier,StackedEnsembleHoldout StackedEnsembleKFold on the Fashion MINIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-6u3XQvDdnxC"
   },
   "outputs": [],
   "source": [
    "data_sampling_rate = 0.1 #sampling rate\n",
    "cv_folds = 3 #number of folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uN3PvKDIdnxF"
   },
   "source": [
    "Set up a dictionary to store model perofrmance for comparions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b0fe-HR4dnxG"
   },
   "outputs": [],
   "source": [
    "model_test_accuracy_comparisons = dict()\n",
    "model_valid_accuracy_comparisons = dict()\n",
    "model_tuned_params_list = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UqoOrJIKdnxI"
   },
   "source": [
    "Load & Partition fashion-mnist_test Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "wit38nn4dnxJ",
    "outputId": "ba738c87-9def-4851-c347-15183f22f0e1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44189</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>93</td>\n",
       "      <td>91</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45065</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>96</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29716</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43457</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>146</td>\n",
       "      <td>110</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14111</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "44189      0       0       0       0       0       0       0       0       0   \n",
       "45065      0       0       0       0       0       0       0       0       0   \n",
       "29716      6       0       0       0       0       0       0       0       0   \n",
       "43457      4       0       0       0       0       0       0       0       0   \n",
       "14111      6       0       0       0       0       0       0       2       0   \n",
       "\n",
       "       pixel9    ...     pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "44189       0    ...           93        91        44         0         3   \n",
       "45065      29    ...           89        89        96       106         0   \n",
       "29716       0    ...            0         0         0         0         0   \n",
       "43457       0    ...            1         0         0        92       146   \n",
       "14111       0    ...           53         5         0         0         0   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "44189         1         0         0         0         0  \n",
       "45065         0         0         0         0         0  \n",
       "29716         0         0         0         0         0  \n",
       "43457       110        63         0         0         0  \n",
       "14111         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = pd.read_csv(r'fashion-mnist_train.csv')\n",
    "dataset = dataset.sample(frac=data_sampling_rate) #take a sample from the dataset so everyhting runs smoothly\n",
    "num_classes = 10\n",
    "classes = {0: \"T-shirt/top\", 1:\"Trouser\", 2: \"Pullover\", 3:\"Dress\", 4:\"Coat\", 5:\"Sandal\", 6:\"Shirt\", 7:\"Sneaker\", 8:\"Bag\", 9:\"Ankle boot\"}\n",
    "display(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8a0LiPNdnxN"
   },
   "source": [
    "Isolate the descriptive features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QNpI9zYBdnxO"
   },
   "outputs": [],
   "source": [
    "X = dataset[dataset.columns.difference([\"label\"])]\n",
    "Y = np.array(dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AXQMCEGPdnxS"
   },
   "source": [
    "Normalise the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ErB3sdaFdnxS"
   },
   "outputs": [],
   "source": [
    "X = X/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F7o9behldnxV"
   },
   "source": [
    "Split the data into a training set and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "RP-F5RWHdnxV",
    "outputId": "1cc4ca03-d265-4a77-dae4-18f7445fa9be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test \\\n",
    "    = train_test_split(X, Y, random_state=0, \\\n",
    "                                    train_size = 0.7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oP4veieddnxY"
   },
   "source": [
    "### StackedEnsembleClassifier with stack layer model logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6004X0nBdnxZ"
   },
   "source": [
    "Train StackedEnsembleClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "ZtC9_MkFdnxa",
    "outputId": "bee54a33-eaf1-4cb8-a588-b8950557c40c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackedEnsembleClassifier(base_estimator_duplicates=8,\n",
       "             base_estimator_types=['svm', 'logreg', 'tree'],\n",
       "             stack_layer_classifier_type='logreg')"
      ]
     },
     "execution_count": 80,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StackedEnsembleClassifier_logregmodel = StackedEnsembleClassifier()\n",
    "StackedEnsembleClassifier_logregmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uw6kN7Tmdnxd"
   },
   "source": [
    "### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7jSa5mMudnxe"
   },
   "source": [
    "Assess the performance of the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "id": "vKUtAxsBdnxe",
    "outputId": "56d50a0f-5c90-4bcf-d010-e2addc3b33d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8272222222222222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.79      0.75       171\n",
      "           1       0.98      0.96      0.97       185\n",
      "           2       0.79      0.72      0.75       194\n",
      "           3       0.85      0.82      0.83       195\n",
      "           4       0.64      0.73      0.68       145\n",
      "           5       0.94      0.94      0.94       185\n",
      "           6       0.55      0.51      0.53       171\n",
      "           7       0.91      0.87      0.89       178\n",
      "           8       0.92      0.96      0.94       170\n",
      "           9       0.92      0.94      0.93       206\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      1800\n",
      "   macro avg       0.82      0.82      0.82      1800\n",
      "weighted avg       0.83      0.83      0.83      1800\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>135</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>139</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>193</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>187</td>\n",
       "      <td>181</td>\n",
       "      <td>177</td>\n",
       "      <td>187</td>\n",
       "      <td>166</td>\n",
       "      <td>186</td>\n",
       "      <td>157</td>\n",
       "      <td>170</td>\n",
       "      <td>179</td>\n",
       "      <td>210</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1    2    3    4    5    6    7    8    9   All\n",
       "True                                                             \n",
       "0          135    2    2    7    2    1   18    0    4    0   171\n",
       "1            1  177    0    6    0    0    1    0    0    0   185\n",
       "2            4    0  139    2   29    0   19    0    1    0   194\n",
       "3           14    1    0  159   13    0    7    0    1    0   195\n",
       "4            0    0   12    6  106    0   21    0    0    0   145\n",
       "5            0    0    0    0    0  174    1    5    4    1   185\n",
       "6           33    0   23    6   16    1   87    0    5    0   171\n",
       "7            0    0    0    0    0    7    0  155    0   16   178\n",
       "8            0    1    1    0    0    0    3    1  164    0   170\n",
       "9            0    0    0    1    0    3    0    9    0  193   206\n",
       "All        187  181  177  187  166  186  157  170  179  210  1800"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = StackedEnsembleClassifier_logregmodel.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) # , normalize=True, sample_weight=None\n",
    "model_test_accuracy_comparisons[\"StackedEnsembleClassifier_StackLayerModel_logreg\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "# print(metrics.confusion_matrix(y_train, y_pred))\n",
    "\n",
    "# Print nicer homemade confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UQWmkaPJdnxi"
   },
   "source": [
    "### StackedEnsembleClassifier with stack layer model decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "woswCQL6dnxj"
   },
   "source": [
    "Train StackedEnsembleClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "QC-6lsyrdnxk",
    "outputId": "93148790-03a1-4239-a1b1-b1fb36511a64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackedEnsembleClassifier(base_estimator_duplicates=8,\n",
       "             base_estimator_types=['svm', 'logreg', 'tree'],\n",
       "             stack_layer_classifier_type='tree')"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StackedEnsembleClassifier_treegmodel = StackedEnsembleClassifier(stack_layer_classifier_type='tree')\n",
    "StackedEnsembleClassifier_treegmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AbXRABTtdnxo"
   },
   "source": [
    "### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AmC_7EXBdnxo"
   },
   "source": [
    "Assess the performance of the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "id": "mb-uHbhldnxp",
    "outputId": "5b800887-54fb-4b0a-d293-1d6c4c6cb0fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8016666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.77      0.74       171\n",
      "           1       0.99      0.95      0.97       185\n",
      "           2       0.75      0.65      0.70       194\n",
      "           3       0.87      0.77      0.82       195\n",
      "           4       0.67      0.68      0.67       145\n",
      "           5       0.92      0.88      0.90       185\n",
      "           6       0.44      0.57      0.49       171\n",
      "           7       0.88      0.86      0.87       178\n",
      "           8       0.92      0.92      0.92       170\n",
      "           9       0.91      0.93      0.92       206\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      1800\n",
      "   macro avg       0.81      0.80      0.80      1800\n",
      "weighted avg       0.81      0.80      0.81      1800\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>192</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>182</td>\n",
       "      <td>177</td>\n",
       "      <td>167</td>\n",
       "      <td>174</td>\n",
       "      <td>146</td>\n",
       "      <td>178</td>\n",
       "      <td>221</td>\n",
       "      <td>173</td>\n",
       "      <td>170</td>\n",
       "      <td>212</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1    2    3    4    5    6    7    8    9   All\n",
       "True                                                             \n",
       "0          131    0    2    9    1    0   26    0    2    0   171\n",
       "1            1  176    0    3    0    0    5    0    0    0   185\n",
       "2            6    0  126    1   28    0   30    0    3    0   194\n",
       "3           12    1    0  151    6    0   23    1    1    0   195\n",
       "4            0    0   17    5   98    0   25    0    0    0   145\n",
       "5            0    0    0    0    0  163    3   13    2    4   185\n",
       "6           32    0   20    4   12    1   97    0    5    0   171\n",
       "7            0    0    0    0    0    8    0  153    1   16   178\n",
       "8            0    0    2    0    1    0   11    0  156    0   170\n",
       "9            0    0    0    1    0    6    1    6    0  192   206\n",
       "All        182  177  167  174  146  178  221  173  170  212  1800"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = StackedEnsembleClassifier_treegmodel.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) # , normalize=True, sample_weight=None\n",
    "model_test_accuracy_comparisons[\"StackedEnsembleClassifier_StackLayerModel_Dtree\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "# print(metrics.confusion_matrix(y_train, y_pred))\n",
    "\n",
    "# Print nicer homemade confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "27uFQRVHdnxs"
   },
   "source": [
    "### StackedEnsembleHoldOut with stack layer model logreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BZhmOrbOdnxs"
   },
   "source": [
    "Train StackedEnsembleHoldOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "rb13_Snqdnxt",
    "outputId": "5919e193-6a8c-4644-ff98-2aefddf2f93f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackedEnsembleHoldOut(base_estimator_duplicates=8,\n",
       "            base_estimator_types=['svm', 'logreg', 'tree'],\n",
       "            hold_out_testset=0.3, stack_layer_classifier_type='logreg')"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StackedEnsembleHoldOut_logregmodel = StackedEnsembleHoldOut()\n",
    "StackedEnsembleHoldOut_logregmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SEc5HB4ldnxy"
   },
   "source": [
    "### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c5UTZCDYdnxz"
   },
   "source": [
    "Assess the performance of the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "id": "7znWWuevdnxz",
    "outputId": "b4ca329d-d55e-4fe5-aace-d2cfbebd0335"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8316666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.77      0.77       171\n",
      "           1       0.98      0.95      0.97       185\n",
      "           2       0.79      0.72      0.75       194\n",
      "           3       0.80      0.89      0.84       195\n",
      "           4       0.66      0.70      0.68       145\n",
      "           5       0.94      0.92      0.93       185\n",
      "           6       0.56      0.49      0.52       171\n",
      "           7       0.92      0.88      0.90       178\n",
      "           8       0.92      0.98      0.95       170\n",
      "           9       0.91      0.96      0.93       206\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      1800\n",
      "   macro avg       0.82      0.83      0.82      1800\n",
      "weighted avg       0.83      0.83      0.83      1800\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>139</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>197</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>174</td>\n",
       "      <td>179</td>\n",
       "      <td>176</td>\n",
       "      <td>217</td>\n",
       "      <td>155</td>\n",
       "      <td>182</td>\n",
       "      <td>151</td>\n",
       "      <td>169</td>\n",
       "      <td>181</td>\n",
       "      <td>216</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1    2    3    4    5    6    7    8    9   All\n",
       "True                                                             \n",
       "0          132    1    2   13    0    1   17    0    5    0   171\n",
       "1            1  176    0    6    1    0    1    0    0    0   185\n",
       "2            0    0  139    3   31    0   21    0    0    0   194\n",
       "3            5    1    0  174    6    0    6    1    2    0   195\n",
       "4            0    0   14   10  102    0   19    0    0    0   145\n",
       "5            0    0    0    0    0  171    0    7    4    3   185\n",
       "6           36    0   21   10   15    1   84    0    4    0   171\n",
       "7            0    0    0    0    0    6    0  156    0   16   178\n",
       "8            0    1    0    0    0    0    3    0  166    0   170\n",
       "9            0    0    0    1    0    3    0    5    0  197   206\n",
       "All        174  179  176  217  155  182  151  169  181  216  1800"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = StackedEnsembleHoldOut_logregmodel.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) # , normalize=True, sample_weight=None\n",
    "model_test_accuracy_comparisons[\"StackedEnsembleHoldOut_StackLayerModel_Logreg\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "# print(metrics.confusion_matrix(y_train, y_pred))\n",
    "\n",
    "# Print nicer homemade confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "crHIhdIadnx2"
   },
   "source": [
    "### StackedEnsembleHoldOut with stack layer model Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X7mgmJc-dnx3"
   },
   "source": [
    "Train StackedEnsembleHoldOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "8lMUXNs9dnx3",
    "outputId": "82c8fb9b-c69c-4eb5-81b1-0b9cab85a427"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackedEnsembleHoldOut(base_estimator_duplicates=8,\n",
       "            base_estimator_types=['svm', 'logreg', 'tree'],\n",
       "            hold_out_testset=0.3, stack_layer_classifier_type='tree')"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StackedEnsembleHoldOut_treegmodel = StackedEnsembleHoldOut(stack_layer_classifier_type='tree')\n",
    "StackedEnsembleHoldOut_treegmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jKyEBtxdnx7"
   },
   "source": [
    "### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zsDPTMsOdnx8"
   },
   "source": [
    "Assess the performance of the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "id": "8ZsWsQktdnx9",
    "outputId": "850f52fc-bf70-4981-ffb4-9c20909e204e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8061111111111111\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.84      0.72       171\n",
      "           1       0.99      0.95      0.97       185\n",
      "           2       0.86      0.61      0.71       194\n",
      "           3       0.85      0.82      0.83       195\n",
      "           4       0.66      0.75      0.71       145\n",
      "           5       0.96      0.84      0.90       185\n",
      "           6       0.50      0.42      0.46       171\n",
      "           7       0.90      0.84      0.87       178\n",
      "           8       0.88      0.98      0.93       170\n",
      "           9       0.83      0.97      0.90       206\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      1800\n",
      "   macro avg       0.81      0.80      0.80      1800\n",
      "weighted avg       0.81      0.81      0.80      1800\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>119</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>167</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>230</td>\n",
       "      <td>177</td>\n",
       "      <td>139</td>\n",
       "      <td>189</td>\n",
       "      <td>164</td>\n",
       "      <td>163</td>\n",
       "      <td>143</td>\n",
       "      <td>165</td>\n",
       "      <td>190</td>\n",
       "      <td>240</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1    2    3    4    5    6    7    8    9   All\n",
       "True                                                             \n",
       "0          144    1    1   10    0    1    7    0    7    0   171\n",
       "1            2  175    0    5    0    0    3    0    0    0   185\n",
       "2            6    0  119    1   33    0   32    0    3    0   194\n",
       "3           16    1    0  160    9    0    6    1    2    0   195\n",
       "4            1    0    8    7  109    0   19    0    1    0   145\n",
       "5            0    0    0    0    0  156    1   12    2   14   185\n",
       "6           61    0   11    5   13    1   72    0    8    0   171\n",
       "7            0    0    0    0    0    3    0  149    0   26   178\n",
       "8            0    0    0    0    0    0    3    0  167    0   170\n",
       "9            0    0    0    1    0    2    0    3    0  200   206\n",
       "All        230  177  139  189  164  163  143  165  190  240  1800"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = StackedEnsembleHoldOut_treegmodel.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) # , normalize=True, sample_weight=None\n",
    "model_test_accuracy_comparisons[\"StackedEnsembleHoldOut_StackLayerModel_Dtree\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "# print(metrics.confusion_matrix(y_train, y_pred))\n",
    "\n",
    "# Print nicer homemade confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "61ey4kpjdnx_"
   },
   "source": [
    "### StackedEnsembleKFold with stack layer model logreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJX2AaFRdnyA"
   },
   "source": [
    "Train StackedEnsembleKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "sPjs88pkdnyA",
    "outputId": "dc89c182-1a2b-4597-cee9-816f33b4f430"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackedEnsembleKFold(base_estimator_duplicates=8,\n",
       "           base_estimator_types=['svm', 'logreg', 'tree'], no_of_folds=3,\n",
       "           stack_layer_classifier_type='logreg')"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StackedEnsembleKFold_logregmodel = StackedEnsembleKFold()\n",
    "StackedEnsembleKFold_logregmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3TepQQHKdnyF"
   },
   "source": [
    "### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_VfRErErdnyF"
   },
   "source": [
    "Assess the performance of the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "id": "e7bV1aU2dnyH",
    "outputId": "e986d0d3-a27a-4b65-c22e-cc373025cd28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8372222222222222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.78      0.77       171\n",
      "           1       0.98      0.97      0.98       185\n",
      "           2       0.80      0.70      0.75       194\n",
      "           3       0.83      0.85      0.84       195\n",
      "           4       0.65      0.74      0.70       145\n",
      "           5       0.92      0.95      0.93       185\n",
      "           6       0.58      0.54      0.56       171\n",
      "           7       0.94      0.87      0.90       178\n",
      "           8       0.92      0.98      0.95       170\n",
      "           9       0.92      0.96      0.94       206\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      1800\n",
      "   macro avg       0.83      0.83      0.83      1800\n",
      "weighted avg       0.84      0.84      0.84      1800\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>134</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>154</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>197</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>179</td>\n",
       "      <td>182</td>\n",
       "      <td>169</td>\n",
       "      <td>199</td>\n",
       "      <td>165</td>\n",
       "      <td>190</td>\n",
       "      <td>158</td>\n",
       "      <td>164</td>\n",
       "      <td>180</td>\n",
       "      <td>214</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1    2    3    4    5    6    7    8    9   All\n",
       "True                                                             \n",
       "0          134    1    2   11    0    2   18    0    3    0   171\n",
       "1            1  179    0    4    0    0    1    0    0    0   185\n",
       "2            2    0  136    2   33    0   20    0    1    0   194\n",
       "3            9    1    0  166    8    0    9    0    2    0   195\n",
       "4            0    0   14    8  108    0   15    0    0    0   145\n",
       "5            0    0    0    0    0  175    0    5    3    2   185\n",
       "6           33    0   17    7   16    1   92    0    5    0   171\n",
       "7            0    0    0    0    0    9    0  154    0   15   178\n",
       "8            0    1    0    0    0    0    3    0  166    0   170\n",
       "9            0    0    0    1    0    3    0    5    0  197   206\n",
       "All        179  182  169  199  165  190  158  164  180  214  1800"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = StackedEnsembleKFold_logregmodel.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) # , normalize=True, sample_weight=None\n",
    "model_test_accuracy_comparisons[\"StackedEnsembleKFold_StackLayerModel_logreg\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "# print(metrics.confusion_matrix(y_train, y_pred))\n",
    "\n",
    "# Print nicer homemade confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMpsGZJXdnyK"
   },
   "source": [
    "### StackedEnsembleKFold with stack layer model Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kSUYdNiLdnyL"
   },
   "source": [
    "Train StackedEnsembleKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "hoGF8cNudnyM",
    "outputId": "43e907a3-a7c4-4424-f981-cd0bc118fb95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackedEnsembleKFold(base_estimator_duplicates=8,\n",
       "           base_estimator_types=['svm', 'logreg', 'tree'], no_of_folds=3,\n",
       "           stack_layer_classifier_type='tree')"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StackedEnsembleKFold_treegmodel = StackedEnsembleKFold(stack_layer_classifier_type='tree')\n",
    "StackedEnsembleKFold_treegmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S0XotSpFdnyP"
   },
   "source": [
    "### Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j0OZDucmdnyQ"
   },
   "source": [
    "Assess the performance of the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "id": "0dU891BEdnyQ",
    "outputId": "cbfdb9ae-a6ae-4418-ebc0-5f89afbac72a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8005555555555556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.64      0.73       171\n",
      "           1       0.98      0.96      0.97       185\n",
      "           2       0.74      0.73      0.73       194\n",
      "           3       0.82      0.83      0.82       195\n",
      "           4       0.65      0.71      0.68       145\n",
      "           5       0.85      0.86      0.86       185\n",
      "           6       0.52      0.58      0.55       171\n",
      "           7       0.83      0.88      0.85       178\n",
      "           8       0.88      0.91      0.89       170\n",
      "           9       0.89      0.86      0.87       206\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      1800\n",
      "   macro avg       0.80      0.80      0.80      1800\n",
      "weighted avg       0.81      0.80      0.80      1800\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>177</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>130</td>\n",
       "      <td>180</td>\n",
       "      <td>190</td>\n",
       "      <td>198</td>\n",
       "      <td>158</td>\n",
       "      <td>186</td>\n",
       "      <td>191</td>\n",
       "      <td>190</td>\n",
       "      <td>177</td>\n",
       "      <td>200</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1    2    3    4    5    6    7    8    9   All\n",
       "True                                                             \n",
       "0          110    1    2   11    1    2   39    0    5    0   171\n",
       "1            0  177    1    6    0    0    1    0    0    0   185\n",
       "2            0    0  141    1   34    0   15    0    3    0   194\n",
       "3            1    1    1  162    8    1   19    0    2    0   195\n",
       "4            0    1   20    9  103    0   11    0    1    0   145\n",
       "5            0    0    0    0    0  159    3   13    3    7   185\n",
       "6           18    0   25    7   12    1  100    0    8    0   171\n",
       "7            0    0    0    0    0    5    0  157    0   16   178\n",
       "8            1    0    0    1    0   11    2    0  155    0   170\n",
       "9            0    0    0    1    0    7    1   20    0  177   206\n",
       "All        130  180  190  198  158  186  191  190  177  200  1800"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = StackedEnsembleKFold_treegmodel.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) # , normalize=True, sample_weight=None\n",
    "model_test_accuracy_comparisons[\"StackedEnsembleKFold_StackLayerModel_Dtree\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "# print(metrics.confusion_matrix(y_train, y_pred))\n",
    "\n",
    "# Print nicer homemade confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VWKnS0EndnyV"
   },
   "source": [
    "### Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "896wxNIEdnyW",
    "outputId": "a6e42e27-72b1-4c5c-e450-18a3e77150d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'StackedEnsembleClassifier_StackLayerModel_Dtree': 0.8016666666666666,\n",
       " 'StackedEnsembleClassifier_StackLayerModel_logreg': 0.8272222222222222,\n",
       " 'StackedEnsembleHoldOut_StackLayerModel_Dtree': 0.8061111111111111,\n",
       " 'StackedEnsembleHoldOut_StackLayerModel_Logreg': 0.8316666666666667,\n",
       " 'StackedEnsembleKFold_StackLayerModel_Dtree': 0.8005555555555556,\n",
       " 'StackedEnsembleKFold_StackLayerModel_logreg': 0.8372222222222222}"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(model_test_accuracy_comparisons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "adk4HXATdnyZ"
   },
   "source": [
    "Plot the comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "colab_type": "code",
    "id": "vt9NMFiqdnyZ",
    "outputId": "2ccccf74-a19a-4b7b-dc48-054d213af7c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Used for comparision training data X_train :  (4200, 784) testing data X_test (1800, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvwAAAEZCAYAAADrOfEjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xm8lnP+x/HXqRSaUogQZX1jYkZS\nslV2RrZUxtqMwYxdqGTLMowt+6BfiIzQjH0NKWVLjGEsn2FGlsEUZQmlcn5/fL53Xefuvu9zl7PU\n8Xk+Hj0657qu+3t9v99ruT/Xd7lORWVlJSGEEEIIIYSGqVF9ZyCEEEIIIYRQeyLgDyGEEEIIoQGL\ngD+EEEIIIYQGLAL+EEIIIYQQGrAI+EMIIYQQQmjAIuAPIYQQQgihAYuAP4QaJqm9pNmSNi1j29mS\n9qiLfBXY9+WSvpZ0UX3sPy8v/SV9VoPpTZV0XE2lV1eqqwdJQyVNqct91tA+DpX0Ueb3fpKmSXo2\nf92yTtJ4SZfVUFo9JFVK+llNpFeXqquH+rhGU13uVWRdh7S+Y13mqb7FvdfV0733ZEkzJd1RxrYl\n9y9ppKS/lkqjyZJkMoTaJGlD4CxgF6AVMA14BDjXzD6tz7yVw8zeB5Yvc9uytqtpkloBJwO9zeze\nItvsCgwGNgdaAJ8AfwGGmtnctE1/4BEzm1YX+V4SkoYCZwPfF1j9spltW7c5qjuSKoCjgd8BGwNz\ngTeAa8zsrrrKh5mNAkZlFg0GRgMnmVll3roaJakJMAT4NbA2UAH8A/ijmT2ctmkF9DWz4bWVj5og\naSqwFjC/wOqTzOyGOs1QHZE0HphiZqfmLe8BPA20MLNZtbDf9YAzgd2AlYEvgCeA883sncVIpz+L\ncZ+Me++yLR2by8xs1cyylYFngMnAEem+dzZwnpldURf5ioA/LFUkbY5fFDcDnYD/AZsClwKTJf3C\nzGbWYxYbipXwwKfgl5akrYD7gT8AY4DZwJbA3cAKwABJjYErgCn4Q9nS7BUz61zfmagHNwHdgd/j\ngdGKwAHAzZLWNrMaaYleAq2Ad9KXXm27DNgJD/hfxx/GjwDuk9TVzF5J648CluqAPznZzK6t70w0\ndKmlfyIeaG9tZh9KWhM4FZgiqWc6d6pLZ7Huk3HvbXgkNQceBt4Ejszc91pR5Du4NkTAH5Y2VwNP\nm9mAzLJ/StoH+DPQDpiZWuSuAHbFL5oXgBPM7A3wrlvgYLwVuyMwCfgtcD3QA/gXcICZ/Se1QnQH\nxgIDgGZpX6ebWaWk5YErgb3x1pa3gBPN7Pm0r/HAS8DOwAw8mHgP2MzM/inpcLyFsR3eQnQzcHZK\nuxLoZWYPSWoGXIgHZKsBrwGnmtnEtJ+pwB+BfVN+PwGOMrOnC1WkpG7A5an83wF3AqcBHfDAB+Bl\nSVeb2Wl5H98R+NDMRmaWTZbUJ9UBwFd4APmypEvM7CxJBwJnAOsCM4HrzezCTJ4Oxntv1gb+iR+z\nFwvkfSfgHmAXM5ssqTtwUSrLN3gge7aZ/ZBacIcBB+EtSecXqo9SUovMAPzB8gKgNfAAcLiZzZfU\nFT/fOgLz8Fa+o83sC0mNUpkOBdYEDD9uT6W0pwKX4Md1a/ym3wcYCuyHP9QeZGYvZfJzKH6sW5C+\n/M3suwL5LlUvPYD+QCczezV95Cs82J8FbFCkLnbGz8ONU5p3AwNSPawIXAf8Cj/2BpxmZuOqWdef\n1OKV6qM9cEUq5/VkWsMkbZbqeku8JfuveOv17JTOYOA+4DhgczP7T6FyZOwGjMrUwTfA1ZI+xe8l\nv8Z7GBpJmo23qn5A6Wt+RfycOyCl+RBwrJl9U6A+bwE2xO8P31P6XNkAuBX4JX6e3FRN2RaR7kdj\ngU2AffBjfkquR0fSQOAY/B7zKXCFmV2T1q0NXAtsAzQFHk3l+jydTw8DB+L36Tb4A9KtwEhgI/y6\n6GtmuRbdZpJuw8/zacA5ZnZ7gTyXvIYWo+wlvxfytl0t5X074EP8esu6FnjGzBYMTzGzj/GAux1w\nA9BFUgcy9/uU9lBgrxToLnKfrKYYce9dxu+9eds1TfU5Ezg4lakZ8GXa5B5J95lZX0m9gPPwa2kG\nfn1dUKhhRNLv8OO9CnAH0LhUPUOM4Q9LEUlt8EB2kdYrM/vezH5nZrlA9f/wG9uW+BfXVODB1PKR\n8wf8C29ToBvwJH4xrQkshz8M5GwJ/AwPynfBvxD7pXWnpXxtjt+MnsZbXrJ+nT6zc16Z2uEB/rEp\n/Z7AIXhQlO8CYHf8ht8KeCyVqXVmm1Pxm9XKeNfg5QXSydXlkymfbdJ+9wbONLN/AcqVu0CwD35j\n3EDS0enmBICZvWRm49KvP8+kcVb64rsdGGxmPwP2B86RtEvK05b4cTsW72G4B3hI0gp5eRdwF3BY\n+sJphwdUI1K5d8Lr8Mj0kd/iQUh3PIj9ObB6oXqpRnugC36+7IAf/9xxGoV/0awMrI9/GQxJ607A\nv3B+lcp1PXB/6sLNORY/P9bBj8czeCC9Gh4snJ3ZtgV+vH4OdMXPx8H5mS2jXg4Ans0EuguY2d3Z\nYCCT5grAvXgAtxIeDB2E1zHASXjP26aZst6evvhLrcvuuwPwPt5S3TVv/yvi5/0E/Bh2AjoD52Q2\nawtU4tfie/llKOBN4HBJVVoZUx28Z2aj8WvvFTNbPl0f1V3zFwK/SGVV+ndx/o5TcL0NsI+Zzab6\nc+U24CP8vDgIv4ctiWPxa3GV9P+fJVVI2gY4F29kWBE/x8+VtFka/vUA3pCwLn6et6Tq/XgF/AHq\n58Bv8Hvon/D7VidgD6BXZvtD8fNpVTyguzU91OQr5xoqRznfCzlX4vfk9vi1kzvHs99FVxfZz1XA\nVpLal5GnKvfJMraPe++yf+/NbdcIPy7L48Nn5wKY2ZzMcN79U7DfEb9WLsS///vi19dvCux/I/xh\n4DT82noWf4ApKQL+sDRZL/1vpTZKAXBvPHj9NLWqnY7f6LtkNr3TzD42s/fwFo1XzWyymX2FBxQb\nZratwMdHzkmtHg/hDwvgT/FdzOwzM5uH3xDXkrRG5vMvmdnzBZ7EW+LX2Swzq0zBxPpm9lCBov0O\nuMjM/m1mc/DWkkb4l2nOI+nG/z3e+rBJkWo6CPjYzK5IZXoTvxn2K7J9FWb2IP5FfhXwuaRxks5M\nN5pin5kKtLE0Ljq1mhgesAEchvfePJXq8Ur8ht00l0a6UT+Iz9e4Py3+tSdnN5vZvFSWq/HWa/Ab\n3Wgze8PMvsXH3C6Xl71O8gnS+f/6Z7ZpCZxlZt+kIPkdFtZvK+C7tP+ZeOvdwLTuSOBKc3PNx4H/\nB29JynnEzN42s8+A5/Fj83AKAsdS9VxsircUfZ3Ol9uAPQtUeXX1sj7VXEv5UktWO+DGdL7+O+U3\ndwxb4a1s35rZfDMbAayVjmepdeX6FdDUzM5PD/kf4F+A/TPbtAQuTnVdzpCgE/HW5ZckfSjpDkmH\ny7vZiyl6zafA+HBgmJlNM7PP8Wv3vmwC8l7JAcAeaRsoca5Iaos3TPwpnYPv4EFaviuKnMvZoHay\nmT2eAox78KBkNfwYAcyCBdfoqqkhpTP+gDPQzGalc/XMlLdcXVXgLcff4tcpwANm9j8zM/yayZ7L\nU8zs3nQ/uxFvUd2tQJnKuYZOyi8zfu3k6rvc74WcffFjOMPMPsFbkXOq+y56M/2/fpH1SyzuvQ3i\n3ptzfcrLECvQS5DnCGC8mY1JZXke75Uv9J29H/C6mf013SdHAW9Xk34M6QlLpeq6pjrgXzy5my5m\nNk3S12nd82nxh5nPzAb+m/d7dsLse+nLMed9vGUOvMXiytSl3TKzTbO87Qt5C//SniTpefwGMzIv\nb7kvq1Z5ZZqXuiQ7ZPOZ+flbik8OXi/tO+vdvLRKMrMhki7Gey264y0p50o62cyKtXz9QdJv8YmF\nFfgNNFdP62fzn4KA0QDesERjfPjG15aGGGQ+t0X6gs+pwIMH8AD18Uy6X0j6H1WVM450ppl9kfn9\nW7xFE7xF6WpJh6V93YEP48rlb5iqvpGkEd51nrM45+LXZpZ9a82/8QAzX3X1AmV08xbQBzg5tRo2\nxr/Ac5Nq/4w/CP9X0lj8wfhOfDJwqXXlWh9YJa9MAI0zrZ1f5R2nklJdbp9aL3fGWxCvAS6UtGMK\nVPOVuuZXwa/V7Ln8Bj4ZOmczfN7E6VZ1yFGpc2Wt9Hv2Gi+Ut3LG8OffJ8DP5afwnj+TD/15HL8f\nfZ7y1giYlq7HrDUzP38IYD7ECkqfy9n7WaWk/7CwnFnlXENXWvFJu1D+9wKSVsHro7q6rq5RtFbm\noMS9t0Hce1fBj8WNwChJW2Ue/AtZj8y5m7yL9x7ka8eivZtG1ZhkERHwh6XJv/Ab6M8pHkBD6ZM6\newP+IW9d/u9Z+YFRRSatO/GWyy3N7ANJvwDyh0kUbMVMLZBHSboEb1E6ABgsn/A1ObPpkpapmGLp\nLdYXlJl9Cfwt/UPShcAlkhZ5G4ikI/Cb8/7AuPTA8vfMJj9Q+gu0FR54bCmpny18i8x3wFgzK/b6\n0mYsei9bkt7LonVrZiMk3YsPi9oHeF7SSSnw+g74vZV+683inIv56yrwL6Z81dWLsfChtSzy8bs3\n4C2CfzWz71O5PUGzqfLXzfbEh25cBhwjaftS6xYjC9/5bqxgz1UKThanx2CBFNgbcF16wH4W765f\npMuc0td87viUOsd2wLvnB0u63Ra+aKDouSIfbgNVz+Ul7YUveH6lQK9XKs/eeNkHycdJfwfMNrMV\nCn1WUi5Qr61zubprqDrl3kOz2xar69x30Sb4fI58uSciI9NKnrEkD9pVxL3XLaP3XvAx+73wc+FJ\nYIykXa14j+fifGcvUb3HkJ6w1Ehfik8Bp+Svk7ScpEmS9sS77CAznEX+9oQW+BPxklhbUrYrsj0+\nlha8O3i4+fAC8PGhZZHUSNLKZvaumV1mZlvjY+8Pzdt0GvA1Vcu0PN4ytSRl+jc+6TJr43LTknSa\nfAJRvsfxL7hCPQtd8DHjY9MXTkuqTgz9Dwu/KHN1MyAdO/AbZG7c4p8zy98FOsrHQ+Y+u5oWjj/9\nmEyLTmq9W62ccpZL0qpm9rmZ3WJm++LDrY7O5G/zvO07/IjdrSQpOw52fRaei1nV1csYfJzxDvkf\nlHSApKflw1OyugD/NrM7UrDfGB+rnvtcc3zIzZNmdmLafmvgF6XWLUbZ3wU6pHMnt8/WklZajDSy\n5Wwn6c/5n0/3mhep2nqfVfSaN7MZ+OT77Lm8WQq6cm7AXxrwDt6tn1PqXPk4/Z9tnaz2b3ksDklN\nJLUys3+Y2fn45OAv8UDxXWB5+WuRc9uvKJ/cusS7zKRVgbdiFjuXf+w1tDjfC9PxnqeCdZ3Ojyfw\neSmFHA9MMJ/EmwsIV8ysX+KhPnHvrWoZvfcC/GA+LHIe3mu6AcXnhIB/Z+c3dBT7zq5S70m194oI\n+MPS5iS8lWGM/A9YNZJPZnkQn2D1jPl7hx8Bzpe0arq5XYKP0395CfdbAZwuqVlq7foVC8fk/gfo\nmh46dsTHiULhrul8/YB/yN88gqR18O64Khex+cz+UXiLYPt04zgX79p8bAnKcxf+EHNCyvfm+MSl\nkWV+/mfATZL2Sl/6jSRtjE9wGm8+DyI3JnEjSS3wetpI0irySU3D8e7UXD2NxIdW7Jsero7Dx9h+\nldbnbpA3A8/hk53Bu3BbAkNTXtbBj//paf2jQD9Jm8j/QNGFwJyya6oaqSz/lbSfpMaprB1ZeAyv\nx7vTt0vr9wbekBYdF1GmOSws6/p44FjobyWUrBczexafVHafpL6SmkpqkQLTW4CRtugY+P8Aa8j/\nCNGq+ITNL1g4pOMe4AZJrdKX3db42zk+qGZduR7HJ41eIWkl+eTJ2/EhOAWl4/J8kdXT8Il3t6fz\no7Gk5eXj6/fH58GAn8tt07nbjOqv+VuA09IDRWv8izw7bGF+qtv+wJ7yNwFBiXPFfBz2Wynd5ul6\nK9T78GOcBozPBEXChx68m4YlTcSHT7RJ5/k1+FCPJbW1pN3lE7ePSPsqNH/pR19Di/O9kIZvPgWc\nmB4o2+Fj2rNOwN/CMzpdW0hqK+k6fF5VbkL1dPyhqXfKe08g26uVf59E0tvy4UiFxL03WVbvvfnM\nbDp+v/mNpKMLbYO/MapnKmsTec/oryn8nf0osHk6nk3lQ7nWK7BdFRHwh6VK+tLZCm99mYxPLrsP\n707fwRb+cZX+wGf4zfxdvNVjtwIBTLnexl8B+BHe/XatmeW+6I7Fu+Zm4r0Pv8UDk8dTIF3KnXhw\n8Iik7/BhBPfjry/Mdxr+Grln8XGGvwS62xL8QZnUMrkv/uaAz/Fu4WvwV6iVYyjeknIu3prwLX6T\neY30NgAz+19KdzT+hpIb8IDlffzLdBT+hXuQpD+aT8bqi79Z6At8YvFeRcp3JP7gd2xqbdsbfwPI\n5/hY3GdY+Aq4K/A6nYS3qL6Gt5ZkFZs4NlveKlVUGtN5aNrf1yxsSTw2/X8LPsHurrT+PODXRcaG\nl2Mafu7/C3/P9qMUOF/KqBfwd8ufhX8RzcTHfR4I7GdmtxbY99/wh+vX8SBpMjAID3xuxyenroof\n4y+BgfjrbadXs64sqTVsH/zL6xP8+p6GByjFrESRFlXzye074Ofwo/jx+Qwf/nCsmd2WNr0P787/\nAG8tre6aH4wHx//Ej9N7qbz5+5+KT9y9Tj4kprpz5YBUlmn4+98L/Z2EYpN27ylRRznD8PvLi5K+\nxd/Kc7EtnKR5MH7vfQ9/w01rPOhYUiPwe/UX+Dl4kBX+Q1E1dQ31p/zvhSPwsn6A3/OrtL6mfee+\ni16Uj9d+BW/J72xmb6Xt5uONKQfj5/3RqSy5dPLvk+APWtkegayhxL03V3fL8r03/zNT8Hk918hf\n6Zm/fjJ+/p6LH6Mb8FenLvLAbf5ikePxc/YzfOjmIq+7zVdRWVkXf/ckhKWXqr4zOYSwjJH0mJnt\nXv2WIdS/9J3zmJm9UN95CT8d0cIfQghhmSXpl3hrdAjLih7A36vbKISaFG/pCSGEsMxKwxV+X9/5\nCKFcZtajvvMQfnpiSE8IIYQQQggNWAzpCSGEEEIIoQGLIT0hhKXKvHnzK2fO/Lb6DX8CWrdekagL\nF3WxUNTFQlEXC7Vp0yL/72qEsEC08IcQlipNmvzoP1LZYERdLBR1sVDUxUJRFyGUJwL+EEIIIYQQ\nGrAI+EMIIYQQQmjAIuAPIYQQQgihAYuAP4QQQgghhAYsAv4QQgghhBAasAj4QwghhBBCaMAi4A8h\nhBBCCKEBi4A/hBBCCCGEBiz+0m4IYanS65T76zsLIYQG5ObBO9Z3FkKod9HCH0IIIYQQQgMWAX8I\nIYQQQggNWAT8IYQQQgghNGAR8IcQQgghhNCARcAfQgghhBBCAxYBfwghhBBCCA1YBPwhhBBC+EkZ\nMuQ0NtxwHbp168TkyS9WWTdmzJ1sscWmdOiwBoce2o9vvvmGWbNm8dvfHkqHDmuw5ZYdufvu0fWU\n8xCWTAT8IYQQQvjJmDDhaW66aTh33XUPXbt2Y9CgAQvWzZ8/n1NPPZGjjjqGhx9+grFjH+Puu0dz\nww3X8vLLLzFp0mS6d+/JwIEnM2/evHosRQiLJwL+EEIIIfxkPPfcRNZaqx2dOnVmp5125Y03Xmfm\nzBkAVFRU0KxZM1q3bk3btmvQqFEjmjdvzqmnDuYf/3ibdu3WpkWLlrRsuRKNGzeu55KEUL74S7sh\nhBBC+MmYPn06LVq0BKBVq1YATJs2jdatV6ZRo0ZceumVHHPMkVRUVLD11tvQu3ffBZ/dYIO1adKk\nMbfeeicVFRX1kv8QlkSdBPyS9gBOB+YDzYH3gKOBTYFPzew/i5leB2CSmbVbwvx8BGwHdADuB/6e\nt8lNZjZqSdJewvx0oEh5JFUCy5lZ0b5DSRsBlwLrAN8C3wEDzewVSUOBJmZ2Zg3l9U7gFOBTYALQ\nGDgBOMzMjq+B9FcArgY2AeYBLYBLzOyutP4QM7t9CdIdidfxiLzl1dZvTZHUH7gF2MTM3s4sPxM4\n38zK/vaQdDvwpJmNLLFNybKl/OxsZoeUu98QQmjIZs2axZAhAxk4cAhdu3ajX7/9GDVqJP37HwHA\nU09N5KKLzuf444/m2WensNxyy9VzjkMoT60H/JKaArcDHc3sk7TsYuAIYGPgLmCxAv4a9rqZ9ajH\n/f8oKUB+DDjJzB5Iy3oAj0hSTe/PzA5M+1gb2NDMVk+rXqqhXQwAvjWz7TL7eVjSw8BKwO/x82lZ\n9S/gt8DAzLLewMf1k50QQvhpWW211fnqqy8BmDHjcwDatm0LgNlbTJ8+jX337U379h1Yd931eeGF\nZ2nfvgOzZ89mjz1+RZ8+/bjnnjF8+OH7rLfeBvVWjhAWR1208K+At+o3zy0ws0GS9gPOArpIOhn4\nHrgYmAOsCByTWqhXw1tFV8J7CI4FZuXSktQOD3gPAj4EbgDapO0vN7M7JK0O3I23Rr8MVNuSmlrd\nHwAeB7riLc2/AqYBIwABlcDfzezY9GBzHbBB2na0mV2eWlF3T/vshAerTYGeadnOmX1eCXROy/ua\n2X8z6wqmDxwMTM4F+6l+x0vaxMy+zMb8kv4AHJbqejbQz8y+kPQnYMdU9/8FDgc2BIZnjsd5Zvaw\npKkpzzcArSSNBy4Ezjaz7SStA/w5feZnwBAzezK1sM9J9XZwtmx5VgZaSKows0oz+xDYPOX/YWAz\nSbcB/VMeNgaaAS+a2QlpuyOAPwBzgafNbEh2B6nXY20zO6JIHgrWFdAH2NbM+qdt+gG9zayvpAuB\nbfHzfQIe0HfHz/HZwD34+fso0EfS6WY2X9L2wDtA65RmY+BKYEv8/BpnZmdJagTcBGwGvE/mepLU\nFzgeP2+mA78zs8+Lla1IebsCl6c6qwSOM7M3JW2JnwezgEeAc/HjeiawLtAe7/GZTuHjvh4wKqU5\nGdgT2MvM3l2c/IUQQk3p3r0nw4ZdwpQpkxk79jG22KITTZs2Y/bs2bRrtw5NmjRh/PhxbLPNdkyd\n+h577rkXEyY8zT33jGHTTX/OpEkTWXHF5rRtu2Z9FyWEstX6pF0z+xI4B3hV0pOSzpAkM7sXeBU4\nxczGAasCfzCzHYGrgFyQdhHwSGrxPRs4NJe2pJbA39LnXgMuAB5LaewAnCepDXAi8EJK41ag3Kt0\nU2Ckme2Q8toPD7i6mlk3M9smlWultI+Pzawn/oBwoKTNUzqd8eBxl1SGJ9Jn56RlAGsBf0l5HAec\nnJeXYun/nAKt62Y2s0B5VgB2NbPuwFTgEEmt8Yeobma2PR6Yrg4cCdyf9tcLWCUvrd8B01PvyPeZ\n5dfjD1o7AnsDIyTlHiybm1mPEsE++LHfCnhP0ghJfdLDDvh59LqZHYYHyK+Z2Q5m1hXYVVJHSe2B\nM4DtzawbsGa2p0PSb4BfAEeVyEPBugJGp/38LG3TN5WvD7CWmXU3sy74Q9leaZvOwKFmdlP6/Qtg\nCh74gj+43JHZb188kN4WP4d3ldQdf8jaONXNoakMuR6QM/ChOdsB41l47SyO24CT0/Eehj9cgh+P\nc1M9fIE/XOWsC/Q0s5cpftzPA+5KeRsLbLQEeQshhCXWpk2LKv/23nt3BgwYwMEH9+HNN19n+PAb\nOeywvhx//JF07LgBN954I1dffTm77daDXXbZmTPOGMQFFwylS5et6NlzGx555AFuvXUk7duvvkja\n9fkvhFLqZAy/mV0saQSwK96y/aKk0/M2+xS4TNLyeOt8LmDtigcgmNkEYEJqfW+CB/t3mNnEtG1P\nYCtJh6ff5+JByWZ4KyWp1+DLzH43S63UWYel/z8zszfSz+/jrc9vAZ9JegR4ELg7taT3BNql4Axg\neTzwA5hiZnPS3IFGwKS0/KNUVoAvzSwXuD+Hj4vPKpb+fLznohyf40N9fsDnL3xiZjMlPY7X6714\ncPaRpL8BI1MA/RDeSluOnngL/Tnp97nAaplylWRmH6QHma3wXodTgQskbZW36RfA2pKexx+c1sAf\nGjcGXjaz71J6/QFSzL8zsA2wkZnNryYrhepqlqT7gQMk/RV/IHwSuBboljmPVsLPu9c8CzYjL+1R\nwG8kPQXshPdGXJnWdcXH5lcC8yVNTHVRCTyXln8rKffi6G6p7I+nMjbD58iUTVIrYPXM+TceuDP9\n/Mv0O8Bf8V6VnBdSfqD4cf8lcAleEY9JmkUIIdSh6dO/XmTZ4MFDGTx46ILfx4x5cMG2vXr1oVev\nPgvWzZ3r/48YUXU0aaF061ME/aGUupq0u2IaYjAaGC1pDD58IBsIjQKONrNxkvbCAz3wQKdQT8TK\neEvpUZJGmNk3eOB3jJlNydt/BfBDZlE2QC44hj89VORPdqwws9nA9pI64a24L0naNu37PDP7a146\n/fPTyZtEmRte9EPeskqqKpZ+C2D/AvnfEg84c7+3Ay4Dfm5m0yRdlsnPAZI2xocsTZDU28yekdQR\nD0j74y3cB+Xvp4A5wP5m9llefqBqT0BBaU7CbDObDExO8z0m4sF69nw5EA+EtzezeZJyx7zY+QLe\ns/MOPgzqpiLblKwr4Eb83J0D3GlmP0iaAww3s8vy0ulRpMyP4oFzf+BxM/s+0wmRf9xz50Kxc3gO\nPqRrL5ZcsX2C12Vuv/kPSdmyFTvu2c+T93MIIYQQ6kCtD+mRtBvwfApMc9YD3sW//HNT3FcH3khj\nmPuwcOjAc/gYeCRtJ+nWtHyamZ0O3Ie/1QW85bxv2nYFSX9OwwrexFtCc2OVc0MylqQ8nSUdbmav\nmNl5+JyAjfL23UjSMEkrL0bSrdNDBPhwjtfz1hdLfzTwc0kLgvHUC/BXFvYegLe2fpYC2JXx3pZm\nktaTdLKZvZ3mBNwD/ELS8UA7M3sQn2DdtcxyZPO5qnxewuIYx8IeFvBjtSo+sTv/fLEU7G+J93Y0\nw4c3dUnDvZB0d1oPPmzlEOCs7DCfAgrWFb7DV/HhPsfhc0tyZd4/N3RJ0tmSNiyWuJnNxev5fBad\ngPwCsIukipRe97TsTWDrtLwFC49Hrrxt0777SNqnRNkK5edL4JN0bYA/XL2Qfn4b7xWBAg+WGcWO\n+4LPS9oFn38SQgghhDpU6y0NNgHgAAAgAElEQVT8Zva4/LWRT0n6Fm89/B8+brw/cKOkk/AJu+Pw\noTOXAqPS8rOAWyT1Skkel7eLc4CJ8omLQ/Gxw5PwAG14CgivAu6WNA54g6pvBSo0pOd5vCW3kH8D\n50g6Gp+M+W/gWeBFPPB+Hm99fcjMZpSOK6t4DzhM0qUp7wfkrb+uUPrgD0LAtZIG4UNdZgK7mdln\nmf2/CrwjaXKuDPi460eBLdLyr9Nnz8UfkEZL+irtb3CZ5TgBGC7p16kcF5RbAcmvgasy9bsC8Ccz\ne1XSKsDqkp7A33TzoKQJeP1fhj/4bY2fB09Kmoe/ivPlXD2Y2SfpYWa0pG5pn0/JX2EJ8AF+Xi5S\nV5IeNrNJeJC+t5l9kD5zT9rvc5LmA6/g59haJco5Cu8hmpS3fAweIE/C6/0+M3s2PQgfjJ9n7+Pn\nKGb2saQTgYfS9fUtPul6cR0GDEv5n48PMwLvabtW0sfAw3jLf6FW+mLH/Rzg9rT8eXwYW/x5yhBC\nCKEOVVRW5vfmhxCKScPDHgCuMbOx9Z2f2iafmzLDzP6ReqBGm1nZT7GSOgPLm9kk+duy3gZWS70c\nBfU65f64KYUQaszNg3es7yzUiTZtWsRfAgtFxV/aDXVO0o346znzPWZmf6rr/JQrBbwj8HH3y0Sw\nL3/97YmF1hWau1LAXLzXbDb+OtmjFzMLs/AeG3KfLxXshxBCCKHmRQt/CGGpEi38IYSaFC38IdTB\npN0QQgghhBBC/YmAP4QQQgghhAYsAv4QQgghhBAasAj4QwghhBBCaMAi4A8hhBBCCKEBi4A/hBBC\nCCGEBixeyxlCWNpUTp/+dX3nYanQpk0Loi5c1MVCURcLRV0sFK/lDKVEC38IIYQQQggNWAT8IYQQ\nQgghNGAR8IcQQgghhNCARcAfQgghhBBCAxYBfwghhBBCCA1Yk/rOQAghZPU65f76zkIIYRnx4OX7\n1HcWQlgmRAt/CCGEEEIIDVgE/CGEEEIIITRgEfCHEEIIIYTQgEXAH0IIIYQQQgMWAX8IIYQQQggN\nWAT8IYQQQgghNGAR8IcQQgihwRgy5DQ23HAdunXrxOTJL1ZZN2bMnWyxxaZ06LAGhx7aj2+++aae\nchlC3YqAP4QQQggNwoQJT3PTTcO566576Nq1G4MGDViwbv78+Zx66okcddQxPPzwE4wd+xh33z26\nHnMbQt2JgD+EEEIIDcJzz01krbXa0alTZ3baaVfeeON1Zs6cAUBFRQXNmjWjdevWtG27Bo0aNaJ5\n8+b1nOMQ6kb8pd0QQgghNAjTp0+nRYuWALRq1QqAadOm0br1yjRq1IhLL72SY445koqKCrbeeht6\n9+5bn9kNoc4sswG/pD2A04H5QHPgPeBoYFPgUzP7z2Km1wGYZGbtljA/HwHbAR2A+4G/521yk5mN\nWpK0lzA/HShSHkmVwHJmNq/IZ3sAF5jZdun3dYHHgUPMbHL6/DNAZeZjfzKzx4qk1x/Y2cwOyVu+\nM3CmmfUoUY5GwEXA9sD3QEvgFjO7Jq0/xMxuL/b5EukOBZqY2Zl5y6emvL67uGkuQR56AE8De2Tr\nTtIhwChgXTObWmZaFwDzzGxoiW2mUqJs6Zwx4Pm0aDlgInCemX2by9uS1HcIIdS3WbNmMWTIQAYO\nHELXrt3o128/Ro0aSf/+R9R31kKodctkwC+pKXA70NHMPknLLgaOADYG7gIWK+CvYa+XCmKXJZJW\nAx4Efm9mkzOrdir2wFDDfg0I2NbMKiW1Ap6Q9BDwAXA2fi4sq/4F/BbIPiwdnpbXh+m5c1fS8sDl\nwB3AvpLWAn7Psl3fIYQGbLXVVuerr74EYMaMzwFo27YtAGZvMX36NPbdtzft23dg3XXX54UXno2A\nP/wkLJMBP7AC3qq/YPCdmQ2StB9wFtBF0sl4i/DFwBxgReAYM3slBbG3ACvhPQTHArNyaUlqhwdg\nBwEfAjcAbdL2l5vZHZJWB+4GGgMvAxXVZTq1oD6At5Z3BVoAvwKmASPwwLYS+LuZHZsebK4DNkjb\njjazy1OL+e5pn53wAKwp0DMt2zmzzyuBzml5XzP7b2ZdwfQz61vgwf5ZZjaujPI1B4YDa+Otw7eZ\n2fV52+wL/BH4CHinujSBlfFj1xhvwf4C2CqldSvQXtJYM9tV0nnATulzH+E9EnMl7QWcA8zGA+mj\n8/LUHzgQ6FWibPsBA1MaTYBDgQ2BMzIBclfgGjPrIul4oG/a9m3gGGB1vD5fB/4JPAe8CGwnaWUz\nmyFpHfxYfJLZ95nAXsDc9LkTUrn+mJZ/CHwDvJW275nKW5E+c6SZvVd9VVdlZrMlnQS8I2lT4Hpg\nM0m3ATfj19ps4B68R6LguSTpQmBb/LqdAAw0s8r8/YUQwo/VvXtPhg27hClTJjN27GNssUUnmjZt\nxuzZs2nXbh2aNGnC+PHj2Gab7Zg69T323HOv+s5yCHVimZy0a2Zf4gHNq5KelHSGJJnZvcCrwCkp\nQF0V+IOZ7QhcBQxJSVwEPJKGrJyNB28ASGoJ/C197jXgAuCxlMYOwHmS2gAnAi+kNG4F1iwz+5sC\nI81sh5TXfsBmQFcz62Zm26RyrZT28bGZ9cQfEA6UtHlKpzNwGLBLKsMT6bNz0jKAtYC/pDyOA07O\ny0up9JsC9wFvpHotxwnAF6lsOwKDJK2Xt821wAFmthvwQxlp3gY0Az6SdLuk/pJ+ltadg7dI7yqp\nCfAtsL2ZbQu0AnaTtCL+MLWnmW0PfIYHnwBI2gXvGeptZnNL5KMV0C/V1SPAccCTwFppyBN4gD9C\nUhdgP2AHM+sGfAH8Lm2zCXCumV2Yfv8BP98OTr8fDtyZyV83oHcq1/b4g+dBkjZKn+kC7Is/fJDK\newOwv5l1B64BLitRrpJSnUzBz9Fz8N6rw9LqzsChZnYTRc4lSX2Atcysu5l1wR8I4hs2hFBj2rRp\nseDf3nvvzoABAzj44D68+ebrDB9+I4cd1pfjjz+Sjh034MYbb+Tqqy9nt916sMsuO3PGGYOqfH5Z\n/hdCKctqCz9mdrGkEcCueMv2i5JOz9vsU+CyNDRhJWBmWt4VGJbSmQBMSK3vTfDg6w4zm5i27Qls\nJenw9PtcYF08ABqe0nhF0peZ/W4maXxeXnJB0mdm9kb6+X28Bfst4DNJj+AtwHeb2ZeppbadpO5p\n++XxgAlgipnNSXMHGgGT0vKPUlkBvjSzl9LPz+EBeVax9GcAHYEBwEBJ3VM9ZT2VxvLnHI/X68hU\nJ99JmoL3QAAgaRVgBTN7Ky0aB2xOCenhrrukjnjPxcHARZK2zttunqT5wERJ8/ChXaviD1gfmtn0\ntN2glJee+DE8CtjMzKp7GfP/gFvTnIK2wPNpiNEI4HBJ5wJ7AOfiw142AJ6WBN4TlXuYmGFmlpf2\nKLzerknl644H8eB1OiHzMDIe7+GYBbxsZnNSeZ5J6zsCawD3pH03pupciyWR6wnLZ2Y2I/1c7Fzq\nCXTLXA8r4ddPCCHUiOnTv67y++DBQxk8eOiC38eMeXDBdr169aFXrz4L1s2du+jnl1UR9IdSltmA\nX9KKZvY5MBoYLWkMPt54RmazUcDRZjYuDes4NS2vpHDvxsp4a+ZRkkakIHAOPhRoSt7+K6jaQt04\n83PBMfzpoSJ/3HuFmc0GtpfUCW/9fEnStmnf55nZX/PS6Z+fTt54+tzwoh/yluUHfsXS7wG8YmY3\nSHoF+Jukbc3sg8xmi4zhz3sAKLTPUnVWkKTlgEoz+yc+nOVKSX/BW73vyWy3LT4WvrOZfSMpV6Zi\nxxo8IB2Pt9afVU0e7gI6mdk7ko7DW7fBh4ZNwIdpvWhmX0maAzxgZsflpdMBH2ZWhZm9JqmxpCOB\n983sfylYz+U/K1enxepyDvBBTc0hST0GvwReAdbJW50tS7FzaXtguJktcS9DCCGEEH6cZXJIj6Td\ngOfTGPOc9YB38SBoubRsdeANSY2BPvjQEPDW7t1TWtulseAA08zsdHwoy9Vp2SR8qAaSVpD05zR8\n5E2gW1reFcgNM1mS8nSWdLiZvWJm5+FzAjbK23cjScMkrbwYSbdODxHgw1hez1tfbfppou75wL2S\nVqhmfy8Au6X0mgNbprLkfA7Ml7Rh+n1nqncbcEbulxR8r03hYz01Bfvtga3x4/02PuymXfr8MEn7\npM/cC/wG6J1pmS6kRdrX1NRbtE9KGzObBrwGXArclLZ/FtgjN/RI0jFpaE4po4A/seiE2BeAnqnc\n4HMUXsB7hTpJaprW5fL/L2DV1COCpB0kHVXNvgtK6V6NDxf7D1XrO1+xc2kSsH+6ZpB0dub4hxBC\nCKEOLJMBv5k9jo/LfkrSeEkT8EDoWOAJ4EZJ++MTdsfhw2RGAmvLJyGeBfRIwyAuxHsGss4BNpXU\nFxgKbChpEv4qyr+nlu2r8EBsHHAIVd8KtFnKV/bfRSWK9G/gAEnPpfS+wIPG64BZkp7Hg7wvMkMo\nyvEecJikp/CA8Iq89WWlb2bD8fkG/1fN/q4BWqR6HYe3+E7NpFMJnATcJ+lB4LsyynAsXv8vprqZ\nCDxkZg8AHwOfSnoZeApomY7TEPy4nYHPYzgC76V4BlgFeDiTp2/w43dz5mHnL5njNjbVyR3AS3hL\n/6XAjvLx6eBzOFYxs0kpzSl43Y5P+ekB/KOact6BB9NV5kuY2Yv4mP6Jkp7FJ+iOTsPC7sMn/Y7B\njw9m9l0qz03pujgf74EoV5tU7on4q2W/wntOAN4AVpf0RIHPFTuX7sHP5efSutWp3zdohRBCCD85\nFZWV8bKMEH4MSdcB/0gPRuFH6nXK/XFTCiGU5cHL92kwY/B/rDZtWlT7tsDw07XMjuEPDUea8Fpo\nSM2rZnZSXeenXJLWxFvk38Z7nJZ6aWhRsd6mA83s07rMTwghhBBqX7TwhxCWKtHCH0IoV7TwLxQt\n/KGUZXIMfwghhBBCCKE8EfCHEEIIIYTQgJUV8KfX7G2Xe51hGa9nDCGEEEIIISwFqg34JW0JfIC/\ncvG6tPgmSb+pzYyFEEIIIYQQfrxyWvhvBvqY2RZAbmbMCcAptZarEEIIIYQQQo0oJ+Bf3syeTz9X\nApjZZ0DjWstVCCGEEEIIoUaU8x7+TyT1N7ORuQWSegPxvu4QQo2L1+wt1KZNi6iLJOpioaiLEMLi\nKifg/wNwn6RhQHNJ04CPgF/Xas5CCCGEEEIIP1q1Ab+ZvSVpY2BjoBXwsZm9X+s5CyGEEEIIIfxo\n1Qb8klYC9gfWJI3blwSAmZ1Xm5kLIYQQQggh/DjlDOl5FA/03wDm1252QgghhBBCCDWpnIB/NTPb\noNZzEkIIIYQQQqhx5QT8j0na3swm1npuQgg/eb1Oub++sxBCaKBuHrxjfWchhHpRTsD/FPCopG+B\nWdkVZrZereQqhBBCCCGEUCPKCfj/DAwCXifG8IcQQgghhLBMKSfg/9jMrqv1nIQQQgghhBBqXDkB\n/y2SrgfuA6r8aT8ze65WchVCCCGEEEKoEeUE/Kek/3fPW14JxBj+EEIIIYQQlmLl/KXddQstlxTB\nfgghhBBCCEu5RuVsJGktSdtJ2iH92xOI13SGEEIIYZk2ZMhpbLjhOnTr1onJk1+ssm7MmDvZYotN\n6dBhDQ49tB/ffPPNgnUTJ05gtdVacsklF9Z1lkNYbNUG/JJOBv4N3A6MBe5IP99Wu1kLIYQQQqg9\nEyY8zU03Deeuu+6ha9duDBo0YMG6+fPnc+qpJ3LUUcfw8MNPMHbsY9x99+gF6848cxCtW7eur6yH\nsFjKaeE/DtjEzDoA75lZO+Bk4IPazFgIIYQQQm167rmJrLVWOzp16sxOO+3KG2+8zsyZMwCoqKig\nWbNmtG7dmrZt16BRo0Y0b94cgJEjb2KttdqxySY/r8/sh1C2cgL+783svez2ZnYrcHSt5SqEEEII\noZZNnz6dFi1aAtCqVSsApk2bBkCjRo249NIrOeWUE/jFL8TWW29D7959mTlzBsOGXcJ5511Ub/kO\nYXGV85aeqZKuBU4EPpR0FPAKsGqt5iyPpD2A0/E//tUceA9/6NgU+NTM/rOY6XUAJqUeiyXJz0fA\ndkAH4H7g73mb3GRmo5Yk7SXMTweKlEdSJbCcmc0r8tkewAVmtl36fV3gceAQM5ucPv8M/mamnD+Z\n2WNF0usP7Gxmh+Qt3xk408x6lChHI+AiYHvge6AlcIuZXZPWH2Jmtxf7fIl0hwJNzOzMvOVTU17f\nXdw0lyAPPYCngT2ydSfpEGAUsK6ZTS0zrQuAeWY2tMQ2UylRtvzjHkIIYaFZs2YxZMhABg4cQteu\n3ejXbz9GjRrJ22+/yf7792GDDTas7yyGULZyAv7D8SBtvqQheGDSBji7VnOWIakpPm+go5l9kpZd\nDBwBbAzcBSxWwF/DXi8VxC5LJK0GPAj83swmZ1btVOyBoYb9GhCwrZlVSmoFPCHpIXwY2dn4ubCs\n+hfwWyD7sHR4Wh5CCKEOrbba6nz11ZcAzJjxOQBt27YFwOwtpk+fxr779qZ9+w6su+76vPDCs0yZ\n8hKffPIxI0eO4Pvvv+fFF5+nSZMmDBgwsN7KEUJ1ynkt5zTghPTzZDwYq2sr4K36zTP5GiRpP+As\noEuaXPw9cDEwB1gROMbMXklB7C3ASngPwbHArFxaktrhAdhBwIfADfhDzUrA5WZ2h6TVgbuBxsDL\nQEV1mU6t7g/greVdgRbAr4BpwAi8LiuBv5vZsenB5jpgg7TtaDO7PLWY75722QkPeJsCPdOynTP7\nvBLonJb3NbP/ZtYVTD+zvgUe7J9lZuPKKF9zYDiwNrAccJuZXZ+3zb7AH4GPgHeqSxNYGT92jfEW\n7C+ArVJatwLtJY01s10lnQfslD73Ed4jMVfSXsA5wGw8kK4y/CzV54FArxJl2w8YmNJoAhwKbAic\nkXu4k9QVuMbMukg6Huibtn0bOAZYHa/P14F/As8BLwLbSVrZzGZIWgc/Fp9k9n0msBcwN33uhFSu\nP6blHwLfAG+l7Xum8lakzxyZGYZXFkkb4ed9o1SGwWY2Kb1+dxR+nk4G9kx52C793xoYlspW6LpZ\nBRiNX7vvAOsAF5rZk4uTvxBCqA3du/dk2LBLmDJlMmPHPsYWW3SiadNmzJ49m3bt1qFJkyaMHz+O\nbbbZjqlT32PPPfdi0KAzmTfP27+OO+4otthiSw4//Ih6LkkIpRUdwy/p7Gr+nVVXmTSzL/GA5lVJ\nT0o6Q5LM7F7gVeCUFKCuCvzBzHYErgKGpCQuAh5JQxfOxoO3XDlbAn9Ln3sNuAB4LKWxA3CepDb4\nkKYXUhq3AmuWmf1NgZFmtkPKaz9gM6CrmXUzs21SuVZK+/jYzHriDwgHSto8pdMZOAzYJZXhifTZ\nOWkZwFrAX1Iex+GTq7NKpd8U/2vKb6R6LccJwBepbDsCgwr8fYZrgQPMbDfghzLSvA1oBnwk6XZJ\n/SX9LK07B5iegv0mwLfA9ma2LdAK2E3SivjD1J5mtj3wGbBtLnFJu+A9Q73NbG6JfLQC+qW6egSf\nvP4ksFYa8gQe4I+Q1AXYD9jBzLoBXwC/S9tsApxrZrn3tv2An28Hp98PB+7M5K8b0DuVa3s8gD4o\nBeQHA12AffGHD1J5bwD2N7PuwDXAZSXKVcw1wPXpYeYPLHwL13nAXemcGgtslPnML/F6fpji183J\nwD/TMboMf1AIIYR60aZNiyr/9t57dwYMGMDBB/fhzTdfZ/jwGznssL4cf/yRdOy4ATfeeCNXX305\nu+3Wg1122ZkzzhhEly6/YJtttmSbbbakZcsWrL32mmy8cYdF0q7rfyGUUqqFv1gLdgXeOroecH6N\n56gIM7tY0ghgV7xl+0VJp+dt9ilwmaTl8VbGmWl5V7wVEjObAExIre9N8ODrDjPL/V2BnsBWkg5P\nv88F1sWD9OEpjVckfZnZ72aSxufl5bD0/2dm9kb6+X28Bfst4DNJj+AtwHeb2ZeppbadpO5p++Xx\n1niAKWY2J80daARMSss/SmUF+NLMXko/P0fqmckolv4MoCMwABgoqXuqp6yn0lj+nOPxeh2Z6uQ7\nSVPwHggAUuvuCmb2Vlo0DticEtLDXXdJHfGei4OBiyRtnbfdPEnzgYmS5uFDu1bFH7A+NLPpabtB\nKS898WN4FLCZmX1Daf8Dbk1zCtoCz6chRiOAwyWdC+wBnAv8Hq/HpyWBt2bnHiZmmJnlpT0Kr7dr\nUvm640E8eJ1OyDyMjMd7OGYBL5vZnFSeZ9L6jsAawD1p342pOteiXF3xh1HM7HVJLSWtigf1l6Tl\nj0malfnMK7n8UPy6+SULr5t/SsqvixBCqDPTp3+9yLLBg4cyePDQBb+PGfPggm179epDr159Fqyb\nO7dqGtlt61sE/aGUogG/mZ2bvyy1Pl6Jj6XuXYv5WoSkFc3sc3x4wGhJY4DL8WA1ZxRwtJmNS8M6\nTk3LKyncm7EyMAU4StKIFATOwYcCTcnbfwVVW6gbZ34uOIY/PVTkj3uvMLPZwPaSOuHDIl6StG3a\n93lm9te8dPrnp5M3nj73cPZD3rL8wK9Y+j3w4O0GSa8Af5O0rZllX726yBj+vAeAQvssVWcFSVoO\nqDSzf+LDWa6U9Bf8fLsns922+Fj4zmb2jaRcmYoda/CgfDzeWl+0hyrl4S6gk5m9I+k4vIcFfGjY\nBHyY1otm9pWkOcADZnZcXjod8GFmVZjZa5IaSzoSeN/M/peC9Vz+s3J1Wqwu5wAf1MAckmL7bZS3\n3+zP2bIVu27yPz//R+YzhBBCCItpcf7S7h3ATXjAuGum1brWSdoNeD6NMc9ZD3gXDyaWS8tWB96Q\n1Bjogw8NAW/t3j2ltV0aCw4wzcxOx4eyXJ2WTcKHaiBpBUl/TsNH3gS6peVdgdwwkyUpT2dJh5vZ\nK2Z2Hj4nYKO8fTeSNEzSyouRdOv0EAE+jOX1vPXVpp/maZwP3CtphWr29wKwW0qvObBlKkvO58B8\nSblXGexM9W4Dzsj9koLvtSl8rKemYL89sDV+vN/Gh920S58fJmmf9Jl7gd8AvTO9HIW0SPuamnqL\n9klp5+a0vAZcil8PAM8Ce+SGHkk6Jj0clzIK+BOLTkB+AeiZyg0+R+EFvFeok6SmaV0u//8CVk09\nIsj/EvZR1ey7kOyx3AL4PD1gvw1sk5bvgtdNIcWum+znN8V7YkIIIYRQh0oG/OmL+1x8ouFkYPM0\nXrdOmdnj+LjspySNlzQBD4SOBZ4AbpS0Pz5hdxw+TGYksLakk/DW3B5pGMSFeM9A1jnAppL6AkOB\nDSVNwl9F+ffUsn0VHoiNAw6h6luBNkv5yv4r9YLefwMHSHoupfcFHjReB8yS9DwegH1hZjNKpJPv\nPeAwSU/hAeEVeevLSt/MhuPzDf6vmv1dA7RI9ToOfxicmkmnEjgJuE/Sg8B3ZZThWLz+X0x1MxF4\nyMweAD4GPpX0MvAU0DIdpyH4cTsDn8dwBN5L8QywCrDgnE29OIcAN2cedv6SOW5jU53cAbyEt/Rf\nCuwoKdeveyuwiplNSmlOwet2fMpPD+Af1ZTzDvzhpcp8CTN7ER/TP1HSs/gE3dHpAfs+/Focgx8f\nzOy7VJ6b0nVxPt4DsbiOB46U9DR+XHPzXM4Bjk3Le+JDyAq9rWkoha+bYXjdTcTnkLxc5PMhhBBC\nqCUVlZWFh/tKOhgPHh4GzjazmQU3DOEnRtJ1wD/Sg1GDJqkzsHx6Y8/qeIv9atVMeM5+XsB6ZvZo\n6jH6N9DFzD4q9plep9y/JHMQQgihWjcP3rG+s1Br2rRpUe3bA8NPV6lJu6Pw1ryOeGvpIhukN3KE\nsFhSr1GhITWvmtlJdZ2fcklaE2+RfxvvcVrqpaFFxXqbDjSzT6tJYhZwVbr+m+JzZMoK9pMvgQGS\nzsbvN38qFeyHEEIIoeaVauEvNcYZWPDGmxBCqDHRwh9CqC3Rwh9+qkq9pSeC+RBCCCGEEJZxZb2l\nJ4QQQgghhLBsioA/hBBCCCGEBqzagD/3PvMQQgghhBDCsqecFv4naj0XIYQQQgghhFpR6rWcOXdI\nugF4CKjyR5rM7LlayVUIIYQQQgihRhR9LWeOpPeKrKo0s/VqPkshhJ+4yunTv67vPCwV2rRpQdSF\ni7pYKOpioaiLheK1nKGUalv4zWzdushICCGEEEIIoeaVM6QHSXsC+wMrmtlBknYFJpnZt7WauxBC\nCCGEEMKPUs5bek4HzgdeB7qmxVsBN9ZivkIIIYQQQgg1oJy39BwJbGdmVwHfp2UXAp1rLVchhBBC\nCCGEGlFOwD83/QPIzfCNiSEhhBBCCCEsA8oZw/848IikPwMrpPH8R6blIYRQo3qdcn99ZyGEsIy5\nefCO9Z2FEJZq5bTwnwpMBE7Hh/ScBkxI/4cQQgghhBCWYuW8lvN7fNLu+bWfnRBCCCGEEEJNKhrw\nS3rUzPaQ9A4Lx+5XYWYb1VrOQgghhBBCCD9aqRb+s9P/v6uLjIQQQgghhBBqXqmA/0agE3ClmW1R\nR/kJIYQQQggh1KBSAX8LSf/f3p3HW1XV/x9/XUQQCRQTxURF095mWIEDgwPgLIljgKUiWVqZsyYI\nmmQ2mENmNsBXCyVFpcwhZ0UQREQc0p/oJystLQ3MERAUvL8/1jpwON5z7mU6Fw/v5+NxH9yz9j5r\nr7X2PpfPXnutdSYD20q6t6EdImK/1VMsMzMzMzNbFSqt0rMfcBXwLnBdmR8zMzOzNc6IEd9lu+22\npFev7syY8egy2yZMuIFu3XagS5fNOOaYwcybNw+AP//5NrbddgsOPbR/cxTZbLUp28MfES8CL0r6\na0Q8Wm4/MzMzszXJ5MkPcvXVY7jrrge49trfMWzYGTz44MMALF68mLPOOpXhw89jzz37stdeu3HT\nTePZZptPc955w+nUqVMzl95s1Wt0lR7g95K8So+ZmZl9LEybNoXNN+9M9+478+9//5vrrx/Hm2++\nQYcOG1FXV0fr1q3p0C6WYucAACAASURBVKEDnTptRosWLWjbti2dO3fmgQemcNppJ/HOO283dxXM\nVimv0mNmZmY1Zc6cObRr1x6ADTfcEIDZs2fTocNGtGjRgosvvpwTTzyeuro6evbszRFHDGKdddZp\nziKbrVaVhvQ8ln99HOgVEfdJ2oD0zbv1wGWrsiCSDiR9m+9ioC3wIvBNYAfgtYj4x3Lm1wWYGhGd\nV7A8rwC7A12AW4EnS3a5OiLGrUjeK1ieLpSpT34Cs25ELCrz3r7AhRGxe1PyK9pnKLBPRBxdkr4P\ncG5E9M2vjwTOIF0XbUht9d2ImN1Inb4K3BARH1bYpyPwK2DTnP96wDkRMVHS+sABEXFzpeOUyXcS\nqU3uL0rrwkpcMytQhlHAMGCziHirKP0qUrt3WY68ppLOyaQy27vQtPN9EfAcUJd/xkXE/+XtnwK2\nj4iJTS2XmdmaZu7cuYwYcTZnnz2CHj16MXjwYYwbN5ahQ7/e3EUzW20a/aZdYCzwLHAf8EtSMP58\nTj90VRRCUivg90DXiHg1p10EfB3YHrgRWK6AfxV7phDc2rIk7Q+MBA6MiFdy2lnAzaQbpkq+D9wE\nlA34gR8B0yLiZznvnYArJfUGugGH52N9XP0L+CrppoZ8E/OFZizPfYUbPEmbARMktY2Iy4F+wGcB\nB/xmtkbbZJNNlwzLeeON/wEsGZsf8Rxz5szm0EOPYKuturD11p9m+vSHHfBbTWtKwP/5iPhyDkQO\nBraMiLckPbsKy9GGdCPRtpAQEcMkHQacB+wq6XTgfVIP5EJgfeDEiHhC0ibA74ANSE8IvgPMLeQl\nqTNwNymwehn4DdAx739pRFwvaVNS8LkO6alGXWOFzr2mtwH3AD2AdsCXgNmkFY5E6pV+MiK+k29s\nfglsm/cdHxGX5p7VA/Ixu5NuflqRAqw6YJ+iY14O7JzTB0XEv4u2NZh/E+rRFhgDbAGsC1wbEb8u\n2edQ4IfAK8ALRZvOB84uBPsAEXGJpK/kJwGLKHq6IGksMDUfa1vgAUmHRcQbZYq3EdC+KO/HgV6S\n2gBXAx0k/ZR083Bt3r8dMCEiLsrHPBc4hHRjMS4iriyp2+9IT5SurdBGFwB755evAEcDo4APImJU\n3mcY8EngXMqf54OADix9QvYn4GvkgB84AngQGJTzbPDc5M/jDaTr+AXSk49CWU/O729Jujk/sVy9\nKomIVyV9DZgi6VbS+a+T9AbpnGwNbAWcCczJdVgf+AQwIiLul9SBBj5vK1IeM7Om6tOnH5dd9lNm\nzpzBvffeTbdu3WnVqjULFiygc+ctadmyJZMmTaR379156aUX6d//IObOfZfZs2czf/58Fix4j3/8\n4+9sttmnaNOmTXNXx2ylVVqWs6DQ+7o/MLNo6EFTbhaaJCLeJgWOT0m6X9JISYqIPwFPAWfmYQQb\nA9+OiL2AnwMjchY/Bu7MQeX3gGMKeUtqD/wxv+9p4ELg7pzHnsAFedjIqcD0nMc1wKeaWPwdgLER\nsWcu62BgR6BHRPSKiN65XhvkY/wnIvqRbhCOlPT5nM/OwBBg31yH+/J7F+Y0gM2B63IZJwKnl5Sl\nUv6VnAK8leuwFzBM0jYl+1wJfDki9mfZHvluwIwG8pxOunlpUEScn3/du0KwD/AD4DhJz0m6UlJ/\nSS0i4j3gJ6R2OhvYBLgl1303YISk9pL2IAXZPUlPHPaTtGEhc0nfB+ZGxAXlCiCpJTAf2CMidgM2\nJH0e/g84WlLh5nAg6Sak0nn4ItA/Iu7Ir5/Px+iaXw8FigPicufmaOC9iOhFGhbUNeezK3AYsGfe\n9hYrMQ8nIl4gfdbnk57qjYuIws3K1kC/fBP2a1IwvxepY+Cq3G7lPm9mZqtMx47tlvk5+OADOOOM\nMzjqqIHMmvUMY8aMZsiQQZx88vF07boto0eP5oorLmX//fuy7777MHLkMCZPvpeePbvx0EMP8sQT\nj9OzZzf+8Y9ZH8l7Tf0xq6QpQfsUSfeTHuWfDEt6TGetyoJExEV57PJ+pJ7tRyWdU7Lba8AlktYj\n9Ra+mdN7kHtMI2IyMDn3vrckBfvXR8SUvG8/YBdJx+bXH5AClx1JPankpwbFU/R3zGO+iw3J/74e\nEYWnHf8k9TA/B7wu6U7gduCmiHhbUj+gs6Q+ef/1SL3AkG6mFua5Ay1IveCQepM3yL+/XTS3Yhop\nGCxWLv83GqjDekW/9yAFc0TEe5JmUhSsS/ok0CYinstJE4FCADuP8jeOlYbqNElEPJUD3N1J9buY\nFMz3Kdl1NrCHpG+TngStRzoXPYApEbGY9PTn4FwnSMH19sCujZRhkaTFpM/CovyejSPiJUkvAH0k\nvQzMj4ho5Dw/ERELSw4xjnRTcznQMde5sK3cudmRfI3knvjn8/5987EezHm0JV3jK0RSC1KP/eIG\nNk+PiMIKXv1IX9ZXuJH7gHQTVu7zNmdFy2RmVmrOnHc/kjZ8+CiGDx+15PWECbcv2XfAgIEMGDBw\nybYPPoAvfekIZs8+okl5r4kc9FslTQn4v0XqzZxTFGy+QhqysMpIWj8i/geMB8ZLmgBcSgpWC8YB\n38wTNg8iTSCGNGymoaBzI2AmcIKkqyJiHqnH/MSImFly/DqWDVCLp+s3OIY/31SUTpSti4gFpOCz\nO6l3+TFJu+VjXxARfyjJZ2hpPiUTcAs9yB+WpJUul1ou/76ldShM4swvS/MpzbtS2zwN9CINbSq2\nC2nuRenQqFYsh3xdzAcKN3I/JA1hKR3nfhrQGtgtIuolvZ7Ty10b5P1bkXrO7y+zD/ncHQfsHBHz\nJBW372jSE6W/kXr3ofJ5fr+BQ9wAPEG61seXbCt3bsqdk4XAbRFxUsmxu5SpXmN2Bv4bEa8X3YQU\nFNdlIXB4RLxevIOkBj9vZmZmVj1NGdLThjRO+bE8ROIHwDZ8NBBZYXni5yOSim9PtyEFUR+Sxi5D\nWqnlWUnrkIZPtM7p00hj4JG0u6RrcvrsiDgHuAW4IqdNZen46DaSfpWHHswiBa5I6kHq1VzR+uws\n6diIeCIPFXkc+EzJsVtIukzSRsuRdYd8EwFp2MozJdtXNP/ppJu6wpjxnXKZC/4HLJa0XX69T9G2\nC4GLJG1ZSJB0EmkYykPAO8DmkuryuPMeRe+tZ+m5/Yh8np/PNywFG5OC9Ff46LUxKwf7B5PGkrcm\nXRt7S1pXUktJDypNRoUUrB8FjGlkmMmmwEs52N+KNDyocO39mfSE4GBgQk5brvOQVzN6ijQUqPQb\nrMudm+LrdQvSfBGAh4EDJX0ibztRUq8KdStLaV7LlaSx+7Bse5cqrvPG+WlFaXrx583MzMyqpCkB\n/1iWrrbyK9JY4XVz+ioREfeQJrk+IGmSpMmkCZLfIa0ONFrS4aQJuxNJw2TGAltIOo00sbevpIdI\nq7qUTlQ9H9hB0iDSRMvtlJYxfIg0oXYRaU5AP0kTSeOji1cF2jGXq/jnxxWq9Hfgy5Km5fzeIgVi\nvwTmSnqEFMi91cj49VIvAkMkPQD0AX5Wsn1F8/8FaTjGQ6T2vSAiXipszMM2TgNukXQ78F7Rtomk\noUUTJE2X9CRpXP/gvMtfSE8BniDNjZhWdNy7gZmSPt1QofIwnEOAcyVNyfWeAByfg+QZwJ6Sfgv8\nFhia23trUuB8XUQ8QhrWNYUUfN5SWAkqH+MZ0nCwsaRe844l5/mnwL1A+3zNjCBdQyMlfSZfO3cB\nf8lPImDFzsM40g3Lv0rSy52bccDGkqaQAvIZuT4z8/En5fL2JZ2Dpto31/sR0mT0qyNiTN42Bfha\nvukvdQpwWC7PnSxdyWcUDX/ezMzMrErq6usrd9RL+mtEfCb3zr5G0So9EfG5qpTSbA2ltDLSVGBo\nRKzSeS1rqwFn3rrKnh6a2drht8P3au4iNLuOHds1urqgrb2a8mh9ta/SY2s3pSUfN2hg09iIGFvl\n4jSZ0pfFXQSM+bgE+0qTmgc3sOm1iDiy2uUxMzOz1a+pq/TcR1p+crWt0mNrr4g4pLnLsCIi4i7S\ncJ6Pjfz9Cr9udEczMzOrGSuzSs+V5d9iZmZmZmZrgkYD/jxx8s6S5AnAJNLSi2ZmZmZmtoZqNOCX\ntDfwG9LKJ8UTQqavrkKZmZmZmdmq0ZRlOX8OXABsR1qqUqRlL4etxnKZmZmZmdkq0JSAf52IGBcR\nLwKLIuJvpLXIS9e6NzMzMzOzNUxTAv55kgZKqiN9mdCO+X2dVm/RzMzMzMxsZTVllZ6TgdHAH0jf\n+vkYMJeP2XKEZvbxcPulhzBnzrvNXYw1QseO7dwWmdtiKbfFUm4Ls6Zpyio9jwCfzy+vlXQ/0DEi\n/rJaS2ZmZmZmZiutbMAvaUSlN0r6UkT8aNUXyczMzMzMVpVKPfzbNfLe+lVZEDMzMzMzW/XKBvwR\n8TUASXURsSS4l9QmIt6rRuHMzMzMzGzllF2lR1J7SZOB/iWbRkr6s6RWq7doZmZmZma2sioty/kj\n4K/AfSXpo4DZ+V8zMzMzM1uD1dXXNzwUX9ILwOcbGr4jaX3gsYj43Goun5mtZQaceavnB5nZKvfb\n4Xs1dxFWq44d29U1dxlszVWph39RubH6ETG/kfeamZmZmdkaoGLAL6nBb9OV9Gngw9VTJDMzMzMz\nW1UqBfy/A/4kaZnlOSV9EfgT8OvVWTAzMzMzM1t5lZblvEzSpsBfJL0M/BfYHNgUuDgirqxSGc3M\nzMzMbAVV+uItImKYpJ8APYGNgNeB6RHxdjUKZ2ZmZmZmK6fRibcR8WZE3BUR10XEPQ72zczM7ONu\nxIjvst12W9KrV3dmzHh0mW0TJtxAt2470KXLZhxzzGDmzZvH3LlzOe64Y+jSZTN22qkrN900vplK\nbrb8vNKOmZmZrVUmT36Qq68ew4033kyPHr0YNuyMJdsWL17MWWedygknnMgdd9zHvffezU03jec3\nv7mSxx9/jKlTZ9CnTz/OPvt0Fi1a1Iy1MGs6B/xmZma2Vpk2bQqbb96Z7t13Zu+99+PZZ5/hzTff\nAKCuro7WrVvToUMHOnXajBYtWtC2bVvOOms4f/nL83TuvAXt2rWnffsNWGeddZq5JmZNU3EMv5mZ\nmVmtmTNnDu3atQdgww03BGD27Nl06LARLVq04OKLL+fEE4+nrq6Onj17c8QRg5a8d9ttt6Bly3W4\n5pobqKvzd13Zx0PVAn5JBwLnAIuBtsCLwDeBHYDXIuIfy5lfF2BqRHRewfK8AuwOdAFuBZ4s2eXq\niBi3InmvYHm6UKY+kuqBdSOiwWeHkvoCF0bE7k3Jr2ifocA+EXF0Sfo+wLkR0Te/PhI4A6gH2pDa\n6rsRMbuROn0VuCEiyn5ng6SOwK9Iqz/VA+sB50TExPyNzgdExM2VjlMm30mkNrm/KK0LK3HNrEAZ\nRgHDgM0i4q2i9KtI7d5lOfKaSjonk8ps78IKnm8zM1tq7ty5jBhxNmefPYIePXoxePBhjBs3lqFD\nvw7AAw9M4cc//gEnn/xNHn54Juuuu24zl9iscVUJ+CW1An4PdI2IV3PaRcDXge2BG4HlCvhXsWcK\nwa0tS9L+wEjgwIh4JaedBdxMumGq5PvATVT+krYfAdMi4mc5752AKyX1BroBh+djfVz9C/gq6aaG\nfBPzhWYtkZnZWm6TTTblnXfSGiRvvPE/ADp1St81GvEcc+bM5tBDj2Crrbqw9dafZvr0h9lqqy4s\nWLCAAw/8EgMHDubmmyfw8sv/ZJtttm22epg1VbV6+NuQevXbFhLykp+HAecBu0o6HXgfuAhYCKwP\nnBgRT0jahPRFYBuQnhB8B5hbyEtSZ+BuUmD1MvAboGPe/9KIuD5/p8BNwDrA40Cjz+Fyr+ltwD1A\nD6Ad8CVgNnAVIFKv9JMR8Z18Y/NLYNu87/iIuDT3rB6Qj9mddPPTCuiX0/YpOublwM45fVBE/Lto\nW4P5N6EebYExwBbAusC1EfHrkn0OBX4IvAK8ULTpfODsQrAPEBGXSPpKfhKwiKKnC5LGAlPzsbYF\nHpB0WES8UaZ4GwHti/J+HOglqQ1wNdBB0k9JNw/X5v3bARMi4qJ8zHOBQ0g3FuNKvyNC0u9IT5Su\nrdBGFwB755evAEcDo4APImJU3mcY8EngXMqf54OADsBlOa8/AV8jB/zAEcCDwKCcZ4PnJt8Y3EC6\njl8gPfkolPXk/P6WwPPAieXq1RSS1gEuB3YiXc8TI+I8SXXAlaRleV8jfbZej4hzJb1DOj/rRMQp\nDZUpIt6TNDKn/xd4CviUnzCYWXPr06cfl132U2bOnMG9995Nt27dadWqNQsWLKBz5y1p2bIlkyZN\npHfv3XnppRfp3/8gJk9+kJtvnsAOO3yOqVOnsP76benU6VPNXRWzJqnKpN28lOf5wFOS7pc0UpIi\n4k+kIODMiJgIbAx8OyL2An4OjMhZ/Bi4MweV3wOOKeQtqT3wx/y+p4ELgbtzHnsCF+RhI6eSvkNg\nd+AaoKmf0h2AsRGxZy7rYGBHoEdE9IqI3rleG+Rj/Cci+pFuEI6U9Pmcz87AEGDfXIf78nsX5jRI\nX2x2XS7jROD0krJUyr+SU4C3ch32AoZJ2qZknyuBL0fE/izbI98NmNFAntNJNy8Niojz8697Vwj2\nAX4AHCfpOUlXSuovqUVEvAf8hNROZwObALfkuu8GjJDUXtIepCC7J+mJw36SNixkLun7wNyIuKBc\nASS1BOYDe0TEbsCGwP7A/wFH58AXYCApyK10Hr4I9I+IO/Lr5/MxuubXQ4Hriw5f7twcDbwXEb1I\nw4K65nx2BQ4D9szb3gK+UaF9m2IQsDWpXfcktWEf0g3QrvlnEEtviAA+QfpMnlKuTPlbur8F9CLd\nKPdcyXKama2wjh3bLfk5+OADOOOMMzjqqIHMmvUMY8aMZsiQQZx88vF07boto0eP5oorLmX//fuy\n7777MHLkMC68cBS77roL/fr15s47b+Oaa8ay1VabLpNvc/6YVVK1MfwRcVEeu7wfqWf7UUnnlOz2\nGnCJpPVIvfNv5vQe5B7TiJgMTM697y1Jwf71ETEl79sP2EXSsfn1B6RgZkdSTyr5qUHx9wnsmMd8\nFxuS/309Ip7Nv/+T1MP8HPC6pDuB24GbIuJtSf2AzjlYgtQrW3jWNzMiFua5Ay1IveCQepM3yL+/\nHRGP5d+nkYLBYuXyf6OBOqxX9HsPYGyu+3uSZlIUrEv6JNAmIp7LSROBQgA7j/I3hpWG6jRJRDyV\nA9zdSfW7mBTM9ynZdTawh6Rvk54ErUc6Fz2AKRGxmPT05+BcJ0jB9fakgLVSGRZJWgxMkbQov2fj\niHhJ0gtAn/xt0/MjIho5z09ExMKSQ4wj3dRcDnTMdS5sK3dudiRfIxHxqqTn8/5987EezHm0JV3j\nK6MHcH9E1AOLJU0BdsnbCm07T9LdRe+pAx5upExfAB6LiPkAkm4l3UCamVXdnDnvLvN6+PBRDB8+\nasnrCRNuX7LfgAEDGTBg4JJtH+S/sldd9fuKeTYnB/1WSTUn7a4fEf8DxgPjJU0ALiUFqwXjgG/m\nCZsHAWfl9HoaDjo3AmYCJ0i6KiLmkXrMT4yImSXHr2PZALV4La0Gx/Dnm4rSibJ1EbGAFHx2J/Uu\nPyZpt3zsCyLiDyX5DC3Np2QCbqEH+cOStPqSY5fLv29pHQqTOPPL0nxK867UNk+TemhvK8ljF9Lc\ni9KhUa1YDvm6mA8UbuR+SBrCUjrO/TSgNbBbRNRLej2nl7s2yPu3IvWc319mH/K5Ow7YOSLmSSpu\n39GkJ0p/I/XuQ+Xz/H4Dh7gBeIJ0rZd+U0u5c1PunCwEbouIk0qO3aVM9ZqiXBnWKSnD4pL9CnUt\nV6ZBjbzfzMzMqqAqQ3ryxM9HJBXffm5DCqI+JI1dhrRSy7N5TPFAUsAGqbf7gJzX7pKuyemzI+Ic\n4Bbgipw2laXjo9tI+lUesjGLFLgiqQdpSMKK1mdnScdGxBN5qMjjwGdKjt1C0mWSNlqOrDvkmwhI\nwyueKdm+ovlPJw1RKYwZ3ymXueB/pJ7d7fLrfYq2XQhcJGnLQoKkk0jDUB4C3gE2l1SXx533KHpv\nPUvP7Ufk8/x8vmEp2JgUpL/CR6+NWTnYP5g0x6M16drYW9K6klpKelDSZvk9o4GjgDF5WFc5mwIv\n5WB/K9LQk8K192fSE4KDgQk5bbnOQ17N6CnSUKDrSjaXOzfF1+sWpPkikHrVD5T0ibztREm9KtSt\nKaYD++Zz2BLok9OeB3oWndv9y7y/XJmeB3aS1Crne/BKltPMzMxWQLXG8N9DmuT6gKRJkiaTxgN/\nB7gPGC3pcNKE3YmkYTJjgS0knUaa2NtX0kOkVV1KJ6qeD+yQexRHAdspLWP4EGlC7SLSnIB+kiaS\nxkcXrwq0Yy5X8c+PK1Tp78CXJU3L+b1FCnp+CcyV9AgpYHqrkfHrpV4Ehkh6gBR0/axk+4rm/wug\nXW6/iaTe6ZcKG/NQjtOAWyTdDrxXtG0iaWjRBEnTJT1JGpYxOO/yF9JTgCdIcyOmFR33bmCmpE83\nVKg8VOQQ4FxJU3K9JwDH5yB5BrCnpN8CvwWG5vbemhQ4XxcRj5CGdU0hBeK3FFaCysd4hjQcbCyp\n57pjyXn+KXAv0D5fMyNI19BISZ/J185dwF8KQ1NYsfMwjnTD8q+S9HLnZhywcR5e88PcFuQnV78E\nJuXy9iWdg6bat6T+XyW1+d9I7Vdow4eBO0kTdWeS2nsaH33iVbZMeU7Nrfn9t+Ry+mspzczMqqyu\nvr70ab6ZFSitjDQVGBoRs5q7PNWkNBH9UNLKQfWSbiOtSFQ6LKnc+1uS5lGMy/NXrgBejYhKN9MM\nOPNW/1Eys1Xut8P3au4irFYdO7bzt4BZWf6mXVvt8mTNDRrYNDYixla5OE2m9GVxFwFjPi7BvtKk\n5sENbHotIo5czuzeJQ0tO1XSe8BfWTqsqVF5MvSWpAn675DmMJy7nGUwMzOzleQefjNbo7iH38xW\nB/fw29qsKmP4zczMzMyseTjgNzMzMzOrYQ74zczMzMxqmAN+MzMzM7Ma5oDfzMzMzKyGOeA3MzMz\nM6thXpbTzNY09XPmvNvcZVgjdOzYDrdF4rZYym2xlNtiKS/LaZW4h9/MzMzMrIY54DczMzMzq2EO\n+M3MzMzMapgDfjMzMzOzGuaA38zMzMyshjngNzMzMzOrYS2buwBmZsUGnHlrcxfBzD4mbr/0kOYu\ngtnHgnv4zczMzMxqmAN+MzMzM7Ma5oDfzMzMzKyGOeA3MzMzM6thDvjNzMzMzGqYA34zMzOrGSNG\nfJftttuSXr26M2PGo8tsmzDhBrp124EuXTbjmGMGM2/evGYqpVl1OeA3MzOzmjB58oNcffUYbrzx\nZnr06MWwYWcs2bZ48WLOOutUTjjhRO644z7uvfdubrppfDOW1qx6HPCbmZlZTZg2bQqbb96Z7t13\nZu+99+PZZ5/hzTffAKCuro7WrVvToUMHOnXajBYtWtC2bdtmLrFZdfiLt8zMzKwmzJkzh3bt2gOw\n4YYbAjB79mw6dNiIFi1acPHFl3PiicdTV1dHz569OeKIQc1ZXLOqaZaAX9KBwDnAYqAt8CLwTWAH\n4LWI+Mdy5tcFmBoRnVewPK8AuwNdgFuBJ0t2uToixq1I3itYni6UqY+kemDdiFhU4f2fAS4GtgTm\nA+8BZ0fEE5JGAS0j4txVVNYbgDOB14DJwDrAKcCQiDh5FeTfBrgC+CywCGgH/DQibszbj46I369A\nvmNJbXxVSXqj7buqSBoK/A74bEQ8X5R+LvCDiKhbjrx+D9wfEWMr7FOxbpL6svT6ryP9fbgNuCQi\nFktaHzggIm5uarnMzNYUc+fOZcSIszn77BH06NGLwYMPY9y4sQwd+vXmLprZalf1gF9SK+D3QNeI\neDWnXQR8HdgeuBFYroB/FXsmIvo24/FXSg6Q7wZOi4jbclpf4E5JWtXHi4gj8zG2ALaLiE3zpsdW\n0SHOAOZHxO5Fx7lD0h3ABsC3SNfTx9VfgeOAs4vSjgD+0zzFWXr9S9qAdENyKXAa0A04HHDAb2Zr\npE022ZR33nkbgDfe+B8AnTp1AiDiOebMmc2hhx7BVlt1YeutP8306Q874Le1QnP08Lch9eovGTgX\nEcMkHQacB+wq6XTgfeAiYCGwPnBi7qHehBSEbEB6QvAdYG4hL0mdSQHvV4GXgd8AHfP+l0bE9ZI2\nBW4i9UY/TurNrCj3ut8G3AP0IPU0fwmYDVwFCKgHnoyI7+Qbm18C2+Z9x0fEpblX94B8zO6kYLUV\n0C+n7VN0zMuBnXP6oIj4d9G2BvMHjgJmFIL93L6TJH02It4ujvklfRsYktt6ATA4It6S9BNgr9z2\n/waOBbYDxhSdjwsi4g5JL+Uy/wbYUNIk4EfA9yJid0lbAr/K7/kEMCIi7s897Atzux1VXLcSGwHt\nJNVFRH1EvAx8Ppf/DmBHSdcCQ3MZtgdaA49GxCl5v68D3wY+AB6MiBHFB8hPPbaIiLJ/9RtqK2Ag\nsFtEDM37DAaOiIhBkn4E7Ea63ieTAvo+pGt8ASloXgzcBQyUdE7uRd8DeAHokPNcB7gc2Il0fU2M\niPMktQCuBnYE/knR50nSIOBk0nUzB/hGRPyvXN3KydfLccCLks7Px+sg6afALOCgXM7LgGk0/Fkr\nd52ama1yffr047LLfsrMmTO499676datO61atWbBggV07rwlLVu2ZNKkifTuvTsvvfQi/fsf1NxF\nNquKqk/ajYi3gfOBpyTdL2mkJEXEn4CngDMjYiKwMfDtiNgL+DlQCNJ+DNyZe3y/BxxTyFtSe+CP\n+X1PAxcCd+c89gQukNQROBWYnvO4BvhUE4u/AzA2IvbMZR1MCrh6RESviOid67VBPsZ/IqIf6Qbh\nSEmfz/nsTAoe9811uC+/d2FOA9gcuC6XcSJweklZyuX/ORroXY+INxuoTxtgv4joA7wEHC2pA+km\nqldE7EEKTDcFDb87ygAADlVJREFUjgduzccbAHyyJK9vAHNy7/D7Rem/JgV/ewEHA1dJKtxoto2I\nvhWCfUjnfhdS0HmVpIE5iIR0HT0TEUNIgefTEbFnRPQA9pPUVdJWwEhgj4joBXyq+EmHpK8BXwBO\nqFCGBtsKGJ+P84m8z6Bcv4HA5hHRJyJ2JQW7hf9VdgaOiYir8+u3gJlA//x6KHB90XEHAVuTbh72\nzMfrQ7rJ2j63zTG5DoUnICOBffK1M4mln53lFhFvAX/Px/oJ6VotPI34ItA/Iu6g8met3OfAzGyl\ndezYbsnPwQcfwBlnnMFRRw1k1qxnGDNmNEOGDOLkk4+na9dtGT16NFdccSn779+Xfffdh5Ejhy3z\n/o/zj1klzTKGPyIuknQVsB+pZ/tRSeeU7PYacImk9Ug9hoWAtQepR5GImAxMzr3vLUnB/vURMSXv\n2w/YRdKx+fUHpOBpR1JvNfmpwdtFx90x91IXG5L/fT0ins2//5PU+/wc8LqkO4HbgZtyz2g/oHMO\nzgDWIwV+ADMjYmGeO9ACmJrTX8l1BXg7IgqB+zTSuPhi5fJfTHpy0RT/Iw31+ZA0f+HViHhT0j2k\ndv0TcGNEvCLpj8DYHED/GWjqnIZ+pB768/PrD4BNiupVUUT8KweIu5CeOpwFXChpl5Jd3wK2kPQI\n6cZpM9JN4/bA4xHxXs5vKECO+fcBegOfiYjFjRSlobaaK+lW4MuS/kC6IbwfuBLoVXQdbUC67p5O\nRYg3SvIeB3xN0gPA3qSnEZfnbT1IY/PrgcWSpuS2qAem5fT5kgqLTffKdb8n17E1aY7Myig8TSv1\nREQszL+X+6yVu06fXskymZkBMGfOu8u8Hj58FMOHj1ryesKE25fsN2DAQAYMGLhk2wcffPT9H1cO\n+q2S5pq0u34eYjAeGC9pAmmccHEgNA74ZkRMlHQQKdCDFOg09GRiI1JP6QmSroqIeaTA78SImFly\n/Drgw6Kk4gC5wTH8+aaidLJjXUQsAPaQ1J3Ui/uYpN3ysS+IiD+U5DO0NJ+SSZSF4UUflqTVlxy7\nXP7tSOOsS8u/E0VBVh76dAnwuYiYLemSovJ8WdL2pCFLkyUdEREPSepKCkiHknq4v1p6nAYsBA6P\niNdLygPLPgloUJ6TsCAiZgAz8nyPKaRgvfh6OZIUCO8REYskFc55uesF0pOdF0jDoK4us0/FtgJG\nk67dhcANEfGhpIXAmIi4pCSfvmXqfBdpOMxQ4J6IeL/oIUTpeS9cC+Wu4YWkIV2r5Dm1pE+RnvDM\nArqWbC6uS7nPWoPXqZmZmVVP1Yf0SNofeCQHpgXbAH8jBTDr5rRNgWfzGOaBpJ5KSL3CB+S8dpd0\nTU6fHRHnALeQVnWB1HM+KO/bRtKv8nCSWaSeUCT1II0tX9H67Czp2Ih4IiIuIM0J+EzJsVtIukzS\nRsuRdYd8EwFpOMczJdvL5T8e+JykJcF47l39A0ufHkDqZX89B7AbkZ62tJa0jaTTI+L5PNb6ZuAL\nkk4GOkfE7aQJ1j2aWI/icm6sNC9heUxk6RMWSOdqY9LE7tLrJXKwvxOpF7k1aXjTrnm4F5JuytsB\nriXduJxXPMynAQ22FemAT5GG+5xEmltSqPPhhaFLkr4nabtymUfEB6R2/gEfnYA8HdhXUl3Or09O\nmwX0zOntWHo+CvXtlI89UNIhFepWVs53DHBlRMxn2fYuVe6ztrKfAzMzM1tJVe/hj4h7lJaNfEDS\nfFJP5X9J48aHAqMlnUaasDuRNHTmYmBcTj8P+J2kATnLk0oOcT4wRWni4ijSmOqppABtTA4Ifw7c\nJGki8CzLrgrU0JCeR0g9uQ35O3C+pG+SJmP+HXgYeJQUeD9C6n39c0S8UTmuXMaLwBBJF+eyf7lk\n+y8byh/SjRBwpaRhpKEubwL7R8TrRcd/CnhB0oxCHUjj7e8CuuX0d/N7v0+6QRov6Z18vOFNrMcp\nwBhJX8n1uLCpDZB9Bfh5Ufu2AX4SEU9J+iSwqaT7SCvd3C5pMqn9LyHd+PUkXQf3S1pEWorz8UI7\nRMSr+WZmvKRe+ZgPKC1hCfAv0nX5kbaSdEdETCUF6QdHxL/ye27Ox50maTHwBOka27xCPceRnhBN\nLUmfQBp2NJXU7rdExMP5Rvgo0nX2T9I1SkT8R9KpwJ/z52s+adJ1UxWu/3WB9qRVs36Ut80ALpL0\nW+ChkveNouHPWtnr1MzMzKqjrr6+dMSAmTVVHh52G/CLiLi3uctTCwaceav/KJlZk9x+6SE1MwZ/\nZXXs2K7J391iax9/0641O0mjSctzlro7In5S7fI0VR5ydRVp3P3HIthXWv721Ia2NTR3xczMzD7+\n3MNvZmsU9/CbWVO5h38p9/BbJVWftGtmZmZmZtXjgN/MzMzMrIY54DczMzMzq2EO+M3MzMzMapgD\nfjMzMzOzGuaA38zMzMyshnlZTjNb09R7mb2kY8d2XnIwc1ss5bZYym2xlJfltErcw29mZmZmVsMc\n8JuZmZmZ1TAH/GZmZmZmNcwBv5mZmZlZDXPAb2ZmZmZWwxzwm5mZmZnVMAf8ZmZmZmY1zAG/mZmZ\nmVkNc8BvZmZmZlbD/E27ZmZmZmY1zD38ZmZmZmY1zAG/mZmZmVkNc8BvZmZmZlbDHPCbmZmZmdUw\nB/xmZmZmZjXMAb+ZmZmZWQ1zwG9mZmZmVsNaNncBzGztJOk84EtAHXBHRFxQsv0bwAnAIuAp4KSI\n+LDqBa2CJrTFScCxwGLg78DXIuL9qhe0Chpri6L9TgVOj4guVSxeVTXhutgZ+DXwITAbODIi5lW9\noFXQhLY4DRgMvA+8BQyNiDerXtAqkNQJuA5oHRG7N7B9rfnbaU3nHn4zqzpJPYDDgT2BPYABknoX\nbe8MnAfsB+wGbA4c2QxFXe2a0BZdgVOA3SOiJ7Ae8JXmKOvq1lhbFO33GVLwV7OacF20AG4ETomI\nHsBjwEeCv1rQxL8XpwB7REQf4AXgpOYoa5WMB+5taMPa9LfTlo8DfjNrDgcCt0bE+7mn+lagf9H2\nfYAHI+KtiKgHJpRsryWNtcUsYKeIWJhfzwE2rnIZq6WxtigEur8BTm2G8lVTY23RDXg3Ih4BiIgL\nIuKeZihnNTTWFvOAeqBdfr0h6XNSqw4BHi2zbW3622nLwQG/mTWHTwGvFb1+Lac1dXstqVjXiPgw\nIt4FkLQ1qWf7pqqWsHqact6/C9wdEc9VrVTNo7G22Bb4j6RfSZom6WpJ7atawupp7DPyJvAD4EVJ\n/yC1zVVVLWEVRcQ7FTavTX87bTk44DezNUEdqYduRbfXkgbrKumzwD3A8RHxctVL1TyWaQtJnyP1\n9l7abCVqPg1dF91Ige5upHH8w6tdqGZSel1sCYwEFBHbAM+w9rRFY9amv51WgQN+M2sOL/PRHv1X\nlmN7LWm0rpJ2AG4jTda9u4plq7bG2uIIYCPgYUnTgc0k3VfF8lVTY23xH+C5iHg1D924DfhCFctX\nTY21RU/gqYj4b379Z9JY/7XR2vS305aDA34zaw53AIdKWk/SeqQJebcXbb8P6CPpk3nM9ldIAU0t\nqtgWkloBN5BWYHm4mcpYLRXbIo9T/3xE9MwTmF+NiH2bq7CrWWOfkelAZ0mF4G434P9VuYzV0lhb\nPA98UVKb/LonUOtDvspZm/522nLwspxmVnUR8YSkccBDpMfN4yJipqQbgDMj4t+SRgJ3k5aWmwbc\n3HwlXn0aawugN7AlcKmkwtvui4gfNkuBV6OmXBfNW8LqaeJn5DjgNknvkSapfr0Zi7zaNKEtnpb0\nK2BSbos3geObscirTR6+dC1pYvLWkiaRboh2Yi3722nLp66+3kO7zMzMzMxqlYf0mJmZmZnVMAf8\nZmZmZmY1zAG/mZmZmVkNc8BvZmZmZlbDHPCbmZmZmdUwB/xmZlZzJD0s6S/NXQ4zszWBA34zM6sp\nkroCbwP/ktSructjZtbc/MVbZmZWa44FJgALgCHAIwCShgDn5n0eBb4REQsbSgd6AVdFxLb5vX0L\nryWNAjYHvgBcD1wB/ALYB2gFTAWOi4gPJG0M/A74HDAXOAtYF7goIroWCixpJnBhRNyyylvDzNZ6\n7uE3M7OaIWkd4HDgj8CtQH9JrSR1AS4B+gIC2gKnlEtvwqH6A/0j4nLgMGAPoCvwWdK3ng7O+/0E\nmBUR25BuRMYD9wObSfp8LvOWwLbAXSteczOz8hzwm5lZLdkfeCwi3omI+cAkYACwHzAtIv4TEfXA\nV4GfVUhvzKMR8TpARPwR2DkiPoiIBcBjwDZ5v/6kIJ+IeBLoEhELgT8AX8n7HArcmtPNzFY5D+kx\nM7NaMpTUq/9Wft0S6ABMBwpp5MCcPOSmofTGjvNG4RdJHYFfSOoOfAh0Ai7Pm0vzfzf/Oh4YC5xD\nCvgvaXINzcyWkwN+MzOrCZI6kIbmbBQR7+e0lsArwMOk4Luwb3ugDfA60LuB9MXAOkXZd6hw6B8C\nHwA75jkB1xVtez0f96Wcfxfg38BDQEtJB5GGAt23vPU1M2sqD+kxM7NacSQwsRDsA0TEIuAeoDWw\nm6QukuqA3wBfB+4sk/4qaZz9JnlewFEVjrsJ8EwO9r8A7AZ8Im+7jfTUAUk7AE8ALSPiQ+BG4Erg\ntoj4YJW0gJlZAxzwm5lZrTgWaGiVmz8BBwMnABOBvwL1wGUR8UqZ9L8BvwWeJK2680CF414KfEvS\nc8B3gDOBb0gaCAwDOkt6iRTgfzUi3svvGw9sldPNzFabuvr6+uYug5mZ2VpH0qakHv8tI2Jxc5fH\nzGqXe/jNzMyax/eBXzvYN7PVzZN2zczMqij37D8CPA2c3szFMbO1gIf0mJmZmZnVMA/pMTMzMzOr\nYQ74zczMzMxqmAN+MzMzM7Ma5oDfzMzMzKyGOeA3MzMzM6th/x90bUikQkL43wAAAABJRU5ErkJg\ngg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with highest accuracy: StackedEnsembleKFold_StackLayerModel_logreg  Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Used for comparision training data X_train : \",X_train.shape,\"testing data X_test\",X_test.shape)\n",
    "plt.xlim(0, 1.0)\n",
    "_ = plt.barh(range(len(model_test_accuracy_comparisons)), list(model_test_accuracy_comparisons.values()), align='center')\n",
    "_ = plt.yticks(range(len(model_test_accuracy_comparisons)), list(model_test_accuracy_comparisons.keys()))\n",
    "_ = plt.title(\"Comparision of StackedEnsembleClassifier, StackedEnsembleHoldOut, StackedEnsembleKfold\",fontsize=14)\n",
    "_ = plt.xlabel('Accuracy',fontsize=12)\n",
    "_ = plt.ylabel('Classifier Name',fontsize=12)\n",
    "_ = plt.tick_params(axis='both', which='major', labelsize=11)\n",
    "#displaying labels for each bar\n",
    "for i, v in enumerate(list(model_test_accuracy_comparisons.values())):\n",
    "    a=round(v,2)\n",
    "    plt.text(v+0.01 , i , str(a), color='black', fontweight='bold')\n",
    "plt.show()\n",
    "newD = {k:round(v,2) for k, v in model_test_accuracy_comparisons.items()}\n",
    "haccm = list(max(zip(newD.values(), newD.keys())))\n",
    "print(\"Model with highest accuracy:\",haccm[1],\" Accuracy:\",haccm[0])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above bar chart is showing a comparison between StackedEnsembleClassifier,StackedEnsembleHoldOut and StackedEnsembleKFold. These three classifiers were implemented and tested on MNIST fashion data. The stack layer of all three classifiers was changed once with decision tree and once with logistic regression. As can be seen in the graph, overall, logistics regression gave a better accuracy than decision tree for all classifiers. In that, StackedEnsembleKFold classifier with a stack layer model of logistic regression had the highest accuracy at 84%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nz9msE0qdnyc"
   },
   "source": [
    "## Task 4: Comparing the Performance of Different Stack Layer Approaches with  More Standard Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RQPet1l8dnyc"
   },
   "source": [
    "### Simple Decision Tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UWSS4tDtdnyd"
   },
   "source": [
    "Train the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "GZV6SbRZdnyd",
    "outputId": "3ff5c2ea-cb64-4ef4-a69f-e81ac3d1d9da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tree = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "my_tree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cpi1879ednyh"
   },
   "source": [
    "Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "id": "J6CQPVeYdnyi",
    "outputId": "672ed8ba-2771-4a25-e24d-4b7a144bf47f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7294444444444445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.67      0.70       171\n",
      "           1       0.91      0.92      0.91       185\n",
      "           2       0.64      0.56      0.60       194\n",
      "           3       0.78      0.75      0.77       195\n",
      "           4       0.49      0.57      0.53       145\n",
      "           5       0.82      0.83      0.83       185\n",
      "           6       0.45      0.49      0.47       171\n",
      "           7       0.80      0.76      0.78       178\n",
      "           8       0.83      0.84      0.83       170\n",
      "           9       0.81      0.85      0.83       206\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      1800\n",
      "   macro avg       0.73      0.72      0.72      1800\n",
      "weighted avg       0.73      0.73      0.73      1800\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>115</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>109</td>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>147</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>4</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>175</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>157</td>\n",
       "      <td>187</td>\n",
       "      <td>171</td>\n",
       "      <td>189</td>\n",
       "      <td>169</td>\n",
       "      <td>188</td>\n",
       "      <td>183</td>\n",
       "      <td>169</td>\n",
       "      <td>171</td>\n",
       "      <td>216</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1    2    3    4    5    6    7    8    9   All\n",
       "True                                                             \n",
       "0          115    2    6   13    3    1   26    0    5    0   171\n",
       "1            1  170    2    8    0    0    4    0    0    0   185\n",
       "2            2    1  109    5   41    1   29    0    4    2   194\n",
       "3            8   10    4  147   11    0   11    0    3    1   195\n",
       "4            2    1   23    8   83    0   25    0    3    0   145\n",
       "5            0    1    0    3    1  154    0   13    5    8   185\n",
       "6           26    1   21    1   29    0   83    1    7    2   171\n",
       "7            0    0    0    0    0   19    0  135    0   24   178\n",
       "8            2    0    6    3    1    7    5    0  142    4   170\n",
       "9            1    1    0    1    0    6    0   20    2  175   206\n",
       "All        157  187  171  189  169  188  183  169  171  216  1800"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_tree.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) # , normalize=True, sample_weight=None\n",
    "model_test_accuracy_comparisons[\"Simple Tree\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "# print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print nicer confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E0aVwZ5Adnyl"
   },
   "source": [
    "### Tuned Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ncpNKL-Zdnym"
   },
   "source": [
    "Grid Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 18408
    },
    "colab_type": "code",
    "id": "nUBz7h5adnyn",
    "outputId": "20ce315d-5ac7-4d54-c1f1-43535cb96545"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 98 candidates, totalling 294 fits\n",
      "[CV] criterion=gini, max_depth=3, min_samples_split=10 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=gini, max_depth=3, min_samples_split=10, total=   0.3s\n",
      "[CV] criterion=gini, max_depth=3, min_samples_split=10 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=gini, max_depth=3, min_samples_split=10, total=   0.3s\n",
      "[CV] criterion=gini, max_depth=3, min_samples_split=10 ...............\n",
      "[CV]  criterion=gini, max_depth=3, min_samples_split=10, total=   0.3s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=10 ...............\n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=10, total=   0.6s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=10 ...............\n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=10, total=   0.6s\n",
      "[CV] criterion=gini, max_depth=6, min_samples_split=10 ...............\n",
      "[CV]  criterion=gini, max_depth=6, min_samples_split=10, total=   0.6s\n",
      "[CV] criterion=gini, max_depth=9, min_samples_split=10 ...............\n",
      "[CV]  criterion=gini, max_depth=9, min_samples_split=10, total=   0.9s\n",
      "[CV] criterion=gini, max_depth=9, min_samples_split=10 ...............\n",
      "[CV]  criterion=gini, max_depth=9, min_samples_split=10, total=   0.9s\n",
      "[CV] criterion=gini, max_depth=9, min_samples_split=10 ...............\n",
      "[CV]  criterion=gini, max_depth=9, min_samples_split=10, total=   1.0s\n",
      "[CV] criterion=gini, max_depth=12, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=12, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=12, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=12, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=12, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=12, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=15, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=15, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=15, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=15, min_samples_split=10, total=   1.3s\n",
      "[CV] criterion=gini, max_depth=18, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=18, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=18, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=18, min_samples_split=10, total=   1.3s\n",
      "[CV] criterion=gini, max_depth=18, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=18, min_samples_split=10, total=   1.3s\n",
      "[CV] criterion=gini, max_depth=21, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=21, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=21, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=21, min_samples_split=10, total=   1.3s\n",
      "[CV] criterion=gini, max_depth=21, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=21, min_samples_split=10, total=   1.3s\n",
      "[CV] criterion=gini, max_depth=24, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=24, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=24, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=24, min_samples_split=10, total=   1.3s\n",
      "[CV] criterion=gini, max_depth=24, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=24, min_samples_split=10, total=   1.3s\n",
      "[CV] criterion=gini, max_depth=27, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=27, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=27, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=27, min_samples_split=10, total=   1.3s\n",
      "[CV] criterion=gini, max_depth=27, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=27, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=30, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=30, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=30, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=30, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=30, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=30, min_samples_split=10, total=   1.3s\n",
      "[CV] criterion=gini, max_depth=33, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=33, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=33, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=33, min_samples_split=10, total=   1.3s\n",
      "[CV] criterion=gini, max_depth=33, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=33, min_samples_split=10, total=   1.3s\n",
      "[CV] criterion=gini, max_depth=36, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=36, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=36, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=36, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=36, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=36, min_samples_split=10, total=   1.3s\n",
      "[CV] criterion=gini, max_depth=39, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=39, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=39, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=39, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=39, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=39, min_samples_split=10, total=   1.3s\n",
      "[CV] criterion=gini, max_depth=42, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=42, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=42, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=42, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=42, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=42, min_samples_split=10, total=   1.3s\n",
      "[CV] criterion=gini, max_depth=45, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=45, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=45, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=45, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=45, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=45, min_samples_split=10, total=   1.3s\n",
      "[CV] criterion=gini, max_depth=48, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=48, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=48, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=48, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=48, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=48, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=51, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=51, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=51, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=51, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=51, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=51, min_samples_split=10, total=   1.3s\n",
      "[CV] criterion=gini, max_depth=54, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=54, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=54, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=54, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=54, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=54, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=57, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=57, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=57, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=57, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=57, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=57, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=60, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=60, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=60, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=60, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=60, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=60, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=63, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=63, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=63, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=63, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=63, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=63, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=66, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=66, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=66, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=66, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=66, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=66, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=69, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=69, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=69, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=69, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=69, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=69, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=72, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=72, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=72, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=72, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=72, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=72, min_samples_split=10, total=   1.3s\n",
      "[CV] criterion=gini, max_depth=75, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=75, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=75, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=75, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=75, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=75, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=78, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=78, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=78, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=78, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=78, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=78, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=81, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=81, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=81, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=81, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=81, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=81, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=84, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=84, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=84, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=84, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=84, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=84, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=87, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=87, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=87, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=87, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=87, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=87, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=90, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=90, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=90, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=90, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=90, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=90, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=93, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=93, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=93, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=93, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=93, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=93, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=96, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=96, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=96, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=96, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=96, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=96, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=99, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=99, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=99, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=99, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=99, min_samples_split=10 ..............\n",
      "[CV]  criterion=gini, max_depth=99, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=102, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=102, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=102, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=102, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=102, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=102, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=105, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=105, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=105, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=105, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=105, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=105, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=108, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=108, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=108, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=108, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=108, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=108, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=111, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=111, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=111, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=111, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=111, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=111, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=114, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=114, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=114, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=114, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=114, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=114, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=117, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=117, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=117, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=117, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=117, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=117, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=120, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=120, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=120, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=120, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=120, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=120, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=123, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=123, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=123, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=123, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=123, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=123, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=126, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=126, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=126, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=126, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=126, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=126, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=129, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=129, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=129, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=129, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=129, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=129, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=132, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=132, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=132, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=132, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=132, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=132, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=135, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=135, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=135, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=135, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=135, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=135, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=138, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=138, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=138, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=138, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=138, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=138, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=141, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=141, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=141, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=141, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=141, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=141, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=144, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=144, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=144, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=144, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=144, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=144, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=147, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=147, min_samples_split=10, total=   1.1s\n",
      "[CV] criterion=gini, max_depth=147, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=147, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=gini, max_depth=147, min_samples_split=10 .............\n",
      "[CV]  criterion=gini, max_depth=147, min_samples_split=10, total=   1.2s\n",
      "[CV] criterion=entropy, max_depth=3, min_samples_split=10 ............\n",
      "[CV]  criterion=entropy, max_depth=3, min_samples_split=10, total=   0.7s\n",
      "[CV] criterion=entropy, max_depth=3, min_samples_split=10 ............\n",
      "[CV]  criterion=entropy, max_depth=3, min_samples_split=10, total=   0.7s\n",
      "[CV] criterion=entropy, max_depth=3, min_samples_split=10 ............\n",
      "[CV]  criterion=entropy, max_depth=3, min_samples_split=10, total=   0.7s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=10 ............\n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=10, total=   1.6s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=10 ............\n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=10, total=   1.5s\n",
      "[CV] criterion=entropy, max_depth=6, min_samples_split=10 ............\n",
      "[CV]  criterion=entropy, max_depth=6, min_samples_split=10, total=   1.6s\n",
      "[CV] criterion=entropy, max_depth=9, min_samples_split=10 ............\n",
      "[CV]  criterion=entropy, max_depth=9, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=9, min_samples_split=10 ............\n",
      "[CV]  criterion=entropy, max_depth=9, min_samples_split=10, total=   2.1s\n",
      "[CV] criterion=entropy, max_depth=9, min_samples_split=10 ............\n",
      "[CV]  criterion=entropy, max_depth=9, min_samples_split=10, total=   2.1s\n",
      "[CV] criterion=entropy, max_depth=12, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=12, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=12, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=12, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=12, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=12, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=15, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=15, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=18, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=18, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=18, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=18, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=18, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=18, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=21, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=21, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=21, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=21, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=21, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=21, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=24, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=24, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=24, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=24, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=24, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=24, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=27, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=27, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=27, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=27, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=27, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=27, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=30, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=30, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=30, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=30, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=30, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=30, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=33, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=33, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=33, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=33, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=33, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=33, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=36, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=36, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=36, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=36, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=36, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=36, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=39, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=39, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=39, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=39, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=39, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=39, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=42, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=42, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=42, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=42, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=42, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=42, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=45, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=45, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=45, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=45, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=45, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=45, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=48, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=48, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=48, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=48, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=48, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=48, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=51, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=51, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=51, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=51, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=51, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=51, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=54, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=54, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=54, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=54, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=54, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=54, min_samples_split=10, total=   2.4s\n",
      "[CV] criterion=entropy, max_depth=57, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=57, min_samples_split=10, total=   2.4s\n",
      "[CV] criterion=entropy, max_depth=57, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=57, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=57, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=57, min_samples_split=10, total=   2.4s\n",
      "[CV] criterion=entropy, max_depth=60, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=60, min_samples_split=10, total=   2.4s\n",
      "[CV] criterion=entropy, max_depth=60, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=60, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=60, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=60, min_samples_split=10, total=   2.4s\n",
      "[CV] criterion=entropy, max_depth=63, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=63, min_samples_split=10, total=   2.4s\n",
      "[CV] criterion=entropy, max_depth=63, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=63, min_samples_split=10, total=   2.4s\n",
      "[CV] criterion=entropy, max_depth=63, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=63, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=66, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=66, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=66, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=66, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=66, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=66, min_samples_split=10, total=   2.4s\n",
      "[CV] criterion=entropy, max_depth=69, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=69, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=69, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=69, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=69, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=69, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=72, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=72, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=72, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=72, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=72, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=72, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=75, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=75, min_samples_split=10, total=   2.4s\n",
      "[CV] criterion=entropy, max_depth=75, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=75, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=75, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=75, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=78, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=78, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=78, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=78, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=78, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=78, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=81, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=81, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=81, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=81, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=81, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=81, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=84, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=84, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=84, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=84, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=84, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=84, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=87, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=87, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=87, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=87, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=87, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=87, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=90, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=90, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=90, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=90, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=90, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=90, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=93, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=93, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=93, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=93, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=93, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=93, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=96, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=96, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=96, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=96, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=96, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=96, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=99, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=99, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=99, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=99, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=99, min_samples_split=10 ...........\n",
      "[CV]  criterion=entropy, max_depth=99, min_samples_split=10, total=   2.4s\n",
      "[CV] criterion=entropy, max_depth=102, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=102, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=102, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=102, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=102, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=102, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=105, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=105, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=105, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=105, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=105, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=105, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=108, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=108, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=108, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=108, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=108, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=108, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=111, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=111, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=111, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=111, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=111, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=111, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=114, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=114, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=114, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=114, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=114, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=114, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=117, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=117, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=117, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=117, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=117, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=117, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=120, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=120, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=120, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=120, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=120, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=120, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=123, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=123, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=123, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=123, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=123, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=123, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=126, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=126, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=126, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=126, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=126, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=126, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=129, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=129, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=129, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=129, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=129, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=129, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=132, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=132, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=132, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=132, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=132, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=132, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=135, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=135, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=135, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=135, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=135, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=135, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=138, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=138, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=138, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=138, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=138, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=138, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=141, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=141, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=141, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=141, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=141, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=141, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=144, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=144, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=144, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=144, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=144, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=144, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=147, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=147, min_samples_split=10, total=   2.3s\n",
      "[CV] criterion=entropy, max_depth=147, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=147, min_samples_split=10, total=   2.2s\n",
      "[CV] criterion=entropy, max_depth=147, min_samples_split=10 ..........\n",
      "[CV]  criterion=entropy, max_depth=147, min_samples_split=10, total=   2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 294 out of 294 | elapsed:  8.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy', 'max_depth': 9, 'min_samples_split': 10}"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7340476190476191"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.30923573, 0.60407122, 0.93427896, 1.13709195, 1.21882693,\n",
       "        1.24068753, 1.25634638, 1.2661221 , 1.22407277, 1.20112507,\n",
       "        1.23795565, 1.23513095, 1.22342102, 1.2066745 , 1.20524716,\n",
       "        1.19687899, 1.19126217, 1.18841108, 1.18456078, 1.18721596,\n",
       "        1.18993425, 1.18814079, 1.16980378, 1.19845239, 1.1939671 ,\n",
       "        1.17198396, 1.2032214 , 1.18670019, 1.18805377, 1.19750905,\n",
       "        1.1815184 , 1.18789864, 1.17383575, 1.19089007, 1.18520737,\n",
       "        1.16852172, 1.17363922, 1.19198783, 1.20260835, 1.19990667,\n",
       "        1.2073919 , 1.18838922, 1.18399358, 1.15050475, 1.15753865,\n",
       "        1.15072091, 1.15585375, 1.14657593, 1.16252136, 0.70339449,\n",
       "        1.54998048, 2.1372687 , 2.27030897, 2.28354216, 2.26696952,\n",
       "        2.28575754, 2.28139655, 2.2644132 , 2.26259232, 2.27694805,\n",
       "        2.26469556, 2.28439633, 2.28194785, 2.27131772, 2.30049308,\n",
       "        2.27184192, 2.31606587, 2.37123728, 2.37744133, 2.36067303,\n",
       "        2.29793398, 2.29860655, 2.30276036, 2.30846318, 2.29416998,\n",
       "        2.2912492 , 2.28852272, 2.28634048, 2.29473615, 2.28958885,\n",
       "        2.28769946, 2.3150533 , 2.29040591, 2.26029007, 2.25955764,\n",
       "        2.28769644, 2.25697915, 2.25559998, 2.24691931, 2.24460983,\n",
       "        2.26680756, 2.26299795, 2.25917792, 2.25631499, 2.25519721,\n",
       "        2.25237544, 2.2662855 , 2.26446795]),\n",
       " 'mean_score_time': array([0.00388805, 0.00381374, 0.00431387, 0.00431919, 0.00437403,\n",
       "        0.00450309, 0.00440105, 0.00439064, 0.00446407, 0.00438905,\n",
       "        0.00430187, 0.00559553, 0.00457962, 0.00446312, 0.00500409,\n",
       "        0.00447424, 0.00427628, 0.00421898, 0.0042994 , 0.00432165,\n",
       "        0.00438547, 0.00434224, 0.00415365, 0.00452685, 0.00447663,\n",
       "        0.0041426 , 0.00433064, 0.00447647, 0.00432857, 0.00443236,\n",
       "        0.00414721, 0.00430584, 0.00412091, 0.00421492, 0.00437649,\n",
       "        0.00434852, 0.00430894, 0.00404572, 0.00436465, 0.00448291,\n",
       "        0.0044415 , 0.00459051, 0.00434836, 0.00412862, 0.00418019,\n",
       "        0.00423447, 0.00412369, 0.00422621, 0.00435432, 0.00435901,\n",
       "        0.00408387, 0.00423408, 0.00419617, 0.00416525, 0.00443967,\n",
       "        0.00420133, 0.00432682, 0.00415754, 0.00421246, 0.00417399,\n",
       "        0.00421373, 0.00427787, 0.00434947, 0.00431093, 0.00422875,\n",
       "        0.00412846, 0.00419736, 0.00438563, 0.00470662, 0.00436433,\n",
       "        0.00410191, 0.00457732, 0.00439843, 0.00414864, 0.00446431,\n",
       "        0.00455451, 0.00479579, 0.00442322, 0.00420928, 0.00412591,\n",
       "        0.00423304, 0.00479722, 0.00407386, 0.00413219, 0.00428414,\n",
       "        0.0044922 , 0.00484721, 0.0042607 , 0.00453695, 0.00421357,\n",
       "        0.0041062 , 0.00425037, 0.00435376, 0.00425768, 0.0041995 ,\n",
       "        0.00424115, 0.00435082, 0.00417296]),\n",
       " 'mean_test_score': array([0.48880952, 0.7097619 , 0.73333333, 0.72809524, 0.72428571,\n",
       "        0.72666667, 0.7247619 , 0.72404762, 0.71928571, 0.72452381,\n",
       "        0.72214286, 0.7202381 , 0.72595238, 0.72761905, 0.72357143,\n",
       "        0.72857143, 0.72261905, 0.72333333, 0.725     , 0.72738095,\n",
       "        0.72214286, 0.72404762, 0.72880952, 0.72571429, 0.72333333,\n",
       "        0.72071429, 0.72071429, 0.72547619, 0.72380952, 0.7247619 ,\n",
       "        0.72238095, 0.72642857, 0.71952381, 0.72595238, 0.72357143,\n",
       "        0.72571429, 0.725     , 0.72357143, 0.72404762, 0.72190476,\n",
       "        0.7197619 , 0.72547619, 0.7247619 , 0.7252381 , 0.72809524,\n",
       "        0.72261905, 0.72333333, 0.72595238, 0.72285714, 0.5197619 ,\n",
       "        0.72619048, 0.73404762, 0.72571429, 0.7302381 , 0.72880952,\n",
       "        0.72595238, 0.72547619, 0.72285714, 0.72404762, 0.7252381 ,\n",
       "        0.725     , 0.72619048, 0.72571429, 0.7252381 , 0.72761905,\n",
       "        0.72619048, 0.72214286, 0.72714286, 0.72904762, 0.72571429,\n",
       "        0.71952381, 0.73      , 0.72833333, 0.72428571, 0.72571429,\n",
       "        0.72785714, 0.72452381, 0.73261905, 0.7247619 , 0.72380952,\n",
       "        0.72119048, 0.72690476, 0.73166667, 0.72428571, 0.72404762,\n",
       "        0.72880952, 0.72261905, 0.72714286, 0.72714286, 0.72404762,\n",
       "        0.725     , 0.72404762, 0.72714286, 0.7252381 , 0.72547619,\n",
       "        0.73166667, 0.72761905, 0.72619048]),\n",
       " 'mean_train_score': array([0.50107181, 0.77475879, 0.88012314, 0.92976203, 0.94095392,\n",
       "        0.94261957, 0.94321446, 0.9430955 , 0.9427384 , 0.94321451,\n",
       "        0.94321434, 0.94321463, 0.94357165, 0.94345281, 0.9430955 ,\n",
       "        0.94345286, 0.94369082, 0.94297594, 0.94357182, 0.9430955 ,\n",
       "        0.94321451, 0.94285711, 0.94333372, 0.94309533, 0.94309546,\n",
       "        0.9427384 , 0.94285732, 0.94297658, 0.94309546, 0.94333389,\n",
       "        0.94333351, 0.94345286, 0.94297616, 0.94333351, 0.94380983,\n",
       "        0.94309533, 0.94321434, 0.94297637, 0.94309533, 0.94297633,\n",
       "        0.94297637, 0.94321451, 0.94333394, 0.94333347, 0.94345286,\n",
       "        0.94321497, 0.94321451, 0.94321438, 0.94357212, 0.52940457,\n",
       "        0.79582832, 0.90428481, 0.95345363, 0.95547731, 0.9554774 ,\n",
       "        0.95571524, 0.95559627, 0.95547723, 0.95559619, 0.95583407,\n",
       "        0.95535835, 0.95559627, 0.95595316, 0.95559606, 0.95535805,\n",
       "        0.95595303, 0.95559606, 0.95571524, 0.95571511, 0.95512021,\n",
       "        0.95547701, 0.95559627, 0.95607221, 0.95535805, 0.95559606,\n",
       "        0.95559615, 0.95595325, 0.95559606, 0.95547723, 0.95559619,\n",
       "        0.9554771 , 0.9558342 , 0.95559627, 0.95571524, 0.9548822 ,\n",
       "        0.95595303, 0.95547701, 0.95559619, 0.95547723, 0.95571524,\n",
       "        0.95512034, 0.95571524, 0.95571524, 0.95547701, 0.9554771 ,\n",
       "        0.95607221, 0.95583407, 0.95595303]),\n",
       " 'param_criterion': masked_array(data=['gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42,\n",
       "                    45, 48, 51, 54, 57, 60, 63, 66, 69, 72, 75, 78, 81, 84,\n",
       "                    87, 90, 93, 96, 99, 102, 105, 108, 111, 114, 117, 120,\n",
       "                    123, 126, 129, 132, 135, 138, 141, 144, 147, 3, 6, 9,\n",
       "                    12, 15, 18, 21, 24, 27, 30, 33, 36, 39, 42, 45, 48, 51,\n",
       "                    54, 57, 60, 63, 66, 69, 72, 75, 78, 81, 84, 87, 90, 93,\n",
       "                    96, 99, 102, 105, 108, 111, 114, 117, 120, 123, 126,\n",
       "                    129, 132, 135, 138, 141, 144, 147],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_split': masked_array(data=[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'criterion': 'gini', 'max_depth': 3, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 6, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 9, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 12, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 15, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 18, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 21, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 24, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 27, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 30, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 33, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 36, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 39, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 42, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 45, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 48, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 51, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 54, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 57, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 60, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 63, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 66, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 69, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 72, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 75, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 78, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 81, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 84, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 87, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 90, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 93, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 96, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 99, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 102, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 105, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 108, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 111, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 114, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 117, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 120, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 123, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 126, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 129, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 132, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 135, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 138, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 141, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 144, 'min_samples_split': 10},\n",
       "  {'criterion': 'gini', 'max_depth': 147, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 3, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 6, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 9, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 12, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 15, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 18, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 21, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 24, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 27, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 33, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 36, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 39, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 42, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 45, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 48, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 51, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 54, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 57, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 60, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 63, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 66, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 69, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 72, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 75, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 78, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 81, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 84, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 87, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 90, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 93, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 96, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 99, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 102, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 105, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 108, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 111, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 114, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 117, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 120, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 123, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 126, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 129, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 132, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 135, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 138, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 141, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 144, 'min_samples_split': 10},\n",
       "  {'criterion': 'entropy', 'max_depth': 147, 'min_samples_split': 10}],\n",
       " 'rank_test_score': array([98, 96,  2, 14, 60, 26, 54, 63, 95, 58, 84, 91, 32, 17, 72, 12, 80,\n",
       "        75, 50, 20, 84, 63,  9, 36, 75, 89, 89, 42, 70, 54, 83, 27, 93, 32,\n",
       "        72, 36, 50, 72, 63, 87, 92, 42, 54, 46, 14, 80, 75, 32, 78, 97, 28,\n",
       "         1, 36,  6,  9, 32, 42, 78, 63, 46, 50, 28, 36, 46, 17, 28, 84, 21,\n",
       "         8, 36, 93,  7, 13, 60, 36, 16, 58,  3, 54, 70, 88, 25,  4, 60, 63,\n",
       "         9, 80, 21, 21, 63, 50, 63, 21, 46, 42,  4, 17, 28], dtype=int32),\n",
       " 'split0_test_score': array([0.50320741, 0.71632217, 0.74269423, 0.73841768, 0.74126871,\n",
       "        0.73841768, 0.73342837, 0.73770492, 0.72487527, 0.73200285,\n",
       "        0.73699216, 0.73485388, 0.74126871, 0.73984319, 0.73984319,\n",
       "        0.74198147, 0.73342837, 0.73414113, 0.74126871, 0.73770492,\n",
       "        0.73556664, 0.73485388, 0.73913043, 0.73699216, 0.73271561,\n",
       "        0.72915182, 0.72701354, 0.7362794 , 0.73414113, 0.74055595,\n",
       "        0.73841768, 0.74554526, 0.73200285, 0.73841768, 0.73485388,\n",
       "        0.73699216, 0.74269423, 0.73342837, 0.74126871, 0.73485388,\n",
       "        0.73485388, 0.73485388, 0.7362794 , 0.74126871, 0.73913043,\n",
       "        0.7362794 , 0.73485388, 0.73342837, 0.73699216, 0.53385602,\n",
       "        0.72487527, 0.73200285, 0.74126871, 0.75053457, 0.75053457,\n",
       "        0.74768354, 0.7448325 , 0.74198147, 0.74198147, 0.73841768,\n",
       "        0.74055595, 0.73913043, 0.74625802, 0.73984319, 0.73913043,\n",
       "        0.73841768, 0.73984319, 0.74340699, 0.74554526, 0.73984319,\n",
       "        0.73556664, 0.75196009, 0.7448325 , 0.74340699, 0.7448325 ,\n",
       "        0.7448325 , 0.73984319, 0.74554526, 0.74055595, 0.74269423,\n",
       "        0.73556664, 0.73984319, 0.74910905, 0.74126871, 0.74126871,\n",
       "        0.7448325 , 0.73984319, 0.74554526, 0.74625802, 0.7448325 ,\n",
       "        0.74982181, 0.74055595, 0.75124733, 0.74625802, 0.7448325 ,\n",
       "        0.74839629, 0.74269423, 0.74055595]),\n",
       " 'split0_train_score': array([0.50232392, 0.77118341, 0.88666428, 0.92956739, 0.94279585,\n",
       "        0.94243833, 0.94243833, 0.94243833, 0.9420808 , 0.94243833,\n",
       "        0.9420808 , 0.94279585, 0.94279585, 0.94315338, 0.94243833,\n",
       "        0.94315338, 0.94315338, 0.94136575, 0.94315338, 0.94243833,\n",
       "        0.94243833, 0.94172327, 0.94279585, 0.9420808 , 0.94243833,\n",
       "        0.9420808 , 0.9420808 , 0.94243833, 0.94243833, 0.94315338,\n",
       "        0.94243833, 0.94315338, 0.94172327, 0.94243833, 0.94315338,\n",
       "        0.9420808 , 0.9420808 , 0.9420808 , 0.9420808 , 0.9420808 ,\n",
       "        0.9420808 , 0.94243833, 0.94315338, 0.94243833, 0.94315338,\n",
       "        0.94315338, 0.94243833, 0.9420808 , 0.9435109 , 0.52913836,\n",
       "        0.78512692, 0.9020379 , 0.95495173, 0.95673936, 0.95673936,\n",
       "        0.95673936, 0.95673936, 0.95673936, 0.95673936, 0.95638184,\n",
       "        0.95673936, 0.95673936, 0.95673936, 0.95638184, 0.95638184,\n",
       "        0.95638184, 0.95638184, 0.95673936, 0.95638184, 0.95638184,\n",
       "        0.95638184, 0.95673936, 0.95673936, 0.95638184, 0.95638184,\n",
       "        0.95638184, 0.95673936, 0.95638184, 0.95673936, 0.95673936,\n",
       "        0.95638184, 0.95673936, 0.95673936, 0.95673936, 0.95638184,\n",
       "        0.95638184, 0.95638184, 0.95673936, 0.95673936, 0.95673936,\n",
       "        0.95673936, 0.95673936, 0.95673936, 0.95638184, 0.95638184,\n",
       "        0.95673936, 0.95638184, 0.95638184]),\n",
       " 'split1_test_score': array([0.4810579 , 0.70907791, 0.73695497, 0.73266619, 0.72051465,\n",
       "        0.72623302, 0.7312366 , 0.72337384, 0.7305218 , 0.7305218 ,\n",
       "        0.72122945, 0.72408863, 0.72265904, 0.72766262, 0.72122945,\n",
       "        0.72694782, 0.71979986, 0.72623302, 0.72337384, 0.72480343,\n",
       "        0.72051465, 0.7305218 , 0.73552538, 0.72980701, 0.72909221,\n",
       "        0.72051465, 0.72909221, 0.72909221, 0.7305218 , 0.72122945,\n",
       "        0.71979986, 0.72480343, 0.71765547, 0.72623302, 0.72551823,\n",
       "        0.7305218 , 0.72337384, 0.72551823, 0.72051465, 0.71979986,\n",
       "        0.71765547, 0.72909221, 0.72265904, 0.71408149, 0.72551823,\n",
       "        0.72337384, 0.72194425, 0.73266619, 0.72265904, 0.51679771,\n",
       "        0.72480343, 0.73695497, 0.71837026, 0.72051465, 0.72051465,\n",
       "        0.71837026, 0.71479628, 0.71408149, 0.7119371 , 0.72265904,\n",
       "        0.72122945, 0.72480343, 0.7119371 , 0.71908506, 0.72766262,\n",
       "        0.72337384, 0.71479628, 0.72408863, 0.72766262, 0.72122945,\n",
       "        0.7119371 , 0.72551823, 0.72408863, 0.7119371 , 0.72122945,\n",
       "        0.72337384, 0.71908506, 0.7312366 , 0.71336669, 0.72051465,\n",
       "        0.71765547, 0.72194425, 0.72980701, 0.7112223 , 0.71979986,\n",
       "        0.72623302, 0.70836312, 0.71479628, 0.72265904, 0.71837026,\n",
       "        0.71694067, 0.71551108, 0.71551108, 0.71408149, 0.72051465,\n",
       "        0.72623302, 0.72480343, 0.72194425]),\n",
       " 'split1_train_score': array([0.49803642, 0.76651196, 0.88182792, 0.93181007, 0.94466262,\n",
       "        0.94787576, 0.94858979, 0.94858979, 0.94858979, 0.9489468 ,\n",
       "        0.94930382, 0.94823277, 0.94930382, 0.94858979, 0.94858979,\n",
       "        0.9489468 , 0.94930382, 0.9489468 , 0.9489468 , 0.94858979,\n",
       "        0.9489468 , 0.94823277, 0.94930382, 0.9489468 , 0.94823277,\n",
       "        0.94858979, 0.94823277, 0.9489468 , 0.94823277, 0.9489468 ,\n",
       "        0.94930382, 0.9489468 , 0.9489468 , 0.94930382, 0.94966084,\n",
       "        0.9489468 , 0.94930382, 0.9489468 , 0.9489468 , 0.94858979,\n",
       "        0.9489468 , 0.9489468 , 0.94930382, 0.9489468 , 0.9489468 ,\n",
       "        0.94930382, 0.9489468 , 0.94966084, 0.94966084, 0.52909675,\n",
       "        0.80721171, 0.90789004, 0.95644413, 0.95858622, 0.95930025,\n",
       "        0.95858622, 0.95858622, 0.95787219, 0.95787219, 0.95930025,\n",
       "        0.95858622, 0.95858622, 0.95858622, 0.95858622, 0.95787219,\n",
       "        0.95930025, 0.95858622, 0.95858622, 0.95930025, 0.95858622,\n",
       "        0.95787219, 0.95858622, 0.95930025, 0.95787219, 0.95858622,\n",
       "        0.95930025, 0.95930025, 0.95858622, 0.95787219, 0.95787219,\n",
       "        0.95858622, 0.95858622, 0.95858622, 0.95858622, 0.95787219,\n",
       "        0.95930025, 0.95787219, 0.95787219, 0.95787219, 0.95858622,\n",
       "        0.95787219, 0.95858622, 0.95858622, 0.95787219, 0.95858622,\n",
       "        0.95930025, 0.95930025, 0.95930025]),\n",
       " 'split2_test_score': array([0.48211731, 0.70386266, 0.72031474, 0.71316166, 0.71101574,\n",
       "        0.71530758, 0.70958512, 0.71101574, 0.70243205, 0.71101574,\n",
       "        0.70815451, 0.70171674, 0.71387697, 0.71530758, 0.70958512,\n",
       "        0.7167382 , 0.71459227, 0.70958512, 0.71030043, 0.71959943,\n",
       "        0.71030043, 0.70672389, 0.71173104, 0.71030043, 0.70815451,\n",
       "        0.71244635, 0.70600858, 0.71101574, 0.70672389, 0.71244635,\n",
       "        0.70886981, 0.70886981, 0.70886981, 0.71316166, 0.71030043,\n",
       "        0.70958512, 0.70886981, 0.71173104, 0.71030043, 0.71101574,\n",
       "        0.70672389, 0.71244635, 0.71530758, 0.72031474, 0.71959943,\n",
       "        0.70815451, 0.71316166, 0.71173104, 0.70886981, 0.50858369,\n",
       "        0.72889843, 0.73319027, 0.71745351, 0.71959943, 0.71530758,\n",
       "        0.71173104, 0.7167382 , 0.71244635, 0.71816881, 0.71459227,\n",
       "        0.71316166, 0.71459227, 0.71888412, 0.7167382 , 0.71602289,\n",
       "        0.7167382 , 0.71173104, 0.71387697, 0.71387697, 0.71602289,\n",
       "        0.71101574, 0.71244635, 0.71602289, 0.71745351, 0.71101574,\n",
       "        0.71530758, 0.71459227, 0.72103004, 0.72031474, 0.70815451,\n",
       "        0.71030043, 0.71888412, 0.71602289, 0.72031474, 0.71101574,\n",
       "        0.71530758, 0.71959943, 0.72103004, 0.71244635, 0.70886981,\n",
       "        0.70815451, 0.71602289, 0.71459227, 0.71530758, 0.71101574,\n",
       "        0.72031474, 0.71530758, 0.71602289]),\n",
       " 'split2_train_score': array([0.5028551 , 0.78658101, 0.87187723, 0.92790864, 0.93540328,\n",
       "        0.93754461, 0.93861527, 0.93825839, 0.93754461, 0.93825839,\n",
       "        0.93825839, 0.93861527, 0.93861527, 0.93861527, 0.93825839,\n",
       "        0.93825839, 0.93861527, 0.93861527, 0.93861527, 0.93825839,\n",
       "        0.93825839, 0.93861527, 0.9379015 , 0.93825839, 0.93861527,\n",
       "        0.93754461, 0.93825839, 0.93754461, 0.93861527, 0.9379015 ,\n",
       "        0.93825839, 0.93825839, 0.93825839, 0.93825839, 0.93861527,\n",
       "        0.93825839, 0.93825839, 0.9379015 , 0.93825839, 0.93825839,\n",
       "        0.9379015 , 0.93825839, 0.93754461, 0.93861527, 0.93825839,\n",
       "        0.93718772, 0.93825839, 0.9379015 , 0.93754461, 0.52997859,\n",
       "        0.79514632, 0.90292648, 0.94896502, 0.95110635, 0.95039258,\n",
       "        0.95182013, 0.95146324, 0.95182013, 0.95217702, 0.95182013,\n",
       "        0.95074946, 0.95146324, 0.9525339 , 0.95182013, 0.95182013,\n",
       "        0.95217702, 0.95182013, 0.95182013, 0.95146324, 0.95039258,\n",
       "        0.95217702, 0.95146324, 0.95217702, 0.95182013, 0.95182013,\n",
       "        0.95110635, 0.95182013, 0.95182013, 0.95182013, 0.95217702,\n",
       "        0.95146324, 0.95217702, 0.95146324, 0.95182013, 0.95039258,\n",
       "        0.95217702, 0.95217702, 0.95217702, 0.95182013, 0.95182013,\n",
       "        0.95074946, 0.95182013, 0.95182013, 0.95217702, 0.95146324,\n",
       "        0.95217702, 0.95182013, 0.95217702]),\n",
       " 'std_fit_time': array([0.0066917 , 0.01426406, 0.01451935, 0.01882185, 0.02765478,\n",
       "        0.02382551, 0.03380127, 0.04418861, 0.01742986, 0.05997218,\n",
       "        0.03888761, 0.0481644 , 0.04493585, 0.04394089, 0.04661738,\n",
       "        0.05135326, 0.04837869, 0.03623352, 0.03033937, 0.03729346,\n",
       "        0.03993746, 0.04577801, 0.04202471, 0.04938392, 0.03588174,\n",
       "        0.02421094, 0.0027094 , 0.02982529, 0.0218808 , 0.03478757,\n",
       "        0.02592333, 0.02216859, 0.02657065, 0.03848529, 0.02876909,\n",
       "        0.00828224, 0.06030016, 0.04149317, 0.02983375, 0.0342321 ,\n",
       "        0.00564835, 0.03785097, 0.03712824, 0.03887247, 0.04839757,\n",
       "        0.03381929, 0.03743518, 0.03867167, 0.0390379 , 0.00810244,\n",
       "        0.00621456, 0.01345566, 0.03960579, 0.02955448, 0.02314194,\n",
       "        0.04493922, 0.03779705, 0.02010351, 0.02939881, 0.02312212,\n",
       "        0.02871543, 0.0253963 , 0.02373659, 0.03651457, 0.01554855,\n",
       "        0.04057656, 0.05947991, 0.02307908, 0.02649971, 0.02227473,\n",
       "        0.04785131, 0.02648107, 0.02926636, 0.03568012, 0.02622963,\n",
       "        0.02400801, 0.03486213, 0.02142911, 0.03520823, 0.02509072,\n",
       "        0.02584035, 0.04838139, 0.02689257, 0.02331824, 0.03524903,\n",
       "        0.00986366, 0.03008308, 0.02628867, 0.02804557, 0.02795106,\n",
       "        0.04876736, 0.03103346, 0.02748692, 0.02752   , 0.02096689,\n",
       "        0.02809087, 0.03165626, 0.02574221]),\n",
       " 'std_score_time': array([3.37047387e-05, 3.43606145e-04, 1.35466286e-04, 3.51573775e-05,\n",
       "        6.16292427e-05, 6.76409752e-05, 4.34097899e-05, 2.79796390e-05,\n",
       "        2.40634265e-04, 1.85434587e-04, 2.20095375e-04, 1.68855175e-03,\n",
       "        5.17142203e-04, 2.83090403e-04, 5.95444831e-04, 1.90971380e-04,\n",
       "        2.33517905e-04, 1.03930144e-04, 9.64345437e-05, 4.54809727e-04,\n",
       "        1.69592885e-04, 1.20608908e-04, 1.66362684e-04, 6.14427114e-04,\n",
       "        2.68112856e-04, 4.85111345e-05, 2.33180927e-04, 3.04168684e-04,\n",
       "        2.88550862e-04, 2.88030783e-04, 8.55924544e-05, 1.55395882e-04,\n",
       "        4.10487899e-05, 2.71139851e-04, 2.46795234e-04, 3.36002303e-04,\n",
       "        3.04467340e-04, 2.95157964e-05, 1.62520618e-04, 1.66090938e-04,\n",
       "        2.90823565e-04, 7.67102902e-05, 2.23761168e-04, 2.60508591e-05,\n",
       "        1.19000411e-04, 1.34592209e-04, 3.87389230e-05, 9.67910918e-05,\n",
       "        1.31415192e-04, 4.89381571e-04, 4.33270696e-05, 8.97140527e-05,\n",
       "        6.27148127e-05, 6.99533636e-05, 2.92018111e-04, 4.78904270e-05,\n",
       "        1.69852225e-04, 1.71307800e-05, 4.74128714e-05, 1.80915876e-04,\n",
       "        1.84602948e-04, 6.74114449e-05, 1.20369038e-04, 9.88733726e-05,\n",
       "        1.13961476e-04, 7.89697138e-05, 1.41060140e-04, 1.25585024e-04,\n",
       "        4.98225962e-04, 2.69081067e-05, 5.87724231e-05, 3.77810046e-04,\n",
       "        1.34592913e-04, 3.60647452e-05, 9.93196464e-05, 2.05549819e-04,\n",
       "        9.36650341e-04, 4.04026778e-04, 8.36583129e-05, 9.56099790e-05,\n",
       "        6.79879980e-05, 7.87207267e-04, 5.97535499e-05, 1.11743023e-04,\n",
       "        3.53438798e-04, 3.73037155e-04, 7.25237975e-04, 1.40459416e-04,\n",
       "        5.68891988e-04, 7.91386890e-05, 9.28599472e-05, 1.50103300e-04,\n",
       "        8.56585454e-05, 2.82848653e-04, 1.71623954e-04, 8.70007512e-05,\n",
       "        2.69681544e-04, 7.33973918e-05]),\n",
       " 'std_test_score': array([0.01020637, 0.00511039, 0.00948969, 0.01080668, 0.01263714,\n",
       "        0.0094413 , 0.01075741, 0.01090814, 0.01212587, 0.00956057,\n",
       "        0.01179269, 0.01380152, 0.01142422, 0.01001844, 0.01246536,\n",
       "        0.01037102, 0.00794504, 0.010234  , 0.0126971 , 0.00761396,\n",
       "        0.01038069, 0.01236384, 0.01215288, 0.01127607, 0.01082319,\n",
       "        0.00682264, 0.01042199, 0.01062751, 0.01215861, 0.01174609,\n",
       "        0.01220212, 0.01501932, 0.00953754, 0.01031446, 0.01011965,\n",
       "        0.01169526, 0.01385894, 0.00896566, 0.01288914, 0.00984662,\n",
       "        0.01158212, 0.00949965, 0.00869123, 0.01163496, 0.00818023,\n",
       "        0.01149635, 0.00891162, 0.01005005, 0.01148378, 0.0105297 ,\n",
       "        0.00191298, 0.00211113, 0.01102267, 0.0143797 , 0.01553262,\n",
       "        0.01562754, 0.01373186, 0.0135611 , 0.01295355, 0.00989768,\n",
       "        0.01149868, 0.01006727, 0.01482351, 0.0103882 , 0.00943534,\n",
       "        0.00907323, 0.01259838, 0.01224939, 0.01296781, 0.01022973,\n",
       "        0.01136845, 0.01644223, 0.01214003, 0.0137283 , 0.01416719,\n",
       "        0.01246505, 0.01100363, 0.01005763, 0.01153969, 0.01429422,\n",
       "        0.0106149 , 0.00924824, 0.01357354, 0.01258726, 0.01271245,\n",
       "        0.01219237, 0.01303199, 0.01327926, 0.014165  , 0.0152226 ,\n",
       "        0.0179417 , 0.01169378, 0.01707595, 0.01489563, 0.01424634,\n",
       "        0.01209223, 0.01135819, 0.0104572 ]),\n",
       " 'std_train_score': array([0.00215728, 0.00857435, 0.00615597, 0.00159869, 0.0039982 ,\n",
       "        0.00421962, 0.00410889, 0.0042433 , 0.00453309, 0.00439791,\n",
       "        0.00457996, 0.00393748, 0.00439793, 0.00407758, 0.0042433 ,\n",
       "        0.00436866, 0.0043801 , 0.0043688 , 0.00422819, 0.0042433 ,\n",
       "        0.00439791, 0.00400735, 0.00467049, 0.00442211, 0.00395373,\n",
       "        0.00453309, 0.00410888, 0.00467046, 0.00395373, 0.00451103,\n",
       "        0.00455349, 0.00436866, 0.00445255, 0.00455349, 0.00453316,\n",
       "        0.00442211, 0.00457996, 0.00455348, 0.00442211, 0.00426505,\n",
       "        0.00455348, 0.00439791, 0.00480237, 0.00426506, 0.00436866,\n",
       "        0.00494657, 0.00439791, 0.00486719, 0.00494662, 0.00040625,\n",
       "        0.00902896, 0.00257497, 0.00323187, 0.00318137, 0.00374443,\n",
       "        0.00285559, 0.00301819, 0.00262699, 0.00246156, 0.00307821,\n",
       "        0.00334505, 0.00301819, 0.00253262, 0.00281757, 0.00257461,\n",
       "        0.00292381, 0.00281757, 0.00285559, 0.00323399, 0.00346196,\n",
       "        0.00241147, 0.00301819, 0.00294606, 0.00257461, 0.00281757,\n",
       "        0.00339097, 0.00310393, 0.00281757, 0.00262699, 0.00246156,\n",
       "        0.00297748, 0.00269369, 0.00301819, 0.00285559, 0.00323242,\n",
       "        0.00292381, 0.00241147, 0.00246156, 0.00262699, 0.00285559,\n",
       "        0.00312508, 0.00285559, 0.00285559, 0.00241147, 0.00297748,\n",
       "        0.00294606, 0.00307821, 0.00292381])}"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up the parameter grid to seaerch\n",
    "param_grid ={'criterion': ['gini', \"entropy\"], \\\n",
    "             'max_depth': list(range(3, 150, 3)), \\\n",
    "             'min_samples_split': [10]}\n",
    "\n",
    "# Perform the search\n",
    "my_tuned_tree = GridSearchCV(tree.DecisionTreeClassifier(), \\\n",
    "                                param_grid, cv=cv_folds, verbose = 2, \\\n",
    "                            return_train_score=True)\n",
    "my_tuned_tree.fit(X_train, y_train)\n",
    "\n",
    "# Print details\n",
    "print(\"Best parameters set found on development set:\")\n",
    "display(my_tuned_tree.best_params_)\n",
    "model_tuned_params_list[\"Tuned Tree\"] = my_tuned_tree.best_params_\n",
    "display(my_tuned_tree.best_score_)\n",
    "display(my_tuned_tree.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uJtP3VVjdnyq"
   },
   "source": [
    "Test the tuned tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "id": "dijoH-PCdnyr",
    "outputId": "4322870c-f7c8-4d79-e4d5-f34058b52529"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.735\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.68      0.70       171\n",
      "           1       0.92      0.91      0.92       185\n",
      "           2       0.72      0.52      0.60       194\n",
      "           3       0.80      0.77      0.79       195\n",
      "           4       0.49      0.60      0.54       145\n",
      "           5       0.83      0.83      0.83       185\n",
      "           6       0.42      0.52      0.46       171\n",
      "           7       0.83      0.74      0.78       178\n",
      "           8       0.84      0.84      0.84       170\n",
      "           9       0.81      0.89      0.85       206\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      1800\n",
      "   macro avg       0.74      0.73      0.73      1800\n",
      "weighted avg       0.75      0.73      0.74      1800\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>116</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "      <td>4</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>151</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>3</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>183</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>161</td>\n",
       "      <td>183</td>\n",
       "      <td>140</td>\n",
       "      <td>189</td>\n",
       "      <td>176</td>\n",
       "      <td>184</td>\n",
       "      <td>213</td>\n",
       "      <td>159</td>\n",
       "      <td>169</td>\n",
       "      <td>226</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1    2    3    4    5    6    7    8    9   All\n",
       "True                                                             \n",
       "0          116    3    3    7    4    0   33    0    5    0   171\n",
       "1            2  169    2    8    1    0    3    0    0    0   185\n",
       "2            2    1  101    4   49    0   31    0    3    3   194\n",
       "3            8    6    1  151    8    1   17    0    3    0   195\n",
       "4            3    0   11   10   87    0   30    0    3    1   145\n",
       "5            0    1    0    3    0  153    0   16    5    7   185\n",
       "6           26    2   17    1   25    1   89    0    7    3   171\n",
       "7            0    0    0    0    0   20    0  132    0   26   178\n",
       "8            4    0    5    3    2    4    7    0  142    3   170\n",
       "9            0    1    0    2    0    5    3   11    1  183   206\n",
       "All        161  183  140  189  176  184  213  159  169  226  1800"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_tuned_tree.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) # , normalize=True, sample_weight=None\n",
    "model_test_accuracy_comparisons[\"Tuned Tree\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJ8r8jYHdnyu"
   },
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "leQvuKzednyv"
   },
   "source": [
    "Train the bagging model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "id": "0ZwfhptBdnyw",
    "outputId": "59732926-0b3d-4b33-fbe7-5eb42fa80e0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=50, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "         max_samples=1.0, n_estimators=10, n_jobs=None, oob_score=False,\n",
       "         random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_bagmodel = ensemble.BaggingClassifier(base_estimator = tree.DecisionTreeClassifier(criterion=\"entropy\", min_samples_leaf = 50), \\\n",
    "                                      n_estimators=10)\n",
    "my_bagmodel.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x6Ifx3ncdny0"
   },
   "source": [
    "Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "id": "cUsoP7UJdny0",
    "outputId": "3862018f-9152-4ec0-f613-8ccada8c1c52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7472222222222222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.74      0.74       171\n",
      "           1       0.96      0.85      0.91       185\n",
      "           2       0.77      0.55      0.64       194\n",
      "           3       0.74      0.86      0.80       195\n",
      "           4       0.49      0.74      0.59       145\n",
      "           5       0.82      0.72      0.77       185\n",
      "           6       0.53      0.42      0.46       171\n",
      "           7       0.83      0.80      0.81       178\n",
      "           8       0.81      0.88      0.84       170\n",
      "           9       0.81      0.89      0.85       206\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      1800\n",
      "   macro avg       0.75      0.74      0.74      1800\n",
      "weighted avg       0.76      0.75      0.75      1800\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>167</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>134</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>171</td>\n",
       "      <td>164</td>\n",
       "      <td>137</td>\n",
       "      <td>225</td>\n",
       "      <td>219</td>\n",
       "      <td>164</td>\n",
       "      <td>135</td>\n",
       "      <td>172</td>\n",
       "      <td>185</td>\n",
       "      <td>228</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1    2    3    4    5    6    7    8    9   All\n",
       "True                                                             \n",
       "0          126    2    1   11    1    1   20    0    9    0   171\n",
       "1            0  158    3   22    0    0    1    0    1    0   185\n",
       "2            0    0  106    2   66    0   15    0    5    0   194\n",
       "3            5    0    0  167    5    3   12    0    3    0   195\n",
       "4            0    2    7   16  108    1   11    0    0    0   145\n",
       "5            1    0    0    0    0  134    1   16    8   25   185\n",
       "6           38    1   15    5   30    1   71    0   10    0   171\n",
       "7            0    0    0    0    0   17    0  142    0   19   178\n",
       "8            0    1    3    1    9    3    4    0  149    0   170\n",
       "9            1    0    2    1    0    4    0   14    0  184   206\n",
       "All        171  164  137  225  219  164  135  172  185  228  1800"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of predictions for the validation data\n",
    "y_pred = my_bagmodel.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) # , normalize=True, sample_weight=None\n",
    "model_test_accuracy_comparisons[\"Bagging\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bb73PmE0dny2"
   },
   "source": [
    "Choose parameters using a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3345
    },
    "colab_type": "code",
    "id": "klkwftyFdny3",
    "outputId": "84d67dad-ce22-4117-bdeb-6e10965b923d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=10, total=  10.4s\n",
      "[CV] base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   10.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=10, total=  11.0s\n",
      "[CV] base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=10 \n",
      "[CV]  base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=10, total=  10.8s\n",
      "[CV] base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=30 \n",
      "[CV]  base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=30, total=  31.9s\n",
      "[CV] base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=30 \n",
      "[CV]  base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=30, total=  32.1s\n",
      "[CV] base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=30 \n",
      "[CV]  base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=30, total=  31.4s\n",
      "[CV] base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=50 \n",
      "[CV]  base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=50, total=  52.6s\n",
      "[CV] base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=50 \n",
      "[CV]  base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=50, total=  53.1s\n",
      "[CV] base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=50 \n",
      "[CV]  base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=50, total=  52.5s\n",
      "[CV] base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=70 \n",
      "[CV]  base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=70, total= 1.2min\n",
      "[CV] base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=70 \n",
      "[CV]  base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=70, total= 1.2min\n",
      "[CV] base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=70 \n",
      "[CV]  base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=70, total= 1.2min\n",
      "[CV] base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=90 \n",
      "[CV]  base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=90, total= 1.6min\n",
      "[CV] base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=90 \n",
      "[CV]  base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=90, total= 1.6min\n",
      "[CV] base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=90 \n",
      "[CV]  base_estimator=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), n_estimators=90, total= 1.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed: 13.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "{'base_estimator': DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=10, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'), 'n_estimators': 90}\n",
      "0.7830952380952381\n"
     ]
    }
   ],
   "source": [
    "# Set up the parameter grid to seaerch\n",
    "param_grid = [\n",
    " {'n_estimators': list(range(10, 100, 20)),\n",
    "  'base_estimator': [tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth = 6, min_samples_leaf = 10)]}\n",
    "]\n",
    "\n",
    "# Perform the search\n",
    "my_tuned_model = GridSearchCV(ensemble.BaggingClassifier(), param_grid, cv=cv_folds, verbose = 2)\n",
    "my_tuned_model.fit(X_train, y_train)\n",
    "\n",
    "# Print details\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(my_tuned_model.best_params_)\n",
    "model_tuned_params_list[\"Tuned Bagging\"] = my_tuned_model.best_params_\n",
    "print(my_tuned_model.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uKaW25FBdny5"
   },
   "source": [
    "Test on tuned bagging model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "id": "_YRP7nIidny5",
    "outputId": "4154f7c7-936a-41bd-ea6d-f261791cecc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7844444444444445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.77      0.77       171\n",
      "           1       0.99      0.88      0.93       185\n",
      "           2       0.78      0.53      0.63       194\n",
      "           3       0.75      0.88      0.81       195\n",
      "           4       0.50      0.77      0.61       145\n",
      "           5       0.91      0.85      0.88       185\n",
      "           6       0.56      0.45      0.50       171\n",
      "           7       0.83      0.85      0.84       178\n",
      "           8       0.89      0.95      0.92       170\n",
      "           9       0.88      0.90      0.89       206\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      1800\n",
      "   macro avg       0.79      0.78      0.78      1800\n",
      "weighted avg       0.79      0.78      0.78      1800\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>162</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>4</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>172</td>\n",
       "      <td>164</td>\n",
       "      <td>130</td>\n",
       "      <td>228</td>\n",
       "      <td>220</td>\n",
       "      <td>174</td>\n",
       "      <td>137</td>\n",
       "      <td>183</td>\n",
       "      <td>181</td>\n",
       "      <td>211</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1    2    3    4    5    6    7    8    9   All\n",
       "True                                                             \n",
       "0          132    1    1    9    1    1   20    0    6    0   171\n",
       "1            0  162    3   18    0    0    1    0    1    0   185\n",
       "2            0    0  102    4   71    1   12    0    4    0   194\n",
       "3            3    0    0  172    5    1   12    0    2    0   195\n",
       "4            0    0    6   15  111    0   13    0    0    0   145\n",
       "5            1    1    0    0    0  158    0   15    3    7   185\n",
       "6           36    0   16    6   30    2   77    0    4    0   171\n",
       "7            0    0    0    0    0    7    0  152    0   19   178\n",
       "8            0    0    1    2    2    2    2    0  161    0   170\n",
       "9            0    0    1    2    0    2    0   16    0  185   206\n",
       "All        172  164  130  228  220  174  137  183  181  211  1800"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_tuned_model.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) # , normalize=True, sample_weight=None\n",
    "model_test_accuracy_comparisons[\"Tuned Bagging\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "\n",
    "pd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rJiERJfEdny9"
   },
   "source": [
    "Plot the comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "FIkMOl1XCo8M",
    "outputId": "3190a458-db46-49d4-ecea-3f0048c64c59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Used for comparision training data X_train :  (4200, 784) testing data X_test (1800, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2gAAAEZCAYAAADi9wZwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH7xcOUSNlo/FAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with highest accuracy: StackedEnsembleKFold_StackLayerModel_logreg  Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Used for comparision training data X_train : \",X_train.shape,\"testing data X_test\",X_test.shape)\n",
    "plt.xlim(0, 1.0)\n",
    "_ = plt.barh(range(len(model_test_accuracy_comparisons)), list(model_test_accuracy_comparisons.values()), align='center')\n",
    "_ = plt.yticks(range(len(model_test_accuracy_comparisons)), list(model_test_accuracy_comparisons.keys()))\n",
    "_ = plt.title(\"Comparision of StackedEnsembleClassifier, StackedEnsembleHoldOut, StackedEnsembleKfold, Bagging Ensemble, Decision tree \",fontsize=14)\n",
    "_ = plt.xlabel('Accuracy',fontsize=12)\n",
    "_ = plt.ylabel('Classifier Name',fontsize=12)\n",
    "_ = plt.tick_params(axis='both', which='major', labelsize=11)\n",
    "#displaying labels for each bar\n",
    "for i, v in enumerate(list(model_test_accuracy_comparisons.values())):\n",
    "    a=round(v,2)\n",
    "    plt.text(v+0.01 , i , str(a), color='black', fontweight='bold')\n",
    "plt.show()\n",
    "newD = {k:round(v,2) for k, v in model_test_accuracy_comparisons.items()}\n",
    "haccm = list(max(zip(newD.values(), newD.keys())))\n",
    "print(\"Model with highest accuracy:\",haccm[1],\" Accuracy:\",haccm[0])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph shows a comparison of StackedEnsembleClassifier, StackedEnsembleHoldOut, StackedEnsembleKFold, BaggingEnsemble and Decision Tree. The three StackedEnsembles were run after modifying the stack layer with decision tree and logistic regression. For bagging and decision tree classifier, grid search was used to tune parameters and run to get outputs for tuned bagging and tuned tree. As is seen in the graph, the highest accuracy was still given by StackedEnsembleKFold with a stack layer model of logistic regression at 84%. The accuracy of bagging and decision tree was low compared to the StackedEnsembles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EsH-UJpDdnzA"
   },
   "source": [
    "## Task 5: Implement the StackedEnsembleOneVsOne Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VF4pmhgednzB"
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "# Create a new classifier which is based on the sckit-learn BaseEstimator and ClassifierMixin classes\n",
    "class StackedEnsembleOneVsOne(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    \"\"\"An ensemble classifier that uses heterogeneous models at the base layer and a aggregatnio model at the aggregation layer. A k-fold cross validation is used to gnerate training data for the stack layer model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimators: list \n",
    "        A list of the classifiers in the base layer of the ensemble. Supported types are\n",
    "        - \"svm\" Support Vector Machine implemented by sklearn.svm.SVC\n",
    "        - \"logreg\" Logistic Regression implemented by sklearn.linear_models.LogisticRegression\n",
    "        - \"knn\" k Nearest Neighbour implemented by sklearn.neighbors.KNeighborsClassifier\n",
    "        - \"tree\" Decision Tree implemented by sklearn.tree.DecisionTreeClassifier\n",
    "        - \"randomforest\" RandomForest implemented by sklearn.tree.RandomForestClassifier    \n",
    "    classifier_duplicates: int, optional (default = 1)\n",
    "        How many instances of each classifier type listed in base_estimators is included in the ensemble\n",
    "    stack_layer_classifier: string, optional (default = \"logreg')\n",
    "        The classifier type used at the stack layer. The same classifier types as are supported at the base layer are supported        \n",
    "    training_folds: int, optional (default = 4)\n",
    "        How many folds will be used to generate the training set for the stacked layer\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : array of shape = [n_classes] \n",
    "        The classes labels (single output problem).\n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The default values for most base learners are used.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    \n",
    "    ----------\n",
    "    .. [1]  van der Laan, M., Polley, E. & Hubbard, A. (2007). \n",
    "            Super Learner. Statistical Applications in Genetics \n",
    "            and Molecular Biology, 6(1) \n",
    "            doi:10.2202/1544-6115.1309\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_iris\n",
    "    >>> from sklearn.model_selection import cross_val_score\n",
    "    >>> clf = StackedEnsembleClassifier()\n",
    "    >>> iris = load_iris()\n",
    "    >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
    "\n",
    "    \"\"\"\n",
    "    # Constructor for the classifier object\n",
    "    def __init__(self, base_estimator_types = \"svm\", base_estimator_duplicates = 8, stack_layer_classifier_type = \"logreg\"):\n",
    "        \"\"\"Setup a SuperLearner classifier .\n",
    "        Parameters\n",
    "        ----------\n",
    "        base_estimator_types: The types of classifiers to include at the base layer\n",
    "        base_estimator_duplicates: The number of duplicates of each type of classiifer to include\n",
    "        stack_layer_classifier_type: The type of classifier to include at the stack layer \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Nothing\n",
    "        \"\"\"     \n",
    "\n",
    "        # Initialise class variabels\n",
    "        self.base_estimator_types = base_estimator_types\n",
    "        self.base_estimator_type_list = list()\n",
    "        self.base_estimator_duplicates = base_estimator_duplicates\n",
    "        self.stack_layer_classifier_type = stack_layer_classifier_type\n",
    "\n",
    "    # The fit function to train a classifier\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build a SuperLearner classifier from the training set (X, y).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            The training input samples. \n",
    "        y : array-like, shape = [n_samples] \n",
    "            The target values (class labels) as integers or strings.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"    \n",
    "        \n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "        \n",
    "        #store all the pairs of the classes. Calculated the combination of unique classes\n",
    "        pairs = list(itertools.combinations(self.classes_, 2)) \n",
    "        \n",
    "        ########################\n",
    "        # LEVEL 0\n",
    "        ########################\n",
    "        \n",
    "        # Set up the base classifeirs in the ensemble\n",
    "        self.classifiers_ = list()\n",
    "        \n",
    "        # Set up empty arrays to hold stack layer training data\n",
    "        self.X_stack_train = None #(dtype = float)\n",
    "        self.y_stack_train = y\n",
    "        \n",
    "        \n",
    "        clf=self.base_estimator_types\n",
    "        \n",
    "        #for each pair a classifier is created and is trained on data having class in the pair.\n",
    "        #and then the classifier is made a make a prediction on entire data set, its ypred is used as training for stack layes\n",
    "        for pair in pairs:\n",
    "            \n",
    "            #create classifier\n",
    "            c = create_classifier(clf, tree_min_samples_split=math.ceil(len(X)*0.05))\n",
    "            \n",
    "            X_train=None\n",
    "            y_train=None\n",
    "            index=None\n",
    "            \n",
    "            #get index of class for the given pair\n",
    "            index = np.array(np.where(np.isin(y,pair)))\n",
    "              \n",
    "            #from the indexex get the data X and y    \n",
    "            for ind, x in enumerate(X):\n",
    "                x=np.reshape(x,(1,-1))\n",
    "                if ind in index:\n",
    "                    try:\n",
    "                        X_train=np.concatenate((X_train,x),axis=0)                    \n",
    "                    except ValueError:\n",
    "                        X_train=x\n",
    "            \n",
    "            for ind, yi in enumerate(y):\n",
    "                yi=np.reshape(yi,(1,-1))\n",
    "                if ind in index:\n",
    "                    try:\n",
    "                        y_train=np.concatenate((y_train,yi),axis=0)                    \n",
    "                    except ValueError:\n",
    "                        y_train=yi\n",
    "            \n",
    "            #Rsample the data\n",
    "            #X_train_samp, y_train_samp = resample(X_train, y_train, replace=True)    \n",
    "            \n",
    "            # Train a base classifier np.ravel is used to supress warning thrown by svm\n",
    "            c.fit(X_train, np.ravel(y_train,order='C'))\n",
    "            \n",
    "            #store the classifiers\n",
    "            self.classifiers_.append(c)\n",
    "            \n",
    "            # Make predictions for all instances in the training set\n",
    "            y_pred = c.predict_proba(X)\n",
    "\n",
    "            # Append the predictions ot the stack layer traing set (a bit of hacking here!)\n",
    "            try:\n",
    "                self.X_stack_train = np.c_[self.X_stack_train, y_pred]\n",
    "            except ValueError:\n",
    "                self.X_stack_train = y_pred\n",
    "            \n",
    "                                  \n",
    "            \n",
    "        # Store the number of classifers in the ensemble\n",
    "        self.n_estimators_ = len(self.classifiers_)\n",
    "              \n",
    "     \n",
    "        ########################\n",
    "        # LEVEL 1\n",
    "        ########################\n",
    "        \n",
    "        # Create the stack layer classifier\n",
    "        self.stack_layer_classifier_ = create_classifier(self.stack_layer_classifier_type, tree_min_samples_split=math.ceil(len(X)*0.05))\n",
    "\n",
    "        # Train the stack layer using the newly created dataset\n",
    "        self.stack_layer_classifier_.fit(self.X_stack_train, self.y_stack_train)\n",
    "\n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels of the input samples X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like matrix of shape = [n_samples, n_features]\n",
    "            The input samples. \n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, ].\n",
    "            The predicted class labels of the input samples. \n",
    "        \"\"\"\n",
    "        \n",
    "        # Check is fit had been called by confirming that the teamplates_ dictiponary has been set up\n",
    "        check_is_fitted(self, ['stack_layer_classifier_'])\n",
    "        \n",
    "\n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        X = check_array(X)\n",
    "        \n",
    "   \n",
    "        X_stack_queries = None\n",
    "              \n",
    "        # Make a prediction with each base classifier and assemble the stack layer query\n",
    "        for classifier in self.classifiers_:\n",
    "            \n",
    "            y_pred = classifier.predict_proba(X)\n",
    "            \n",
    "            try:\n",
    "                X_stack_queries = np.c_[X_stack_queries, y_pred]\n",
    "            except ValueError:\n",
    "                X_stack_queries = y_pred\n",
    "        \n",
    "        # Return the prediction made by the stack layer classifier\n",
    "        return self.stack_layer_classifier_.predict(X_stack_queries)\n",
    "    \n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities of the input samples X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like matrix of shape = [n_samples, n_features]\n",
    "            The input samples. \n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, n_labels].\n",
    "            The predicted class label probabilities of the input samples. \n",
    "        \"\"\"\n",
    "        # Check is fit had been called by confirming that the teamplates_ dictiponary has been set up\n",
    "        check_is_fitted(self, ['stack_layer_classifier_'])\n",
    "\n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        X = check_array(X)\n",
    "        \n",
    "        X_stack_queries = None\n",
    "        \n",
    "        # Make a prediction with each base classifier\n",
    "        for classifier in self.classifiers_:\n",
    "            \n",
    "            y_pred = classifier.predict_proba(X)\n",
    "                \n",
    "            try:\n",
    "                X_stack_queries = np.c_[X_stack_queries, y_pred]\n",
    "            except ValueError:\n",
    "                X_stack_queries = y_pred\n",
    "\n",
    "        # Return the prediction made by the stack layer classifier        \n",
    "        return self.stack_layer_classifier_.predict_proba(X_stack_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J_S9lneNdnzC"
   },
   "source": [
    "\n",
    "## Test the StackedEnsembleOneVsOne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-38nAm9hdnzD"
   },
   "source": [
    "Perform a simple test using the StackedEnsembleOneVsOne on the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "colab_type": "code",
    "id": "ufWblgbudnzD",
    "outputId": "7ea61b74-f655-47d6-a763-d27da6427b5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        50\n",
      "           1       1.00      0.96      0.98        50\n",
      "           2       0.96      1.00      0.98        50\n",
      "\n",
      "   micro avg       0.99      0.99      0.99       150\n",
      "   macro avg       0.99      0.99      0.99       150\n",
      "weighted avg       0.99      0.99      0.99       150\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>50</td>\n",
       "      <td>48</td>\n",
       "      <td>52</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1   2  All\n",
       "True                      \n",
       "0          50   0   0   50\n",
       "1           0  48   2   50\n",
       "2           0   0  50   50\n",
       "All        50  48  52  150"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "clf = StackedEnsembleOneVsOne()\n",
    "clf.fit(iris.data, iris.target)\n",
    "y_pred = clf.predict(iris.data)\n",
    "print(metrics.classification_report(iris.target, y_pred))\n",
    "print(\"Confusion Matrix\")\n",
    "display(pd.crosstab(np.array(iris.target), y_pred, rownames=['True'], colnames=['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cCfq178tdnzH"
   },
   "source": [
    "Perform a cross validation experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "PASqIcgMdnzJ",
    "outputId": "13075374-6572-40eb-ce25-b956a184e014"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.93333333 1.         1.         0.93333333 0.93333333\n",
      " 0.93333333 1.         1.         1.        ]\n",
      "0.9733333333333334  +/-  0.03265986323710904\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, iris.data, iris.target, cv=10)\n",
    "print(scores)\n",
    "print(np.mean(scores), \" +/- \", np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RruAHGrbdnzM"
   },
   "source": [
    "## Task 6 Evaluate the Performance of the StackedEnsembleCalassifierOneVsOne Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eUUC2yZydnzN"
   },
   "source": [
    "### StackedEnsembleOneVsOne\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WZQBySIEdnzN"
   },
   "source": [
    "Train StackedEnsembleOneVsOne model with stack layer model logestic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "cxbqCizddnzO",
    "outputId": "c94e934b-e764-4eac-f241-bef316d5ef60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackedEnsembleOneVsOne(base_estimator_duplicates=8,\n",
       "            base_estimator_types='svm',\n",
       "            stack_layer_classifier_type='logreg')"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StackedEnsembleOneVsOne_logregmodel = StackedEnsembleOneVsOne()\n",
    "StackedEnsembleOneVsOne_logregmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kI-Po7yidnzQ"
   },
   "source": [
    "Make prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "id": "04edChBNdnzR",
    "outputId": "48c173fb-cf2b-4cbf-879b-1aac223ca60e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8216666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.78      0.76       171\n",
      "           1       0.98      0.96      0.97       185\n",
      "           2       0.76      0.71      0.73       194\n",
      "           3       0.86      0.87      0.86       195\n",
      "           4       0.62      0.75      0.68       145\n",
      "           5       0.89      0.90      0.89       185\n",
      "           6       0.61      0.46      0.52       171\n",
      "           7       0.87      0.84      0.85       178\n",
      "           8       0.89      0.97      0.93       170\n",
      "           9       0.90      0.94      0.92       206\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      1800\n",
      "   macro avg       0.81      0.82      0.81      1800\n",
      "weighted avg       0.82      0.82      0.82      1800\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>133</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>138</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>169</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>193</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>178</td>\n",
       "      <td>181</td>\n",
       "      <td>182</td>\n",
       "      <td>196</td>\n",
       "      <td>176</td>\n",
       "      <td>186</td>\n",
       "      <td>130</td>\n",
       "      <td>172</td>\n",
       "      <td>185</td>\n",
       "      <td>214</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1    2    3    4    5    6    7    8    9   All\n",
       "True                                                             \n",
       "0          133    1    3   11    0    3   13    0    6    1   171\n",
       "1            1  178    1    3    0    0    2    0    0    0   185\n",
       "2            0    1  138    2   37    0   13    0    3    0   194\n",
       "3            7    0    1  169    8    1    7    0    2    0   195\n",
       "4            0    0   16    6  109    0   13    0    1    0   145\n",
       "5            0    0    0    0    0  166    0   13    3    3   185\n",
       "6           37    0   23    5   22    0   79    0    5    0   171\n",
       "7            0    0    0    0    0   12    0  149    0   17   178\n",
       "8            0    1    0    0    0    1    3    0  165    0   170\n",
       "9            0    0    0    0    0    3    0   10    0  193   206\n",
       "All        178  181  182  196  176  186  130  172  185  214  1800"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = StackedEnsembleOneVsOne_logregmodel.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) # , normalize=True, sample_weight=None\n",
    "model_test_accuracy_comparisons[\"StackedEnsembleOneVsOne_StackLayerModel_logreg\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "# print(metrics.confusion_matrix(y_train, y_pred))\n",
    "\n",
    "# Print nicer homemade confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eVr2v87gdnzW"
   },
   "source": [
    "Train StackedEnsembleOneVsOne model with stack layer model Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "3uPb2M93dnzX",
    "outputId": "021f8178-e623-4bf7-d98a-96944986dc29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackedEnsembleOneVsOne(base_estimator_duplicates=8,\n",
       "            base_estimator_types='svm', stack_layer_classifier_type='tree')"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StackedEnsembleOneVsOne_treemodel = StackedEnsembleOneVsOne(stack_layer_classifier_type='tree')\n",
    "StackedEnsembleOneVsOne_treemodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g-l8-mw_dnza"
   },
   "source": [
    "Make prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "id": "ELkaDHuadnzc",
    "outputId": "48acd654-b88a-4924-ba25-229dc9967e68",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7555555555555555\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.77      0.72       171\n",
      "           1       0.99      0.92      0.96       185\n",
      "           2       0.81      0.49      0.62       194\n",
      "           3       0.83      0.75      0.79       195\n",
      "           4       0.47      0.75      0.58       145\n",
      "           5       0.82      0.89      0.85       185\n",
      "           6       0.43      0.46      0.44       171\n",
      "           7       0.83      0.75      0.79       178\n",
      "           8       0.91      0.93      0.92       170\n",
      "           9       0.87      0.84      0.86       206\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      1800\n",
      "   macro avg       0.77      0.75      0.75      1800\n",
      "weighted avg       0.78      0.76      0.76      1800\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>171</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>146</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>164</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>174</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>191</td>\n",
       "      <td>172</td>\n",
       "      <td>118</td>\n",
       "      <td>175</td>\n",
       "      <td>230</td>\n",
       "      <td>201</td>\n",
       "      <td>181</td>\n",
       "      <td>160</td>\n",
       "      <td>173</td>\n",
       "      <td>199</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1    2    3    4    5    6    7    8    9   All\n",
       "True                                                             \n",
       "0          131    0    0   15    1    1   21    0    2    0   171\n",
       "1            2  171    0    4    3    0    5    0    0    0   185\n",
       "2            1    0   96    1   77    0   19    0    0    0   194\n",
       "3           11    0    1  146   10    0   25    0    2    0   195\n",
       "4            0    0    5    8  109    0   23    0    0    0   145\n",
       "5            1    0    0    0    0  164    0    8    6    6   185\n",
       "6           45    0   16    1   27    0   78    0    4    0   171\n",
       "7            0    0    0    0    0   27    0  133    0   18   178\n",
       "8            0    1    0    0    1    4    4    1  158    1   170\n",
       "9            0    0    0    0    2    5    6   18    1  174   206\n",
       "All        191  172  118  175  230  201  181  160  173  199  1800"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = StackedEnsembleOneVsOne_treemodel.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) # , normalize=True, sample_weight=None\n",
    "model_test_accuracy_comparisons[\"StackedEnsembleOneVsOne_StackLayerModel_Dtree\"] = accuracy\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "# print(metrics.confusion_matrix(y_train, y_pred))\n",
    "\n",
    "# Print nicer homemade confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOoV0FE_dnze"
   },
   "source": [
    "plot the comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "eUcs2N5Y_BMV",
    "outputId": "13272de8-a23c-4e62-b63d-d545d05c0acf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Used for comparision training data X_train :  (4200, 784) testing data X_test (1800, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/kAAAEZCAYAAADMnuWnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with highest accuracy: StackedEnsembleKFold_StackLayerModel_logreg  Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "print(\"Data Used for comparision training data X_train : \",X_train.shape,\"testing data X_test\",X_test.shape)\n",
    "\n",
    "plt.xlim(0, 1.0)\n",
    "_ = plt.barh(range(len(model_test_accuracy_comparisons)), list(model_test_accuracy_comparisons.values()), align='center')\n",
    "_ = plt.yticks(range(len(model_test_accuracy_comparisons)), list(model_test_accuracy_comparisons.keys()))\n",
    "_ = plt.title(\"Comparision of StackedEnsembleClassifier, StackedEnsembleHoldOut, StackedEnsembleKfold, Bagging Ensemble, Decision tree, StackedEnsembleOneVsOne \",fontsize=14)\n",
    "_ = plt.xlabel('Accuracy',fontsize=12)\n",
    "_ = plt.ylabel('Classifier Name',fontsize=12)\n",
    "_ = plt.tick_params(axis='both', which='major', labelsize=11)\n",
    "#displaying labels for each bar\n",
    "for i, v in enumerate(list(model_test_accuracy_comparisons.values())):\n",
    "    a=round(v,2)\n",
    "    plt.text(v+0.01 , i , str(a), color='black', fontweight='bold')\n",
    "plt.show()\n",
    "newD = {k:round(v,2) for k, v in model_test_accuracy_comparisons.items()}\n",
    "haccm = list(max(zip(newD.values(), newD.keys())))\n",
    "print(\"Model with highest accuracy:\",haccm[1],\" Accuracy:\",haccm[0])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StackedEnsembleOneVsOne was added to the comparison of classifiers. This classifier was also modelled once for decision tree and once for logistic regression. The highest accuracy is again found with StackedEnsembleKFold with logistic regression model at 84%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oBVtZ7x2dnzf"
   },
   "source": [
    "## Task 7 Reflect on the Performance of the Different Models Evaluated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rcSlYCKjdnzf"
   },
   "source": [
    "*Write your reflection here (max 300 words)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From all the classifiers above StackEnsembleKFold classifier performed well with an accuracy of 84% in my case on MINIST Data. There is only 1% difference in the accuracy of StackEnsembleHoldOut, StackEnsembleClassifier and StackEnsembleKFold which is not much. StackEnsembleKFold performed well because Kfold was performed on each of the base classifier to generate data for stack layer, and at the end base classifiers were again trained on full data set. The performance of  StackEnsembleOneVsOne was comparatively good with an accuracy of 82% using logistic regression for stack model. It was be observed that using Decision tree as stack model gave a low accuracy compared to using logistic regression for stack model. Simple bagging and boosting were poor in performing on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wmWgLKQCdnzg"
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "GSfGps-Pdnwq",
    "kp7FxnROdnw0",
    "-7e4u6RPdnw3",
    "UQWmkaPJdnxi",
    "oBVtZ7x2dnzf"
   ],
   "name": "latest.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
